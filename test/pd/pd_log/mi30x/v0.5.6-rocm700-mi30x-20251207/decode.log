merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 12-07 16:18:32 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-07 16:18:33] WARNING server_args.py:1866: KV cache is forced as chunk cache for decode server
[2025-12-07 16:18:33] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='10.194.129.138', port=30026, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.88, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=973294859, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=180.0, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_moe_a2a_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='decode', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device='lo', disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, forward_hooks=None)
[2025-12-07 16:18:34] Using default HuggingFace chat template with detected content format: string
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 12-07 16:18:44 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 12-07 16:18:44 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 12-07 16:18:44 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 12-07 16:18:44 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 12-07 16:18:44 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-07 16:18:44 TP1] Process 160 gpu_id 1 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-12-07 16:18:44 TP0] Process 159 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-07 16:18:45 TP3] Process 162 gpu_id 3 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-12-07 16:18:45 TP2] Process 161 gpu_id 2 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-12-07 16:18:45 TP0] Init torch distributed begin.
[2025-12-07 16:18:45 TP1] Init torch distributed begin.
[2025-12-07 16:18:45 TP3] Init torch distributed begin.
[2025-12-07 16:18:45 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-12-07 16:18:45 TP0] sglang is using nccl==2.26.6
[2025-12-07 16:18:54 TP2] Using AiterCustomAllreduce for ROCm.
[2025-12-07 16:18:54 TP3] Using AiterCustomAllreduce for ROCm.
[2025-12-07 16:18:54 TP0] Using AiterCustomAllreduce for ROCm.
[2025-12-07 16:18:54 TP1] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-12-07 16:18:54 TP0] Init torch distributed ends. mem usage=1.21 GB
[2025-12-07 16:18:54 TP3] Init torch distributed ends. mem usage=1.16 GB
[2025-12-07 16:18:54 TP2] Init torch distributed ends. mem usage=1.21 GB
[2025-12-07 16:18:54 TP1] Init torch distributed ends. mem usage=1.22 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-12-07 16:18:55 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-07 16:18:55 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-07 16:18:55 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-07 16:18:55 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-07 16:18:55 TP3] Load weight begin. avail mem=190.27 GB
[2025-12-07 16:18:55 TP2] Load weight begin. avail mem=190.22 GB
[2025-12-07 16:18:55 TP0] Load weight begin. avail mem=190.22 GB
[2025-12-07 16:18:55 TP0] Detected fp8 checkpoint.
[2025-12-07 16:18:55 TP1] Load weight begin. avail mem=190.21 GB
[2025-12-07 16:18:56 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:55,  2.91it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:35,  4.56it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:57,  2.79it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:01<00:47,  3.32it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:32,  4.80it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:28,  5.45it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:17,  8.93it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:14, 10.29it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:02<00:13, 11.04it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:02<00:12, 11.56it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:02<00:11, 12.40it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:02<00:14, 10.14it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:02<00:12, 11.68it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:02<00:10, 12.81it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:02<00:10, 13.50it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:03<00:15,  8.49it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:03<00:13,  9.85it/s]
Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:03<00:14,  8.95it/s]
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:03<00:12, 10.62it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:04<00:10, 12.06it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:04<00:09, 12.98it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:04<00:09, 13.65it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:04<00:08, 14.09it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:04<00:08, 13.39it/s]
Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:04<00:11, 10.09it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:04<00:09, 11.51it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:05<00:08, 12.83it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:05<00:07, 14.24it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:05<00:06, 16.24it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:05<00:11,  8.89it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:05<00:10, 10.39it/s]
Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:06<00:10,  9.81it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:06<00:08, 12.23it/s]
Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:06<00:09, 10.59it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:06<00:08, 11.53it/s]
Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:06<00:07, 12.38it/s]
Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:07<00:07, 11.84it/s]
Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:07<00:07, 11.98it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:07<00:07, 12.19it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:07<00:08, 10.15it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:07<00:07, 11.39it/s]
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:07<00:06, 12.61it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:08<00:06, 12.83it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:08<00:05, 12.87it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:08<00:05, 13.91it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:08<00:04, 15.26it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:08<00:06, 11.47it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:08<00:05, 12.80it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:09<00:08,  7.49it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:09<00:07,  8.76it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:09<00:06,  9.86it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:09<00:07,  8.48it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:10<00:05, 11.25it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:10<00:04, 12.69it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:10<00:03, 13.62it/s]
Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:10<00:03, 16.60it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:10<00:02, 16.68it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:10<00:02, 17.70it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:11<00:03, 11.10it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:11<00:03, 11.95it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:11<00:02, 14.60it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:11<00:02, 16.84it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:11<00:01, 19.42it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:11<00:01, 20.77it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:11<00:01, 22.94it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:11<00:01, 21.77it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:12<00:02, 10.19it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:12<00:01, 12.50it/s]
Loading safetensors checkpoint shards:  91% Completed | 148/163 [00:12<00:01, 14.14it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:12<00:00, 15.48it/s]
Loading safetensors checkpoint shards:  94% Completed | 154/163 [00:13<00:00, 14.17it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:13<00:00, 15.88it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:13<00:00, 17.12it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00, 18.37it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00, 11.95it/s]

[2025-12-07 16:20:24 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-12-07 16:20:24 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-12-07 16:20:25 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.19 GB, mem usage=158.02 GB.
[2025-12-07 16:20:25 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.25 GB, mem usage=158.02 GB.
[2025-12-07 16:20:25 TP0] Using KV cache dtype: torch.bfloat16
[2025-12-07 16:20:25 TP1] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-12-07 16:20:25 TP1] Memory pool end. avail mem=21.53 GB
[2025-12-07 16:20:25 TP0] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-12-07 16:20:25 TP0] Memory pool end. avail mem=21.54 GB
[2025-12-07 16:20:25 TP2] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-12-07 16:20:25 TP2] Memory pool end. avail mem=21.54 GB
[2025-12-07 16:20:25 TP3] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-12-07 16:20:25 TP3] Memory pool end. avail mem=21.59 GB
[2025-12-07 16:20:30 TP0] max_total_num_tokens=143003, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=2048, context_len=163840, available_gpu_mem=21.22 GB
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1207 16:20:30.798034   162 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1207 16:20:30.798041   161 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1207 16:20:30.798051   162 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1207 16:20:30.798053   161 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1207 16:20:30.798072   161 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15190
I1207 16:20:30.798072   162 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16789
I1207 16:20:30.798118   162 transfer_engine.cpp:185] Auto-discovering topology...
I1207 16:20:30.798120   161 transfer_engine.cpp:185] Auto-discovering topology...
W1207 16:20:30.798151   161 topology.cpp:55] No RDMA devices found, check your device installation
W1207 16:20:30.798152   162 topology.cpp:55] No RDMA devices found, check your device installation
I1207 16:20:30.798192   161 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1207 16:20:30.798192   162 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1207 16:20:30.798207   162 tcp_transport.cpp:299] TcpTransport: listen on port 15892
I1207 16:20:30.798208   161 tcp_transport.cpp:299] TcpTransport: listen on port 16332
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1207 16:20:30.800822   160 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1207 16:20:30.800840   160 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1207 16:20:30.800861   160 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15256
I1207 16:20:30.800913   160 transfer_engine.cpp:185] Auto-discovering topology...
W1207 16:20:30.800953   160 topology.cpp:55] No RDMA devices found, check your device installation
I1207 16:20:30.800988   160 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1207 16:20:30.801005   160 tcp_transport.cpp:299] TcpTransport: listen on port 15266
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1207 16:20:30.809433   159 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1207 16:20:30.809448   159 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1207 16:20:30.809468   159 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15911
I1207 16:20:30.809512   159 transfer_engine.cpp:185] Auto-discovering topology...
W1207 16:20:30.809548   159 topology.cpp:55] No RDMA devices found, check your device installation
I1207 16:20:30.809579   159 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1207 16:20:30.809594   159 tcp_transport.cpp:299] TcpTransport: listen on port 16725
[2025-12-07 16:20:30] INFO:     Started server process [1]
[2025-12-07 16:20:30] INFO:     Waiting for application startup.
[2025-12-07 16:20:30] INFO:     Application startup complete.
[2025-12-07 16:20:30] INFO:     Uvicorn running on http://10.194.129.138:30026 (Press CTRL+C to quit)
[2025-12-07 16:20:31] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-07 16:20:31] INFO:     10.194.129.138:53068 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-07 16:20:31] Start of pd disaggregation warmup ...
[2025-12-07 16:20:33] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:33] INFO:     10.194.129.138:53086 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-07 16:20:33] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:38] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:38] INFO:     10.194.129.138:53116 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-07 16:20:38] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:43] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:43] INFO:     10.194.129.138:38798 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-07 16:20:43] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:48] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:48] INFO:     10.194.129.138:38826 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-07 16:20:48] WARNING:  Invalid HTTP request received.
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-07 16:20:50 TP1] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-07 16:20:50 TP1] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-07 16:20:52 TP0] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-07 16:20:52 TP0] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-12-07 16:20:53] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:53] INFO:     10.194.129.138:57590 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-07 16:20:53] WARNING:  Invalid HTTP request received.
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-07 16:20:54 TP3] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-07 16:20:54 TP3] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-07 16:20:54 TP2] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-07 16:20:54 TP2] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-12-07 16:20:57] INFO:     10.194.129.138:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-07 16:20:57] End of prefill disaggregation mode warmup with status 200, resp: [{'text': '# 2023年10月', 'output_ids': [0, 5, 223, 939, 21, 695, 553, 1266], 'meta_info': {'id': '6baa3a844d394b84a7e9916dc3a695d0', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 4, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 25.579139471054077, 'response_sent_to_client_ts': 1765124457.432179}}]
[2025-12-07 16:20:57] The server is fired up and ready to roll!
[2025-12-07 16:20:58] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:58] WARNING:  Invalid HTTP request received.
[2025-12-07 16:20:59] INFO:     10.194.129.138:57622 - "GET /health HTTP/1.1" 200 OK
[2025-12-07 16:20:59] INFO:     10.194.129.138:57656 - "GET /server_info HTTP/1.1" 200 OK
[2025-12-07 16:20:59] INFO:     10.194.129.138:57656 - "GET /model_info HTTP/1.1" 200 OK
[2025-12-07 16:21:29] INFO:     10.194.129.138:50342 - "GET /health HTTP/1.1" 200 OK
[2025-12-07 16:21:31] WARNING:  Invalid HTTP request received.
[2025-12-07 16:21:31] WARNING:  Invalid HTTP request received.
[2025-12-07 16:21:32] INFO:     10.194.129.138:43734 - "GET /health HTTP/1.1" 200 OK
[2025-12-07 16:21:32] INFO:     10.194.129.138:43766 - "GET /server_info HTTP/1.1" 200 OK
[2025-12-07 16:21:32] INFO:     10.194.129.138:43766 - "GET /model_info HTTP/1.1" 200 OK
[2025-12-07 16:21:47] INFO:     10.194.129.138:38498 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-12-07 16:21:51 TP0] Decode batch, #running-req: 1, #token: 32, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: False, gen throughput (token/s): 0.49, #queue-req: 0, 
[2025-12-07 16:21:54] INFO:     10.194.129.138:38498 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-12-07 16:21:56 TP0] Decode batch, #running-req: 1, #token: 29, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: False, gen throughput (token/s): 7.92, #queue-req: 0, 
Memory access fault by GPU node-6 (Agent handle: 0x56550447d000) on address 0x7fb4813bf000. Reason: Unknown.
[2025-12-07 16:22:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:22:32. last_heartbeat time: 16:21:54
[2025-12-07 16:23:53] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:23:32. last_heartbeat time: 16:21:54
[2025-12-07 16:24:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:24:32. last_heartbeat time: 16:21:54
[2025-12-07 16:25:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:25:32. last_heartbeat time: 16:21:54
[2025-12-07 16:26:53] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:26:32. last_heartbeat time: 16:21:54
[2025-12-07 16:27:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:27:32. last_heartbeat time: 16:21:54
[2025-12-07 16:28:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:28:32. last_heartbeat time: 16:21:54
[2025-12-07 16:29:53] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:29:32. last_heartbeat time: 16:21:54
[2025-12-07 16:30:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:30:32. last_heartbeat time: 16:21:54
[2025-12-07 16:31:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:31:32. last_heartbeat time: 16:21:54
[rank3]:[E1207 16:31:56.195615797 ProcessGroupNCCL.cpp:674] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
[rank3]:[E1207 16:31:56.195961597 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 3]  failure detected by watchdog at work sequence id: 82 PG status: last enqueued work: 82, last completed work: 81
[rank3]:[E1207 16:31:56.195972304 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E1207 16:31:56.196000370 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 3] First PG on this rank to signal dumping.
[rank1]:[E1207 16:31:56.196973466 ProcessGroupNCCL.cpp:674] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
[rank1]:[E1207 16:31:56.197161914 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 1]  failure detected by watchdog at work sequence id: 82 PG status: last enqueued work: 82, last completed work: 81
[rank1]:[E1207 16:31:56.197175360 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1207 16:31:56.197200768 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 1] First PG on this rank to signal dumping.
[rank2]:[E1207 16:31:56.209510370 ProcessGroupNCCL.cpp:674] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
[rank2]:[E1207 16:31:56.209668358 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 2]  failure detected by watchdog at work sequence id: 82 PG status: last enqueued work: 82, last completed work: 81
[rank2]:[E1207 16:31:56.209678217 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E1207 16:31:56.209708970 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 2] First PG on this rank to signal dumping.
[rank0]:[E1207 16:31:56.218006855 ProcessGroupNCCL.cpp:1792] [PG ID 0 PG GUID 0 Rank 0] Observed flight recorder dump signal from another rank via TCPStore.
[rank0]:[E1207 16:31:56.218149449 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 0] Received a dump signal due to a collective timeout from  rank 2 and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E1207 16:31:56.235198417 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank1]:[E1207 16:31:57.521323119 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E1207 16:31:57.521503469 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E1207 16:31:57.536089226 ProcessGroupNCCL.cpp:1921] [PG ID 0 PG GUID 0 Rank 0] Could not acquire GIL within 300 ms on exit, possible GIL induced hang
[rank1]:[E1207 16:31:57.821919896 ProcessGroupNCCL.cpp:1921] [PG ID 0 PG GUID 0 Rank 1] Could not acquire GIL within 300 ms on exit, possible GIL induced hang
[rank0]:[E1207 16:31:57.893316771 ProcessGroupNCCL.cpp:674] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
[rank0]:[E1207 16:31:57.893467659 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 0]  failure detected by watchdog at work sequence id: 82 PG status: last enqueued work: 82, last completed work: 81
[rank0]:[E1207 16:31:57.893475653 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E1207 16:31:57.159774022 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[E1207 16:31:57.159786852 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[E1207 16:31:57.159995315 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank2]:[E1207 16:31:57.159992780 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank3]:[E1207 16:31:58.460379595 ProcessGroupNCCL.cpp:1921] [PG ID 0 PG GUID 0 Rank 3] Could not acquire GIL within 300 ms on exit, possible GIL induced hang
[rank2]:[E1207 16:31:58.460632743 ProcessGroupNCCL.cpp:1921] [PG ID 0 PG GUID 0 Rank 2] Could not acquire GIL within 300 ms on exit, possible GIL induced hang
[2025-12-07 16:32:53] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:32:32. last_heartbeat time: 16:21:54
[rank3]:[E1207 16:32:56.196090950 ProcessGroupNCCL.cpp:735] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E1207 16:32:56.196113859 ProcessGroupNCCL.cpp:749] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E1207 16:32:56.197314000 ProcessGroupNCCL.cpp:735] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E1207 16:32:56.197341609 ProcessGroupNCCL.cpp:749] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E1207 16:32:56.198274676 ProcessGroupNCCL.cpp:2055] [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7fbac210633c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7fbafba36400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7fbafba3aba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7fbafba3c147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7fbac05ce253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fbb0ef32ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fbb0efc4850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank1]:[E1207 16:32:56.198633933 ProcessGroupNCCL.cpp:2055] [PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f570c36833c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f5745c98400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f5745c9cba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f5745c9e147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f570a830253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f5759194ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f5759226850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7fbac210633c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7fbafba36400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7fbafba3aba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7fbafba3c147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7fbac05ce253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fbb0ef32ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fbb0efc4850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2061 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7fbac210633c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x292b742 (0x7fbafba12742 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x857dd5 (0x7fbaf993edd5 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xdc253 (0x7fbac05ce253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7fbb0ef32ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7fbb0efc4850 in /lib/x86_64-linux-gnu/libc.so.6)

Fatal Python error: Aborted

Thread 0x00007f4b37fff640 (most recent call first):
  <no Python frame>

Thread 0x00007f4ca3fff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f4caffff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 918 in heartbeat_checker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f4cbbfff640 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py", line 799 in recv_multipart
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 888 in decode_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f4d4bfff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f4d6c3ff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007fbb0ee9d480 (most recent call first):
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 899 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 903 in custom_wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 197 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 319 in outer_wrapper_dummy
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 281 in wrapper_custom
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 280 in all_reduce
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 309 in custom_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 635 in _all_reduce_out_place
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 156 in outplace_all_reduce
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 616 in all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13 in tensor_model_parallel_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/layers/communicator.py", line 781 in _gather_hidden_states_and_residual
  File "/sgl-workspace/sglang/python/sglang/srt/layers/communicator.py", line 500 in prepare_mlp
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 2839 in forward
  what():    File [PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600038 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f570c36833c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f5745c98400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f5745c9cba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f5745c9e147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f570a830253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f5759194ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f5759226850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2061 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f570c36833c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x292b742 (0x7f5745c74742 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x857dd5 (0x7f5743ba0dd5 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xdc253 (0x7f570a830253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f5759194ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f5759226850 in /lib/x86_64-linux-gnu/libc.so.6)

"/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "Fatal Python error: /sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py"Aborted

, line 3134 in forward
Thread 0x  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in 00007ee86c1de640_call_impl (most recent call first):
  <no Python frame>


Thread 0x  File 00007ee8ebfff640" (most recent call first):
/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File   File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py"", line /usr/lib/python3.10/threading.py"3326, line  in 324forward in 
wait  File 
"/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line   File 120" in /usr/lib/python3.10/threading.py"decorate_context
, line 607  File  in "wait
/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2567 in forward_decode
  File   File ""/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py"", line , line 602699 in  in _forward_raw
run  File 
"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py"  File , line "2652/usr/lib/python3.10/threading.py in "forward
, line   File 1016 in "_bootstrap_inner/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py
", line 392 in   File forward_batch_generation"
/usr/lib/python3.10/threading.py"  File , line 973" in /sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py"_bootstrap

, line Thread 0x200200007ee8f7fff640 in  (most recent call first):
run_batch
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/decode.py"  File , line "825/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py" in , line event_loop_overlap_disagg_decode918
 in   File heartbeat_checker"
/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py"  File , line 120" in /usr/lib/python3.10/threading.pydecorate_context"
, line 953 in run
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py  File ""/usr/lib/python3.10/threading.py, line "2694 in run_scheduler_process, line 
1016 in _bootstrap_inner  File 
"/usr/lib/python3.10/multiprocessing/process.py  File "", line /usr/lib/python3.10/threading.py108" in , line run973
 in _bootstrap

  File Thread 0x"00007ee903fff640/usr/lib/python3.10/multiprocessing/process.py" (most recent call first):
, line 314 in _bootstrap
  File   File ""/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py/usr/lib/python3.10/multiprocessing/spawn.py"", line 799 in , line 129 in recv_multipart
_main  File 
"  File /sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py""/usr/lib/python3.10/multiprocessing/spawn.py", line 116, line  in 888 in spawn_main
decode_thread
  File   File ""/usr/lib/python3.10/threading.py<string>"", line 953, line  in 1run in 
<module>  File 
"/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007ee99ffff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007ee9aafff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f57590ff480 (most recent call first):
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 899 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 903 in custom_wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 197 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 319 in outer_wrapper_dummy
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 281 in wrapper_custom
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 280 in all_reduce
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 309 in custom_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 635 in _all_reduce_out_place
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 156 in outplace_all_reduce
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 616 in all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13 in tensor_model_parallel_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/layers/communicator.py", line 781 in _gather_hidden_states_and_residual
  File "/sgl-workspace/sglang/python/sglang/srt/layers/communicator.py", line 500 in prepare_mlp
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 2839 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3134 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3326 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2567 in forward_decode
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2699 in _forward_raw
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2652 in forward
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 392 in forward_batch_generation
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2002 in run_batch
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/decode.py", line 825 in event_loop_overlap_disagg_decode

Extension modules:   File numpy.core._multiarray_umath"/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2694 in run_scheduler_process
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108 in run
  File , "numpy.core._multiarray_tests/usr/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/usr/lib/python3.10/multiprocessing/spawn.py, "numpy.linalg._umath_linalg, line 129 in _main
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", , line numpy.fft._pocketfft_internal1 in <module>
, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special
Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, pybase64._pybase64, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, zmq.backend.cython._zmq, PIL._imaging, cython.cimports.libc.math, psutil._psutil_linux, psutil._psutil_posix, pybase64._pybase64, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, zmq.backend.cython._zmq, sentencepiece._sentencepiece, regex._regex, yaml._yaml, PIL._imaging, markupsafe._speedups, cython.cimports.libc.math, sentencepiece._sentencepiece, regex._regex, yaml._yaml, PIL._imagingft, markupsafe._speedups, PIL._imagingft, _cffi_backend, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, _cffi_backend, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, , scipy.spatial._voronoiscipy._lib._ccallback_c, , scipy.linalg._fblasscipy.spatial._distance_wrap, , scipy.spatial._hausdorffscipy.linalg._flapack, scipy.linalg.cython_lapack, , scipy.spatial.transform._rotationscipy.linalg._cythonized_array_utils, scipy.optimize._direct, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, pyzstd._c._zstd, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, sklearn.__check_build._check_build, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.integrate._odepack, scipy.integrate._quadpack, scipy._lib._uarray._uarray, scipy.integrate._vode, scipy.integrate._dop, , scipy.special._ufuncs_cxxscipy.integrate._lsoda, scipy.special._ufuncs, scipy.special._specfun, scipy.interpolate._fitpack, scipy.special._comb, scipy.interpolate._dfitpack, scipy.special._ellip_harm_2, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.linalg._decomp_interpolative, , scipy.optimize._bglu_densescipy.interpolate._rbfinterp_pythran, scipy.optimize._lsap, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.spatial._ckdtree, , scipy.special.cython_specialscipy.spatial._qhull, scipy.stats._stats, scipy.spatial._voronoi, scipy.stats._sobol, scipy.spatial._distance_wrap, scipy.stats._qmc_cy, scipy.spatial._hausdorff, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.spatial.transform._rotation, scipy.stats._levy_stable.levyst, scipy.optimize._direct, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyzstd._c._zstd, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, sklearn.__check_build._check_build, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, scipy.integrate._odepack, scipy.integrate._quadpack, pandas._libs.tslib, scipy.integrate._vode, scipy.integrate._dop, pandas._libs.sparse, scipy.integrate._lsoda, pandas._libs.internals, pandas._libs.indexing, scipy.interpolate._fitpack, pandas._libs.index, scipy.interpolate._dfitpack, pandas._libs.writers, scipy.interpolate._dierckx, pandas._libs.join, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, , pandas._libs.window.aggregationsscipy.interpolate._rgi_cython, pandas._libs.window.indexers, scipy.interpolate._bspl, pandas._libs.reshape, scipy.special.cython_special, scipy.stats._stats, pandas._libs.groupby, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, , pandas._libs.jsonscipy.stats._stats_pythran, , scipy.stats._levy_stable.levystpandas._libs.parsers, pandas._libs.testing, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, _cyutility, scipy.ndimage._nd_image, sklearn._cyutility, scipy.ndimage._rank_filter_1d, sklearn.utils._isfinite, , sklearn.utils.sparsefuncs_fast_ni_label, scipy.ndimage._ni_label, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, pyarrow.lib, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, , pandas._libs.tslibs.ccalendarsklearn.metrics._dist_metrics, , pandas._libs.tslibs.np_datetimesklearn.metrics._pairwise_distances_reduction._datasets_pair, pandas._libs.tslibs.dtypes, sklearn.utils._cython_blas, , sklearn.metrics._pairwise_distances_reduction._basepandas._libs.tslibs.base, , pandas._libs.tslibs.nattypesklearn.metrics._pairwise_distances_reduction._middle_term_computer, , sklearn.utils._heappandas._libs.tslibs.timezones, sklearn.utils._sorting, , pandas._libs.tslibs.fieldssklearn.metrics._pairwise_distances_reduction._argkmin, , sklearn.metrics._pairwise_distances_reduction._argkmin_classmodepandas._libs.tslibs.timedeltas, sklearn.utils._vector_sentinel, pandas._libs.tslibs.tzconversion, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, pandas._libs.tslibs.timestamps, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, pandas._libs.properties, sklearn.metrics._pairwise_fast, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, , pandas._libs.algossetproctitle._setproctitle, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, Cython.Utils, pandas._libs.arrays, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, , Cython.Plex.DFApandas._libs.tslib, Cython.Plex.Scanners, , pandas._libs.sparseCython.Compiler.Scanning, Cython.StringIOTree, pandas._libs.internals, Cython.Compiler.Code, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, hip_utils, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, hiredis.hiredis, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, setproctitle._setproctitle, _cbor2, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.Compiler.Scanning, Cython.StringIOTree, Cython.Compiler.Code, msgspec._core, hip_utils, hiredis.hiredis, _cbor2, msgspec._core, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, __triton_launcher (total: 211)
, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, __triton_launcher (total: 211)
[rank2]:[E1207 16:32:56.209829854 ProcessGroupNCCL.cpp:735] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E1207 16:32:56.209855202 ProcessGroupNCCL.cpp:749] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E1207 16:32:56.211106889 ProcessGroupNCCL.cpp:2055] [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f1d4b90433c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f1d85234400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f1d85238ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f1d8523a147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f1d49dcc253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f1d98730ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f1d987c2850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f1d4b90433c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f1d85234400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f1d85238ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f1d8523a147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f1d49dcc253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f1d98730ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f1d987c2850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2061 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f1d4b90433c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x292b742 (0x7f1d85210742 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x857dd5 (0x7f1d8313cdd5 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xdc253 (0x7f1d49dcc253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f1d98730ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f1d987c2850 in /lib/x86_64-linux-gnu/libc.so.6)

Fatal Python error: Aborted

Thread 0x00007eadb3fff640 (most recent call first):
  <no Python frame>

Thread 0x00007eaf2bfff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eaf37fff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 918 in heartbeat_checker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eaf43fff640 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py", line 799 in recv_multipart
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 888 in decode_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eadbffff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eafe9dff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f1d9869b480 (most recent call first):
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 899 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 903 in custom_wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 197 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 319 in outer_wrapper_dummy
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 281 in wrapper_custom
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 280 in all_reduce
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 309 in custom_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 635 in _all_reduce_out_place
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 156 in outplace_all_reduce
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 616 in all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13 in tensor_model_parallel_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/layers/communicator.py", line 781 in _gather_hidden_states_and_residual
  File "/sgl-workspace/sglang/python/sglang/srt/layers/communicator.py", line 500 in prepare_mlp
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 2839 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3134 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3326 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2567 in forward_decode
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2699 in _forward_raw
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2652 in forward
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 392 in forward_batch_generation
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2002 in run_batch
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/decode.py", line 825 in event_loop_overlap_disagg_decode
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2694 in run_scheduler_process
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108 in run
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", line 1 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, pybase64._pybase64, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, zmq.backend.cython._zmq, PIL._imaging, cython.cimports.libc.math, sentencepiece._sentencepiece, regex._regex, yaml._yaml, markupsafe._speedups, PIL._imagingft, _cffi_backend, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, pyzstd._c._zstd, sklearn.__check_build._check_build, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, setproctitle._setproctitle, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.Compiler.Scanning, Cython.StringIOTree, Cython.Compiler.Code, hip_utils, hiredis.hiredis, _cbor2, msgspec._core, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, __triton_launcher (total: 211)
[rank0]:[E1207 16:32:57.893587795 ProcessGroupNCCL.cpp:735] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E1207 16:32:57.893608255 ProcessGroupNCCL.cpp:749] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E1207 16:32:57.894878179 ProcessGroupNCCL.cpp:2055] [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7ff96fa7f33c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7ff9a93af400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7ff9a93b3ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7ff9a93b5147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7ff96df47253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7ff9bc8abac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ff9bc93d850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=82, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600052 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7ff96fa7f33c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7ff9a93af400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7ff9a93b3ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7ff9a93b5147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7ff96df47253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7ff9bc8abac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ff9bc93d850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2061 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7ff96fa7f33c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x292b742 (0x7ff9a938b742 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x857dd5 (0x7ff9a72b7dd5 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xdc253 (0x7ff96df47253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7ff9bc8abac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7ff9bc93d850 in /lib/x86_64-linux-gnu/libc.so.6)

Fatal Python error: Aborted

Thread 0x00007f89ebfff640 (most recent call first):
  <no Python frame>

Thread 0x00007f8b3ffff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f8b57fff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 918 in heartbeat_checker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f8b87fff640 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py", line 799 in recv_multipart
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 888 in decode_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f898bfff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f8bfddff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007ff9bc816480 (most recent call first):
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 899 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 903 in custom_wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 197 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 319 in outer_wrapper_dummy
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 281 in wrapper_custom
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 280 in all_reduce
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 309 in custom_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 635 in _all_reduce_out_place
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 156 in outplace_all_reduce
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 616 in all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13 in tensor_model_parallel_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/layers/communicator.py", line 781 in _gather_hidden_states_and_residual
  File "/sgl-workspace/sglang/python/sglang/srt/layers/communicator.py", line 500 in prepare_mlp
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 2839 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3134 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3326 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2567 in forward_decode
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2699 in _forward_raw
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2652 in forward
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 392 in forward_batch_generation
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2002 in run_batch
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/decode.py", line 825 in event_loop_overlap_disagg_decode
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2694 in run_scheduler_process
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108 in run
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", line 1 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, pybase64._pybase64, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, zmq.backend.cython._zmq, PIL._imaging, cython.cimports.libc.math, sentencepiece._sentencepiece, regex._regex, yaml._yaml, markupsafe._speedups, PIL._imagingft, _cffi_backend, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, pyzstd._c._zstd, sklearn.__check_build._check_build, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, setproctitle._setproctitle, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.Compiler.Scanning, Cython.StringIOTree, Cython.Compiler.Code, hip_utils, hiredis.hiredis, _cbor2, msgspec._core, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, __triton_launcher (total: 211)
[2025-12-07 16:33:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:33:32. last_heartbeat time: 16:21:54
[2025-12-07 16:34:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:34:32. last_heartbeat time: 16:21:54
[2025-12-07 16:35:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:35:32. last_heartbeat time: 16:21:54
[2025-12-07 16:36:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:36:32. last_heartbeat time: 16:21:54
[2025-12-07 16:37:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:37:32. last_heartbeat time: 16:21:54
[2025-12-07 16:38:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:38:32. last_heartbeat time: 16:21:54
[2025-12-07 16:39:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:39:32. last_heartbeat time: 16:21:54
[2025-12-07 16:40:01] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-12-07 16:40:01] Signal SIGTERM received while health check failed. Force exiting.
[2025-12-07 16:40:01] ERROR:    Traceback (most recent call last):
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "uvloop/loop.pyx", line 1512, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1505, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1379, in uvloop.loop.Loop.run_forever
  File "uvloop/loop.pyx", line 557, in uvloop.loop.Loop._run
  File "uvloop/loop.pyx", line 476, in uvloop.loop.Loop._on_idle
  File "uvloop/cbhandles.pyx", line 83, in uvloop.loop.Handle._run
  File "uvloop/cbhandles.pyx", line 63, in uvloop.loop.Handle._run
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 2470, in print_exception_wrapper
    await func()
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 1534, in sigterm_watchdog
    kill_process_tree(os.getpid(), include_parent=True)
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 1073, in kill_process_tree
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 701, in lifespan
    await receive()
  File "/opt/venv/lib/python3.10/site-packages/uvicorn/lifespan/on.py", line 137, in receive
    return await self.receive_queue.get()
  File "/usr/lib/python3.10/asyncio/queues.py", line 159, in get
    await getter
asyncio.exceptions.CancelledError

[2025-12-07 16:40:01] ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "uvloop/loop.pyx", line 1512, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1505, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1379, in uvloop.loop.Loop.run_forever
  File "uvloop/loop.pyx", line 557, in uvloop.loop.Loop._run
  File "uvloop/loop.pyx", line 476, in uvloop.loop.Loop._on_idle
  File "uvloop/cbhandles.pyx", line 83, in uvloop.loop.Handle._run
  File "uvloop/cbhandles.pyx", line 63, in uvloop.loop.Handle._run
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 2470, in print_exception_wrapper
    await func()
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 1534, in sigterm_watchdog
    kill_process_tree(os.getpid(), include_parent=True)
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 1073, in kill_process_tree
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py", line 409, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/opt/venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/fastapi/applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/middleware/cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 78, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 75, in app
    response = await f(request)
  File "/opt/venv/lib/python3.10/site-packages/fastapi/routing.py", line 302, in app
    raw_response = await run_endpoint_function(
  File "/opt/venv/lib/python3.10/site-packages/fastapi/routing.py", line 213, in run_endpoint_function
    return await dependant.call(**values)
  File "/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py", line 1131, in openai_v1_completions
    return await raw_request.app.state.openai_serving_completion.handle_request(
  File "/sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/serving_base.py", line 112, in handle_request
    return await self._handle_non_streaming_request(
  File "/sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/serving_completions.py", line 347, in _handle_non_streaming_request
    ret = await generator.__anext__()
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 526, in generate_request
    async for response in self._wait_one_response(obj, state, request):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 1050, in _wait_one_response
[2025-12-07 16:40:01] INFO:     10.194.129.138:38498 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
    await asyncio.wait_for(state.event.wait(), timeout=4)
  File "/usr/lib/python3.10/asyncio/tasks.py", line 432, in wait_for
    await waiter
asyncio.exceptions.CancelledError
