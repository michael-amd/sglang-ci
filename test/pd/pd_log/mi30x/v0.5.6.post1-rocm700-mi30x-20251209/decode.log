[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-12-09 16:18:27] WARNING core.py:454: WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-09 16:18:27] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-09 16:18:28 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
`rope_parameters`'s factor field must be a float >= 1, got 40
`rope_parameters`'s beta_fast field must be a float, got 32
`rope_parameters`'s beta_slow field must be a float, got 1
[2025-12-09 16:18:29] WARNING server_args.py:1869: KV cache is forced as chunk cache for decode server
[2025-12-09 16:18:29] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='10.194.129.138', port=30026, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.88, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=816099750, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=180.0, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_moe_a2a_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='decode', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device='lo', disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, forward_hooks=None)
[2025-12-09 16:18:30] Using default HuggingFace chat template with detected content format: string
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-12-09 16:18:36] WARNING core.py:454: WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-09 16:18:36] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-12-09 16:18:36] WARNING core.py:454: WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-12-09 16:18:36] WARNING core.py:454: WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-09 16:18:36] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-09 16:18:36] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-12-09 16:18:36] WARNING core.py:454: WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-09 16:18:36] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-12-09 16:18:36] WARNING core.py:454: WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-09 16:18:36] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-09 16:18:37 [__init__.py:241] Automatically detected platform rocm.
INFO 12-09 16:18:37 [__init__.py:241] Automatically detected platform rocm.
INFO 12-09 16:18:37 [__init__.py:241] Automatically detected platform rocm.
INFO 12-09 16:18:37 [__init__.py:241] Automatically detected platform rocm.
INFO 12-09 16:18:37 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:65: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-09 16:18:38 TP3] Process 161 gpu_id 3 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`rope_parameters`'s factor field must be a float >= 1, got 40
`rope_parameters`'s beta_fast field must be a float, got 32
`rope_parameters`'s beta_slow field must be a float, got 1
[2025-12-09 16:18:38 TP1] Process 159 gpu_id 1 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
`rope_parameters`'s factor field must be a float >= 1, got 40
`rope_parameters`'s beta_fast field must be a float, got 32
`rope_parameters`'s beta_slow field must be a float, got 1
[2025-12-09 16:18:38 TP0] Process 158 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`rope_parameters`'s factor field must be a float >= 1, got 40
`rope_parameters`'s beta_fast field must be a float, got 32
`rope_parameters`'s beta_slow field must be a float, got 1
[2025-12-09 16:18:38 TP2] Process 160 gpu_id 2 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`rope_parameters`'s factor field must be a float >= 1, got 40
`rope_parameters`'s beta_fast field must be a float, got 32
`rope_parameters`'s beta_slow field must be a float, got 1
[2025-12-09 16:18:38 TP3] Init torch distributed begin.
[2025-12-09 16:18:39 TP2] Init torch distributed begin.
[2025-12-09 16:18:39 TP1] Init torch distributed begin.
[2025-12-09 16:18:39 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-12-09 16:18:39 TP0] sglang is using nccl==2.26.6
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-09 16:18:48 TP3] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-09 16:18:48 TP3] Using AiterCustomAllreduce for ROCm.
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-09 16:18:48 TP2] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-09 16:18:48 TP0] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-09 16:18:48 TP2] Using AiterCustomAllreduce for ROCm.
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-09 16:18:48 TP1] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-09 16:18:48 TP0] Using AiterCustomAllreduce for ROCm.
[2025-12-09 16:18:48 TP1] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-12-09 16:18:49 TP0] Init torch distributed ends. mem usage=1.21 GB
[2025-12-09 16:18:49 TP3] Init torch distributed ends. mem usage=1.16 GB
[2025-12-09 16:18:49 TP2] Init torch distributed ends. mem usage=1.21 GB
[2025-12-09 16:18:49 TP1] Init torch distributed ends. mem usage=1.22 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-12-09 16:18:50 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-09 16:18:50 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-09 16:18:50 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-09 16:18:50 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-09 16:18:50 TP2] Load weight begin. avail mem=190.22 GB
[2025-12-09 16:18:50 TP0] Load weight begin. avail mem=190.22 GB
[2025-12-09 16:18:50 TP3] Load weight begin. avail mem=190.27 GB
[2025-12-09 16:18:50 TP1] Load weight begin. avail mem=190.21 GB
[2025-12-09 16:18:50 TP0] Detected fp8 checkpoint.
[2025-12-09 16:18:50 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:21,  7.68it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:13, 11.57it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:35,  4.46it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:23,  6.57it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:17,  8.69it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:17,  8.84it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:01<00:14, 10.44it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:01<00:12, 11.88it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:01<00:12, 11.26it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:01<00:11, 12.34it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:02<00:10, 13.50it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:02<00:11, 11.95it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:02<00:10, 13.48it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:02<00:15,  8.48it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:03<00:13,  9.52it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:03<00:10, 12.29it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:03<00:09, 13.51it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:03<00:10, 12.54it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:03<00:09, 13.54it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:03<00:08, 14.53it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:03<00:09, 12.47it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:04<00:08, 13.38it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:04<00:07, 15.93it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:04<00:08, 13.50it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:04<00:07, 14.45it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:04<00:07, 15.54it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:04<00:07, 13.49it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:05<00:12,  8.16it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:05<00:10,  9.79it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:05<00:10,  9.91it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:05<00:07, 12.33it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:06<00:07, 12.38it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:06<00:06, 13.89it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:06<00:06, 14.05it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:06<00:07, 12.15it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:06<00:05, 14.31it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:06<00:06, 12.83it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:07<00:05, 14.69it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:07<00:05, 15.56it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:07<00:05, 13.38it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:07<00:05, 14.44it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:07<00:05, 13.60it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:07<00:04, 14.59it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:08<00:08,  8.01it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:08<00:08,  7.93it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:08<00:06,  9.24it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:08<00:05, 10.82it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:09<00:05, 10.63it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:09<00:04, 12.31it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:09<00:05, 10.85it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:09<00:04, 12.30it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:09<00:03, 14.88it/s]
Loading safetensors checkpoint shards:  71% Completed | 115/163 [00:09<00:03, 13.02it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:09<00:03, 14.29it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:10<00:02, 16.42it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:10<00:02, 13.81it/s]
Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:10<00:02, 15.95it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:10<00:02, 16.51it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:10<00:02, 14.16it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:10<00:02, 15.38it/s]
Loading safetensors checkpoint shards:  82% Completed | 134/163 [00:10<00:01, 17.71it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:11<00:01, 14.58it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:11<00:01, 15.00it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:11<00:01, 15.71it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:12<00:03,  6.86it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:12<00:01,  9.22it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:12<00:01,  9.14it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:12<00:01, 10.51it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:12<00:01, 11.99it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:12<00:00, 11.16it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:13<00:00, 12.44it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:13<00:00, 13.39it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:13<00:00, 12.05it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:13<00:00, 12.90it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00, 14.41it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00, 11.99it/s]

[2025-12-09 16:20:00 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.25 GB, mem usage=158.02 GB.
[2025-12-09 16:20:00 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.20 GB, mem usage=158.02 GB.
[2025-12-09 16:20:01 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.20 GB, mem usage=158.02 GB.
[2025-12-09 16:20:01 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.19 GB, mem usage=158.02 GB.
[2025-12-09 16:20:01 TP0] Using KV cache dtype: torch.bfloat16
[2025-12-09 16:20:01 TP2] KV Cache is allocated. #tokens: 142943, KV size: 9.36 GB
[2025-12-09 16:20:01 TP2] Memory pool end. avail mem=21.54 GB
[2025-12-09 16:20:01 TP1] KV Cache is allocated. #tokens: 142943, KV size: 9.36 GB
[2025-12-09 16:20:01 TP3] KV Cache is allocated. #tokens: 142943, KV size: 9.36 GB
[2025-12-09 16:20:01 TP0] KV Cache is allocated. #tokens: 142943, KV size: 9.36 GB
[2025-12-09 16:20:01 TP1] Memory pool end. avail mem=21.52 GB
[2025-12-09 16:20:01 TP3] Memory pool end. avail mem=21.58 GB
[2025-12-09 16:20:01 TP0] Memory pool end. avail mem=21.54 GB
[2025-12-09 16:20:06 TP0] max_total_num_tokens=142943, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=2048, context_len=163840, available_gpu_mem=21.22 GB
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1209 16:20:07.079779   160 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1209 16:20:07.079797   160 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1209 16:20:07.079792   161 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1209 16:20:07.079804   161 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1209 16:20:07.079821   160 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16487
I1209 16:20:07.079823   161 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16529
I1209 16:20:07.079869   160 transfer_engine.cpp:185] Auto-discovering topology...
I1209 16:20:07.079869   161 transfer_engine.cpp:185] Auto-discovering topology...
W1209 16:20:07.079905   161 topology.cpp:55] No RDMA devices found, check your device installation
W1209 16:20:07.079905   160 topology.cpp:55] No RDMA devices found, check your device installation
I1209 16:20:07.079941   161 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1209 16:20:07.079941   160 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1209 16:20:07.079957   161 tcp_transport.cpp:299] TcpTransport: listen on port 15019
I1209 16:20:07.079960   160 tcp_transport.cpp:299] TcpTransport: listen on port 16042
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1209 16:20:07.080192   159 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1209 16:20:07.080209   159 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1209 16:20:07.080232   159 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15061
I1209 16:20:07.080283   159 transfer_engine.cpp:185] Auto-discovering topology...
W1209 16:20:07.080322   159 topology.cpp:55] No RDMA devices found, check your device installation
I1209 16:20:07.080359   159 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1209 16:20:07.080379   159 tcp_transport.cpp:299] TcpTransport: listen on port 16173
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1209 16:20:07.107874   158 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1209 16:20:07.107893   158 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1209 16:20:07.107915   158 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15559
I1209 16:20:07.107962   158 transfer_engine.cpp:185] Auto-discovering topology...
W1209 16:20:07.107995   158 topology.cpp:55] No RDMA devices found, check your device installation
I1209 16:20:07.108024   158 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1209 16:20:07.108039   158 tcp_transport.cpp:299] TcpTransport: listen on port 16904
[2025-12-09 16:20:07] INFO:     Started server process [1]
[2025-12-09 16:20:07] INFO:     Waiting for application startup.
[2025-12-09 16:20:07] INFO:     Application startup complete.
[2025-12-09 16:20:07] INFO:     Uvicorn running on http://10.194.129.138:30026 (Press CTRL+C to quit)
[2025-12-09 16:20:08] INFO:     10.194.129.138:46732 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-09 16:20:08] WARNING:  Invalid HTTP request received.
[2025-12-09 16:20:08] WARNING:  Invalid HTTP request received.
[2025-12-09 16:20:08] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-09 16:20:08] INFO:     10.194.129.138:46762 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-09 16:20:08] Start of pd disaggregation warmup ...
[2025-12-09 16:20:13] WARNING:  Invalid HTTP request received.
[2025-12-09 16:20:13] INFO:     10.194.129.138:37326 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-09 16:20:13] WARNING:  Invalid HTTP request received.
[2025-12-09 16:20:18] WARNING:  Invalid HTTP request received.
[2025-12-09 16:20:18] INFO:     10.194.129.138:37336 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-09 16:20:18] WARNING:  Invalid HTTP request received.
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-09 16:20:19 TP3] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-09 16:20:19 TP3] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-09 16:20:20 TP0] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-09 16:20:20 TP0] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-09 16:20:21 TP1] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-09 16:20:21 TP1] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-09 16:20:22 TP2] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-09 16:20:22 TP2] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-09 16:20:23] WARNING:  Invalid HTTP request received.
[2025-12-09 16:20:23] INFO:     10.194.129.138:34692 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-09 16:20:23] WARNING:  Invalid HTTP request received.
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-09 16:20:24 TP3] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-09 16:20:24 TP0] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-09 16:20:25 TP1] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-09 16:20:26 TP2] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-09 16:20:26 TP3] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-09 16:20:26 TP3] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-09 16:20:26 TP3] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-09 16:20:26 TP3] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-09 16:20:26 TP3] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-09 16:20:26 TP0] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-09 16:20:27 TP0] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-09 16:20:27 TP0] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-09 16:20:27 TP0] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-09 16:20:27 TP0] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-09 16:20:27 TP1] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-09 16:20:27 TP1] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-09 16:20:27 TP1] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-09 16:20:27 TP1] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-09 16:20:27 TP1] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-12-09 16:20:28] WARNING:  Invalid HTTP request received.
[2025-12-09 16:20:28] INFO:     10.194.129.138:34722 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-12-09 16:20:28] WARNING:  Invalid HTTP request received.
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-09 16:20:28 TP2] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-09 16:20:28 TP2] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-09 16:20:28 TP2] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-09 16:20:28 TP2] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-09 16:20:28 TP2] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-12-09 16:20:30] INFO:     10.194.129.138:46778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-09 16:20:30] End of prefill disaggregation mode warmup with status 200, resp: [{'text': '# 1 092 000', 'output_ids': [0, 5, 223, 19, 223, 25349, 223, 1320], 'meta_info': {'id': '8f0bfa852c274849b675b0f0d948d2aa', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 4, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 22.3731689453125, 'response_sent_to_client_ts': 1765297230.5215938}}]
[2025-12-09 16:20:30] The server is fired up and ready to roll!
[2025-12-09 16:20:33] WARNING:  Invalid HTTP request received.
[2025-12-09 16:20:33] WARNING:  Invalid HTTP request received.
[2025-12-09 16:20:34] INFO:     10.194.129.138:55790 - "GET /health HTTP/1.1" 200 OK
[2025-12-09 16:20:34] INFO:     10.194.129.138:55816 - "GET /server_info HTTP/1.1" 200 OK
[2025-12-09 16:20:34] INFO:     10.194.129.138:55816 - "GET /model_info HTTP/1.1" 200 OK
[2025-12-09 16:20:40] INFO:     10.194.129.138:55824 - "GET /health HTTP/1.1" 200 OK
[2025-12-09 16:21:28] INFO:     10.194.129.138:41240 - "GET /health HTTP/1.1" 200 OK
[2025-12-09 16:21:31] WARNING:  Invalid HTTP request received.
[2025-12-09 16:21:31] WARNING:  Invalid HTTP request received.
[2025-12-09 16:21:32] INFO:     10.194.129.138:41242 - "GET /health HTTP/1.1" 200 OK
[2025-12-09 16:21:32] INFO:     10.194.129.138:35224 - "GET /server_info HTTP/1.1" 200 OK
[2025-12-09 16:21:32] INFO:     10.194.129.138:35224 - "GET /model_info HTTP/1.1" 200 OK
[2025-12-09 16:21:47] INFO:     10.194.129.138:43744 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-12-09 16:21:51 TP0] Decode batch, #running-req: 1, #token: 31, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: False, gen throughput (token/s): 0.38, #queue-req: 0, 
[2025-12-09 16:21:54] INFO:     10.194.129.138:43744 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-12-09 16:21:56 TP0] Decode batch, #running-req: 1, #token: 28, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: False, gen throughput (token/s): 7.69, #queue-req: 0, 
[2025-12-09 16:22:01 TP0] Decode batch, #running-req: 1, #token: 68, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: False, gen throughput (token/s): 7.86, #queue-req: 0, 
[2025-12-09 16:22:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:22:32. last_heartbeat time: 16:22:00
[2025-12-09 16:23:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:23:32. last_heartbeat time: 16:22:00
[2025-12-09 16:24:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:24:32. last_heartbeat time: 16:22:00
[2025-12-09 16:25:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:25:32. last_heartbeat time: 16:22:00
[2025-12-09 16:26:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:26:32. last_heartbeat time: 16:22:00
[2025-12-09 16:27:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:27:32. last_heartbeat time: 16:22:00
[2025-12-09 16:28:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:28:32. last_heartbeat time: 16:22:00
[2025-12-09 16:29:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:29:32. last_heartbeat time: 16:22:00
[2025-12-09 16:30:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:30:32. last_heartbeat time: 16:22:00
[2025-12-09 16:31:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:31:32. last_heartbeat time: 16:22:00
[rank2]:[E1209 16:32:02.302645010 ProcessGroupNCCL.cpp:674] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600010 milliseconds before timing out.
[rank2]:[E1209 16:32:02.302825851 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 2]  failure detected by watchdog at work sequence id: 133 PG status: last enqueued work: 133, last completed work: 132
[rank2]:[E1209 16:32:02.302836221 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E1209 16:32:02.302856894 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 2] First PG on this rank to signal dumping.
[rank3]:[E1209 16:32:02.306333600 ProcessGroupNCCL.cpp:674] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
[rank3]:[E1209 16:32:02.306515129 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 3]  failure detected by watchdog at work sequence id: 133 PG status: last enqueued work: 133, last completed work: 132
[rank3]:[E1209 16:32:02.306525541 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E1209 16:32:02.306551432 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 3] First PG on this rank to signal dumping.
[rank1]:[E1209 16:32:02.327757653 ProcessGroupNCCL.cpp:674] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600031 milliseconds before timing out.
[rank1]:[E1209 16:32:02.327942971 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 1]  failure detected by watchdog at work sequence id: 133 PG status: last enqueued work: 133, last completed work: 132
[rank1]:[E1209 16:32:02.327954838 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1209 16:32:02.327986519 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 1] First PG on this rank to signal dumping.
[rank1]:[E1209 16:32:02.330996050 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E1209 16:32:02.338770192 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E1209 16:32:02.382713668 ProcessGroupNCCL.cpp:674] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
[rank0]:[E1209 16:32:02.382897232 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 0]  failure detected by watchdog at work sequence id: 133 PG status: last enqueued work: 133, last completed work: 132
[rank0]:[E1209 16:32:02.382907712 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E1209 16:32:02.382927270 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 0] First PG on this rank to signal dumping.
[rank1]:[E1209 16:32:03.639741500 ProcessGroupNCCL.cpp:1921] [PG ID 0 PG GUID 0 Rank 1] Could not acquire GIL within 300 ms on exit, possible GIL induced hang
[rank3]:[E1209 16:32:03.047553665 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[E1209 16:32:03.047745559 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank2]:[E1209 16:32:03.263722917 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank2]:[E1209 16:32:03.263894250 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E1209 16:32:03.269841538 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E1209 16:32:03.270000761 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank3]:[E1209 16:32:03.348178444 ProcessGroupNCCL.cpp:1921] [PG ID 0 PG GUID 0 Rank 3] Could not acquire GIL within 300 ms on exit, possible GIL induced hang
[rank2]:[E1209 16:32:04.564432727 ProcessGroupNCCL.cpp:1921] [PG ID 0 PG GUID 0 Rank 2] Could not acquire GIL within 300 ms on exit, possible GIL induced hang
[rank0]:[E1209 16:32:04.570597826 ProcessGroupNCCL.cpp:1921] [PG ID 0 PG GUID 0 Rank 0] Could not acquire GIL within 300 ms on exit, possible GIL induced hang
[2025-12-09 16:32:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:32:32. last_heartbeat time: 16:22:00
[rank2]:[E1209 16:33:02.302974883 ProcessGroupNCCL.cpp:735] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E1209 16:33:02.303003966 ProcessGroupNCCL.cpp:749] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E1209 16:33:02.304595351 ProcessGroupNCCL.cpp:2055] [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600010 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f0d09a0333c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f0d43333400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f0d43337ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f0d43339147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f0d07ecb253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f0d5682fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f0d568c1850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600010 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f0d09a0333c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f0d43333400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f0d43337ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f0d43339147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f0d07ecb253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f0d5682fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f0d568c1850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2061 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f0d09a0333c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x292b742 (0x7f0d4330f742 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x857dd5 (0x7f0d4123bdd5 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xdc253 (0x7f0d07ecb253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f0d5682fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f0d568c1850 in /lib/x86_64-linux-gnu/libc.so.6)

Fatal Python error: Aborted

Thread 0x00007e9d97fff640 (most recent call first):
  <no Python frame>

Thread 0x00007e9f27fff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007e9f3ffff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 918 in heartbeat_checker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007e9f6ffff640 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py", line 799 in recv_multipart
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 888 in decode_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007e9f1bfff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007e9fe47ff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f0d5679a480 (most recent call first):
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 940 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 944 in custom_wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 197 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 319 in outer_wrapper_dummy
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 281 in wrapper_custom
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 281 in all_reduce
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 314 in custom_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 635 in _all_reduce_out_place
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 156 in outplace_all_reduce
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 616 in all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13 in tensor_model_parallel_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 910 in forward_normal
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 793 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 2874 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3151 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3340 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2568 in forward_decode
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2700 in _forward_raw
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2653 in forward
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 392 in forward_batch_generation
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2005 in run_batch
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/decode.py", line 825 in event_loop_overlap_disagg_decode
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2697 in run_scheduler_process
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108 in run
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", line 1 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special[rank3]:[E1209 16:33:02.306660527 ProcessGroupNCCL.cpp:735] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E1209 16:33:02.306684450 ProcessGroupNCCL.cpp:749] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
, psutil._psutil_linux, psutil._psutil_posix, pybase64._pybase64, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, zmq.backend.cython._zmq, PIL._imaging, cython.cimports.libc.math, sentencepiece._sentencepiece, regex._regex, yaml._yaml, markupsafe._speedups[rank3]:[E1209 16:33:02.307911372 ProcessGroupNCCL.cpp:2055] [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f4f1912d33c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f4f52a5d400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f4f52a61ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f4f52a63147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f4f175f5253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f4f65f59ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f4f65feb850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
, PIL._imagingft  what():  [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f4f1912d33c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f4f52a5d400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f4f52a61ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f4f52a63147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f4f175f5253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f4f65f59ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f4f65feb850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2061 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f4f1912d33c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x292b742 (0x7f4f52a39742 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x857dd5 (0x7f4f50965dd5 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xdc253 (0x7f4f175f5253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f4f65f59ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f4f65feb850 in /lib/x86_64-linux-gnu/libc.so.6)

Fatal Python error: Aborted

Thread 0x00007edfbbfff640 (most recent call first):
  <no Python frame>

Thread 0x00007ee133fff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007ee13ffff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 918 in heartbeat_checker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007ee14bfff640 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py", line 799 in recv_multipart
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 888 in decode_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007ee187fff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007ee1ffdff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f4f65ec4480 (most recent call first):
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 940 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 944 in custom_wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 197 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 319 in outer_wrapper_dummy
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 281 in wrapper_custom
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 281 in all_reduce
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 314 in custom_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 635 in _all_reduce_out_place
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 156 in outplace_all_reduce
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 616 in all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13 in tensor_model_parallel_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 910 in forward_normal
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 793 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 2874 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3151 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line , 3340_cffi_backend in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2568 in forward_decode
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2700 in _forward_raw
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2653 in forward
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 392 in forward_batch_generation
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2005 in run_batch
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/decode.py", line 825 in event_loop_overlap_disagg_decode
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2697 in run_scheduler_process
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108 in run
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", line 1 in <module>
, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls
Extension modules: numpy.core._multiarray_umath, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, numpy.core._multiarray_tests, scipy.special._ufuncs, scipy.special._specfun, numpy.linalg._umath_linalg, scipy.special._comb, scipy.special._ellip_harm_2, , scipy.linalg._decomp_interpolativenumpy.fft._pocketfft_internal, scipy.optimize._bglu_dense, scipy.optimize._lsap, numpy.random._common, scipy.spatial._ckdtree, , scipy.spatial._qhullnumpy.random.bit_generator, , numpy.random._bounded_integersscipy.spatial._voronoi, numpy.random._mt19937, scipy.spatial._distance_wrap, numpy.random.mtrand, , scipy.spatial._hausdorffnumpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, scipy.spatial.transform._rotation, numpy.random._generator, scipy.optimize._direct, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, pyzstd._c._zstd, torch._C._sparse, torch._C._special, sklearn.__check_build._check_build, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, , pandas._libs.intervalpsutil._psutil_linux, , pandas._libs.libpsutil._psutil_posix, pybase64._pybase64, pyarrow._compute, pandas._libs.ops, charset_normalizer.md, pandas._libs.hashing, pandas._libs.arrays, requests.packages.charset_normalizer.md, requests.packages.chardet.md, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, zmq.backend.cython._zmq, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, PIL._imaging, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, cython.cimports.libc.math, setproctitle._setproctitle, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.Compiler.Scanning, sentencepiece._sentencepiece, Cython.StringIOTree, , Cython.Compiler.Coderegex._regex, yaml._yaml, markupsafe._speedups, hiredis.hiredis, _cbor2, PIL._imagingft, msgspec._core, hip_utils, _cffi_backend, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, __triton_launcher, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack (total: 211)
, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, pyzstd._c._zstd, sklearn.__check_build._check_build, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, setproctitle._setproctitle, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.Compiler.Scanning, Cython.StringIOTree, Cython.Compiler.Code, hiredis.hiredis, _cbor2, msgspec._core, hip_utils, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, __triton_launcher (total: 211)
[rank1]:[E1209 16:33:02.328102732 ProcessGroupNCCL.cpp:735] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E1209 16:33:02.328132802 ProcessGroupNCCL.cpp:749] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E1209 16:33:02.329408867 ProcessGroupNCCL.cpp:2055] [PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600031 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7fd2b93f333c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7fd2f2d23400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7fd2f2d27ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7fd2f2d29147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7fd2b78bb253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fd30621fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fd3062b1850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600031 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7fd2b93f333c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7fd2f2d23400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7fd2f2d27ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7fd2f2d29147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7fd2b78bb253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fd30621fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fd3062b1850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2061 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7fd2b93f333c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x292b742 (0x7fd2f2cff742 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x857dd5 (0x7fd2f0c2bdd5 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xdc253 (0x7fd2b78bb253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7fd30621fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7fd3062b1850 in /lib/x86_64-linux-gnu/libc.so.6)

Fatal Python error: Aborted

Thread 0x00007f6347fff640 (most recent call first):
  <no Python frame>

Thread 0x00007f64bffff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f64cbfff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 918 in heartbeat_checker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f64d7fff640 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py", line 799 in recv_multipart
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 888 in decode_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f654ffff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f65943ff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007fd30618a480 (most recent call first):
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 940 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 944 in custom_wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 197 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 319 in outer_wrapper_dummy
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 281 in wrapper_custom
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 281 in all_reduce
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 314 in custom_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 635 in _all_reduce_out_place
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 156 in outplace_all_reduce
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 616 in all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13 in tensor_model_parallel_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 910 in forward_normal
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 793 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 2874 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3151 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3340 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2568 in forward_decode
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2700 in _forward_raw
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2653 in forward
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 392 in forward_batch_generation
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2005 in run_batch
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/decode.py", line 825 in event_loop_overlap_disagg_decode
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2697 in run_scheduler_process
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108 in run
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", line 1 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, pybase64._pybase64, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, zmq.backend.cython._zmq, PIL._imaging, cython.cimports.libc.math, sentencepiece._sentencepiece, regex._regex, yaml._yaml, markupsafe._speedups, PIL._imagingft, _cffi_backend, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, pyzstd._c._zstd, sklearn.__check_build._check_build, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, setproctitle._setproctitle, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.Compiler.Scanning, Cython.StringIOTree, Cython.Compiler.Code, hiredis.hiredis, _cbor2, msgspec._core, hip_utils, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, __triton_launcher (total: 211)
[rank0]:[E1209 16:33:02.383038809 ProcessGroupNCCL.cpp:735] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E1209 16:33:02.383061219 ProcessGroupNCCL.cpp:749] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E1209 16:33:02.384294710 ProcessGroupNCCL.cpp:2055] [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f4d23bd133c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f4d5d501400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f4d5d505ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f4d5d507147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f4d22099253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f4d709fdac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f4d70a8f850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=133, OpType=_ALLGATHER_BASE, NumelIn=32320, NumelOut=129280, Timeout(ms)=600000) ran for 600073 milliseconds before timing out.
Exception raised from checkTimeout at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:677 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f4d23bd133c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x260 (0x7f4d5d501400 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1729 (0x7f4d5d505ba9 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x117 (0x7f4d5d507147 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #4: <unknown function> + 0xdc253 (0x7f4d22099253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f4d709fdac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f4d70a8f850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /app/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2061 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9c (0x7f4d23bd133c in /opt/venv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x292b742 (0x7f4d5d4dd742 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #2: <unknown function> + 0x857dd5 (0x7f4d5b409dd5 in /opt/venv/lib/python3.10/site-packages/torch/lib/libtorch_hip.so)
frame #3: <unknown function> + 0xdc253 (0x7f4d22099253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f4d709fdac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126850 (0x7f4d70a8f850 in /lib/x86_64-linux-gnu/libc.so.6)

Fatal Python error: Aborted

Thread 0x00007eddc3fff640 (most recent call first):
  <no Python frame>

Thread 0x00007edf23fff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007edf47fff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 918 in heartbeat_checker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007edf6bfff640 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py", line 799 in recv_multipart
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 888 in decode_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007edd87fff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007edfee1ff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f4d70968480 (most recent call first):
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 940 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/core.py", line 944 in custom_wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 197 in wrapper
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 319 in outer_wrapper_dummy
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/aiter/aiter/jit/utils/torch_guard.py", line 281 in wrapper_custom
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 281 in all_reduce
  File "/sgl-workspace/aiter/aiter/dist/device_communicators/custom_all_reduce.py", line 314 in custom_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 635 in _all_reduce_out_place
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 156 in outplace_all_reduce
  File "/opt/venv/lib/python3.10/site-packages/torch/_ops.py", line 1254 in __call__
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 616 in all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13 in tensor_model_parallel_all_reduce
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 910 in forward_normal
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 793 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 2874 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3151 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
  File "/opt/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
  File "/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py", line 3340 in forward
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2568 in forward_decode
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2700 in _forward_raw
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 2653 in forward
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 392 in forward_batch_generation
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2005 in run_batch
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/decode.py", line 825 in event_loop_overlap_disagg_decode
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2697 in run_scheduler_process
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108 in run
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", line 1 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, pybase64._pybase64, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, zmq.backend.cython._zmq, PIL._imaging, cython.cimports.libc.math, sentencepiece._sentencepiece, regex._regex, yaml._yaml, markupsafe._speedups, PIL._imagingft, _cffi_backend, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, pyzstd._c._zstd, sklearn.__check_build._check_build, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, setproctitle._setproctitle, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.Compiler.Scanning, Cython.StringIOTree, Cython.Compiler.Code, hiredis.hiredis, _cbor2, msgspec._core, hip_utils, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, __triton_launcher (total: 211)
[2025-12-09 16:33:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:33:32. last_heartbeat time: 16:22:00
[2025-12-09 16:34:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:34:32. last_heartbeat time: 16:22:00
[2025-12-09 16:35:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:35:32. last_heartbeat time: 16:22:00
[2025-12-09 16:36:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:36:32. last_heartbeat time: 16:22:00
[2025-12-09 16:37:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:37:32. last_heartbeat time: 16:22:00
[2025-12-09 16:38:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:38:32. last_heartbeat time: 16:22:00
[2025-12-09 16:39:53] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:39:32. last_heartbeat time: 16:22:00
[2025-12-09 16:40:01] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-12-09 16:40:03] Signal SIGTERM received while health check failed. Force exiting.
[2025-12-09 16:40:03] ERROR:    Traceback (most recent call last):
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "uvloop/loop.pyx", line 1512, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1505, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1379, in uvloop.loop.Loop.run_forever
  File "uvloop/loop.pyx", line 557, in uvloop.loop.Loop._run
  File "uvloop/loop.pyx", line 476, in uvloop.loop.Loop._on_idle
  File "uvloop/cbhandles.pyx", line 83, in uvloop.loop.Handle._run
  File "uvloop/cbhandles.pyx", line 63, in uvloop.loop.Handle._run
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 2474, in print_exception_wrapper
    await func()
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 1538, in sigterm_watchdog
    kill_process_tree(os.getpid(), include_parent=True)
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 1072, in kill_process_tree
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 701, in lifespan
    await receive()
  File "/opt/venv/lib/python3.10/site-packages/uvicorn/lifespan/on.py", line 137, in receive
    return await self.receive_queue.get()
  File "/usr/lib/python3.10/asyncio/queues.py", line 159, in get
    await getter
asyncio.exceptions.CancelledError

[2025-12-09 16:40:03] ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "uvloop/loop.pyx", line 1512, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1505, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1379, in uvloop.loop.Loop.run_forever
  File "uvloop/loop.pyx", line 557, in uvloop.loop.Loop._run
  File "uvloop/loop.pyx", line 476, in uvloop.loop.Loop._on_idle
  File "uvloop/cbhandles.pyx", line 83, in uvloop.loop.Handle._run
  File "uvloop/cbhandles.pyx", line 63, in uvloop.loop.Handle._run
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 2474, in print_exception_wrapper
    await func()
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 1538, in sigterm_watchdog
    kill_process_tree(os.getpid(), include_parent=True)
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 1072, in kill_process_tree
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py", line 409, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/opt/venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/fastapi/applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/middleware/cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
[2025-12-09 16:40:03] INFO:     10.194.129.138:43744 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 78, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/opt/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/opt/venv/lib/python3.10/site-packages/starlette/routing.py", line 75, in app
    response = await f(request)
  File "/opt/venv/lib/python3.10/site-packages/fastapi/routing.py", line 302, in app
    raw_response = await run_endpoint_function(
  File "/opt/venv/lib/python3.10/site-packages/fastapi/routing.py", line 213, in run_endpoint_function
    return await dependant.call(**values)
  File "/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py", line 1131, in openai_v1_completions
    return await raw_request.app.state.openai_serving_completion.handle_request(
  File "/sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/serving_base.py", line 112, in handle_request
    return await self._handle_non_streaming_request(
  File "/sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/serving_completions.py", line 348, in _handle_non_streaming_request
    ret = await generator.__anext__()
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 526, in generate_request
    async for response in self._wait_one_response(obj, state, request):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py", line 1054, in _wait_one_response
    await asyncio.wait_for(state.event.wait(), timeout=4)
  File "/usr/lib/python3.10/asyncio/tasks.py", line 432, in wait_for
    await waiter
asyncio.exceptions.CancelledError
