merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-23 00:44:38 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-23 00:44:39] WARNING server_args.py:1787: Cuda graph is disabled for prefill server
[2025-11-23 00:44:39] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='10.194.129.138', port=30025, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.88, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=822976538, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=180.0, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=True, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='prefill', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=4, disaggregation_decode_dp=1, disaggregation_prefill_pp=1, disaggregation_ib_device='lo', disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, hooks=None)
[2025-11-23 00:44:40] Using default HuggingFace chat template with detected content format: string
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-23 00:44:49 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-23 00:44:49 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-23 00:44:49 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-23 00:44:49 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-23 00:44:49 [__init__.py:241] Automatically detected platform rocm.
[2025-11-23 00:44:49 TP0] Process 159 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-23 00:44:50 TP0] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-23 00:44:50 TP1] Process 160 gpu_id 1 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-23 00:44:50 TP3] Process 162 gpu_id 3 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-23 00:44:50 TP2] Process 161 gpu_id 2 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-23 00:44:50 TP1] Init torch distributed begin.
[2025-11-23 00:44:50 TP3] Init torch distributed begin.
[2025-11-23 00:44:50 TP2] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-11-23 00:44:50 TP0] sglang is using nccl==2.26.6
[2025-11-23 00:44:59 TP2] Using AiterCustomAllreduce for ROCm.
[2025-11-23 00:44:59 TP3] Using AiterCustomAllreduce for ROCm.
[2025-11-23 00:44:59 TP1] Using AiterCustomAllreduce for ROCm.
[2025-11-23 00:44:59 TP0] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-11-23 00:44:59 TP0] Init torch distributed ends. mem usage=1.21 GB
[2025-11-23 00:44:59 TP3] Init torch distributed ends. mem usage=1.16 GB
[2025-11-23 00:44:59 TP2] Init torch distributed ends. mem usage=1.21 GB
[2025-11-23 00:44:59 TP1] Init torch distributed ends. mem usage=1.22 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-11-23 00:45:00 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-23 00:45:00 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-23 00:45:00 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-23 00:45:00 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-23 00:45:01 TP1] Load weight begin. avail mem=190.21 GB
[2025-11-23 00:45:01 TP0] Load weight begin. avail mem=190.22 GB
[2025-11-23 00:45:01 TP0] Detected fp8 checkpoint.
[2025-11-23 00:45:01 TP2] Load weight begin. avail mem=190.22 GB
[2025-11-23 00:45:01 TP3] Load weight begin. avail mem=190.27 GB
[2025-11-23 00:45:01 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:20,  7.76it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:38,  4.11it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:35,  4.49it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:00<00:21,  7.18it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:16,  9.21it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:17,  8.99it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:14, 10.76it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:01<00:12, 12.09it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:01<00:13, 10.84it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:01<00:12, 12.06it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:02<00:11, 12.66it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:02<00:12, 11.16it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:02<00:11, 12.29it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:02<00:19,  7.17it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:03<00:17,  7.86it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:03<00:14,  8.89it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:03<00:10, 11.86it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:03<00:09, 13.70it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:03<00:10, 12.38it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:03<00:09, 13.31it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:04<00:08, 14.25it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:04<00:09, 12.07it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:04<00:08, 14.38it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:04<00:08, 12.67it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:04<00:08, 13.76it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:04<00:07, 14.91it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:05<00:14,  7.37it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:05<00:12,  8.74it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:05<00:10, 10.14it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:06<00:10, 10.02it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:06<00:07, 12.60it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:06<00:06, 14.73it/s]
Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:06<00:07, 13.04it/s]
Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:06<00:06, 13.53it/s]
Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:06<00:06, 14.06it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:07<00:07, 11.92it/s]
Loading safetensors checkpoint shards:  48% Completed | 79/163 [00:07<00:05, 14.48it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:07<00:06, 12.85it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:07<00:05, 13.90it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:07<00:05, 14.84it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:07<00:04, 15.80it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:07<00:05, 13.35it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:08<00:04, 15.79it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:08<00:10,  6.67it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:08<00:08,  7.99it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:09<00:07,  9.19it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:09<00:06,  9.10it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:09<00:05, 10.81it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:09<00:04, 13.47it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:09<00:04, 11.89it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:09<00:04, 12.70it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:10<00:04, 11.72it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:10<00:03, 13.98it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:10<00:03, 15.14it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:10<00:03, 13.77it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:10<00:02, 14.81it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:10<00:02, 15.71it/s]
Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:10<00:02, 13.53it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:11<00:02, 14.52it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:11<00:01, 16.87it/s]
Loading safetensors checkpoint shards:  81% Completed | 132/163 [00:11<00:02, 14.35it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:11<00:01, 16.47it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:11<00:01, 17.00it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:12<00:03,  6.42it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:12<00:02,  7.63it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:12<00:02,  7.92it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:12<00:01,  9.45it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:13<00:01, 10.72it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:13<00:01, 10.08it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:13<00:01, 11.71it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:13<00:00, 13.10it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:13<00:00, 11.29it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:13<00:00, 12.70it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:13<00:00, 14.11it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:14<00:00, 12.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00, 13.47it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00, 11.38it/s]

[2025-11-23 00:46:13 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.19 GB, mem usage=158.02 GB.
[2025-11-23 00:46:14 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-11-23 00:46:14 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.25 GB, mem usage=158.02 GB.
[2025-11-23 00:46:14 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-11-23 00:46:14 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-23 00:46:14 TP0] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-23 00:46:14 TP0] Memory pool end. avail mem=21.54 GB
[2025-11-23 00:46:14 TP1] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-23 00:46:14 TP1] Memory pool end. avail mem=21.53 GB
[2025-11-23 00:46:14 TP3] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-23 00:46:14 TP3] Memory pool end. avail mem=21.59 GB
[2025-11-23 00:46:14 TP2] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-23 00:46:14 TP2] Memory pool end. avail mem=21.54 GB
[2025-11-23 00:46:19 TP0] max_total_num_tokens=143003, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=2048, context_len=163840, available_gpu_mem=21.22 GB
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1123 00:46:19.441144   161 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1123 00:46:19.441169   161 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1123 00:46:19.441191   161 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16204
I1123 00:46:19.441251   161 transfer_engine.cpp:185] Auto-discovering topology...
W1123 00:46:19.441288   161 topology.cpp:55] No RDMA devices found, check your device installation
I1123 00:46:19.441329   161 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1123 00:46:19.441349   161 tcp_transport.cpp:299] TcpTransport: listen on port 15786
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1123 00:46:19.593080   160 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1123 00:46:19.593099   160 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1123 00:46:19.593122   160 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15044
I1123 00:46:19.593171   160 transfer_engine.cpp:185] Auto-discovering topology...
W1123 00:46:19.593205   160 topology.cpp:55] No RDMA devices found, check your device installation
I1123 00:46:19.593240   160 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1123 00:46:19.593254   160 tcp_transport.cpp:299] TcpTransport: listen on port 15143
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1123 00:46:19.597213   162 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1123 00:46:19.597229   162 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1123 00:46:19.597250   162 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16805
I1123 00:46:19.597301   162 transfer_engine.cpp:185] Auto-discovering topology...
W1123 00:46:19.597332   162 topology.cpp:55] No RDMA devices found, check your device installation
I1123 00:46:19.597363   162 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1123 00:46:19.597378   162 tcp_transport.cpp:299] TcpTransport: listen on port 16748
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1123 00:46:19.602442   159 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1123 00:46:19.602461   159 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1123 00:46:19.602483   159 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15239
I1123 00:46:19.602527   159 transfer_engine.cpp:185] Auto-discovering topology...
W1123 00:46:19.602558   159 topology.cpp:55] No RDMA devices found, check your device installation
I1123 00:46:19.602591   159 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1123 00:46:19.602605   159 tcp_transport.cpp:299] TcpTransport: listen on port 15283
[2025-11-23 00:46:19] INFO:     Started server process [1]
[2025-11-23 00:46:19] INFO:     Waiting for application startup.
[2025-11-23 00:46:19] INFO:     Application startup complete.
[2025-11-23 00:46:19] INFO:     Uvicorn running on http://10.194.129.138:30025 (Press CTRL+C to quit)
[2025-11-23 00:46:20] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:20] INFO:     10.194.129.138:34054 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-23 00:46:20] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:20] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-23 00:46:20] INFO:     10.194.129.138:34074 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-23 00:46:20] Start of pd disaggregation warmup ...
[2025-11-23 00:46:20 TP0] Prefill batch, #new-seq: 1, #new-token: 4, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.00, 
[2025-11-23 00:46:25] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:25] INFO:     10.194.129.138:33360 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-23 00:46:25] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:30] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:30] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:30] INFO:     10.194.129.138:33390 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-23 00:46:35] INFO:     10.194.129.138:48408 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-23 00:46:35] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:35] WARNING:  Invalid HTTP request received.
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-23 00:46:39 TP0] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-23 00:46:39 TP0] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-23 00:46:39 TP1] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-23 00:46:39 TP1] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-23 00:46:40 TP3] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-23 00:46:40 TP3] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-11-23 00:46:40] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:40] INFO:     10.194.129.138:48450 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-23 00:46:40] WARNING:  Invalid HTTP request received.
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-23 00:46:41 TP2] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-23 00:46:41 TP2] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-11-23 00:46:42] INFO:     10.194.129.138:34086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-23 00:46:42] End of prefill disaggregation mode warmup with status 200, resp: [{'text': ' ', 'output_ids': [223], 'meta_info': {'id': '9c45b6a022034ab0a552c9ca1799adf5', 'finish_reason': {'type': 'length', 'length': 0}, 'prompt_tokens': 4, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 1, 'cached_tokens': 0, 'e2e_latency': 21.451757669448853, 'response_sent_to_client_ts': 1763858802.1328187}}]
[2025-11-23 00:46:42] The server is fired up and ready to roll!
[2025-11-23 00:46:45] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:45] WARNING:  Invalid HTTP request received.
[2025-11-23 00:46:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.16, 
[2025-11-23 00:46:49] INFO:     10.194.129.138:53938 - "GET /health HTTP/1.1" 200 OK
[2025-11-23 00:46:49] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-11-23 00:46:49] INFO:     10.194.129.138:53972 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-23 00:47:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.03, 
[2025-11-23 00:47:21] INFO:     10.194.129.138:50698 - "GET /health HTTP/1.1" 200 OK
[2025-11-23 00:48:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-23 00:48:21] INFO:     10.194.129.138:52642 - "GET /health HTTP/1.1" 200 OK
[2025-11-23 00:49:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-23 00:49:21] INFO:     10.194.129.138:48696 - "GET /health HTTP/1.1" 200 OK
[2025-11-23 00:50:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-23 00:50:10] INFO:     10.194.129.138:35892 - "GET /health HTTP/1.1" 200 OK
[2025-11-23 00:50:13] WARNING:  Invalid HTTP request received.
[2025-11-23 00:50:13] WARNING:  Invalid HTTP request received.
[2025-11-23 00:50:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.24, 
[2025-11-23 00:50:14] INFO:     10.194.129.138:35898 - "GET /health HTTP/1.1" 200 OK
[2025-11-23 00:50:14] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-11-23 00:50:14] INFO:     10.194.129.138:58030 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-23 00:50:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.08, 
[2025-11-23 00:50:29] INFO:     10.194.129.138:49142 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-23 00:50:30 TP0] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.32, 
[2025-11-23 00:50:30] INFO:     10.194.129.138:49142 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-23 00:50:36 TP0] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.91, 
[2025-11-23 00:50:36] INFO:     10.194.129.138:52048 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-23 00:50:43 TP0] Prefill batch, #new-seq: 3, #new-token: 2148, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 1.82, 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-23 00:50:45 TP2] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-23 00:50:45 TP1] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-23 00:50:45 TP0] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-23 00:50:45 TP3] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-23 00:50:45 TP0] Prefill batch, #new-seq: 13, #new-token: 9545, #cached-token: 13, token usage: 0.02, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 1268.16, 
Memory access fault by GPU node-2 (Agent handle: 0x5572a3fd8a10) on address 0x7ec84b70c000. Reason: Unknown.
[2025-11-23 00:51:35] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 00:51:14. last_heartbeat time: 00:50:36
[2025-11-23 00:52:34] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 00:52:14. last_heartbeat time: 00:50:36
[2025-11-23 00:53:34] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 00:53:14. last_heartbeat time: 00:50:36
[2025-11-23 00:54:34] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 00:54:14. last_heartbeat time: 00:50:36
[2025-11-23 00:55:34] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 00:55:14. last_heartbeat time: 00:50:36
[2025-11-23 00:56:34] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 00:56:14. last_heartbeat time: 00:50:36
[2025-11-23 00:57:34] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 00:57:14. last_heartbeat time: 00:50:36
[2025-11-23 00:58:34] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 00:58:14. last_heartbeat time: 00:50:36
[2025-11-23 00:59:34] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 00:59:14. last_heartbeat time: 00:50:36
[2025-11-23 01:00:34] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 01:00:14. last_heartbeat time: 00:50:36
[rank3]:[E1123 01:00:47.895795725 ProcessGroupNCCL.cpp:674] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=108, OpType=ALLREDUCE, NumelIn=68418560, NumelOut=68418560, Timeout(ms)=600000) ran for 600019 milliseconds before timing out.
[rank3]:[E1123 01:00:47.896935351 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 3]  failure detected by watchdog at work sequence id: 108 PG status: last enqueued work: 135, last completed work: 107
[rank3]:[E1123 01:00:47.896950854 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E1123 01:00:47.896987464 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 3] First PG on this rank to signal dumping.
[rank2]:[E1123 01:00:47.899151466 ProcessGroupNCCL.cpp:674] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=108, OpType=ALLREDUCE, NumelIn=68418560, NumelOut=68418560, Timeout(ms)=600000) ran for 600023 milliseconds before timing out.
[rank2]:[E1123 01:00:47.899249499 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 2]  failure detected by watchdog at work sequence id: 108 PG status: last enqueued work: 135, last completed work: 107
[rank2]:[E1123 01:00:47.899258147 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E1123 01:00:47.899291578 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 2] First PG on this rank to signal dumping.
[rank1]:[E1123 01:00:47.905235525 ProcessGroupNCCL.cpp:674] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=108, OpType=ALLREDUCE, NumelIn=68418560, NumelOut=68418560, Timeout(ms)=600000) ran for 600029 milliseconds before timing out.
[rank1]:[E1123 01:00:47.905389994 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 1]  failure detected by watchdog at work sequence id: 108 PG status: last enqueued work: 135, last completed work: 107
[rank1]:[E1123 01:00:47.905398829 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1123 01:00:47.905421442 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 1] First PG on this rank to signal dumping.
[rank0]:[E1123 01:00:47.972518963 ProcessGroupNCCL.cpp:674] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=108, OpType=ALLREDUCE, NumelIn=68418560, NumelOut=68418560, Timeout(ms)=600000) ran for 600097 milliseconds before timing out.
[rank0]:[E1123 01:00:47.972682281 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 0]  failure detected by watchdog at work sequence id: 108 PG status: last enqueued work: 135, last completed work: 107
[rank0]:[E1123 01:00:47.972695100 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E1123 01:00:47.972725272 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 0] First PG on this rank to signal dumping.
[rank1]:[E1123 01:00:47.289388837 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E1123 01:00:47.289538681 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank3]:[E1123 01:00:47.332951100 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[E1123 01:00:47.333080198 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank2]:[E1123 01:00:47.496763753 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank2]:[E1123 01:00:47.496886766 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E1123 01:00:47.497114547 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E1123 01:00:47.504179664 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
