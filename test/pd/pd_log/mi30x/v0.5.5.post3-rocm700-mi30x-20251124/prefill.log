merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-24 16:15:56 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-24 16:15:57] WARNING server_args.py:1793: Cuda graph is disabled for prefill server
[2025-11-24 16:15:57] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='10.194.129.138', port=30025, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.88, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=116037975, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=180.0, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=True, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='prefill', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=4, disaggregation_decode_dp=1, disaggregation_prefill_pp=1, disaggregation_ib_device='lo', disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, hooks=None)
[2025-11-24 16:15:58] Using default HuggingFace chat template with detected content format: string
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-24 16:16:07 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-24 16:16:07 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-24 16:16:07 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-24 16:16:07 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-24 16:16:07 [__init__.py:241] Automatically detected platform rocm.
[2025-11-24 16:16:07 TP2] Process 161 gpu_id 2 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-24 16:16:07 TP0] Process 159 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-24 16:16:08 TP2] Init torch distributed begin.
[2025-11-24 16:16:08 TP0] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-24 16:16:08 TP1] Process 160 gpu_id 1 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-24 16:16:08 TP3] Process 162 gpu_id 3 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-11-24 16:16:08 TP1] Init torch distributed begin.
[2025-11-24 16:16:08 TP3] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-11-24 16:16:09 TP0] sglang is using nccl==2.26.6
[2025-11-24 16:16:17 TP2] Using AiterCustomAllreduce for ROCm.
[2025-11-24 16:16:17 TP3] Using AiterCustomAllreduce for ROCm.
[2025-11-24 16:16:17 TP0] Using AiterCustomAllreduce for ROCm.
[2025-11-24 16:16:17 TP1] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-11-24 16:16:17 TP0] Init torch distributed ends. mem usage=1.21 GB
[2025-11-24 16:16:17 TP3] Init torch distributed ends. mem usage=1.16 GB
[2025-11-24 16:16:17 TP2] Init torch distributed ends. mem usage=1.21 GB
[2025-11-24 16:16:17 TP1] Init torch distributed ends. mem usage=1.22 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-11-24 16:16:18 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-24 16:16:18 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-24 16:16:18 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-24 16:16:18 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-24 16:16:19 TP0] Load weight begin. avail mem=190.22 GB
[2025-11-24 16:16:19 TP0] Detected fp8 checkpoint.
[2025-11-24 16:16:19 TP1] Load weight begin. avail mem=190.21 GB
[2025-11-24 16:16:19 TP2] Load weight begin. avail mem=190.22 GB
[2025-11-24 16:16:19 TP3] Load weight begin. avail mem=190.27 GB
[2025-11-24 16:16:19 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:21,  7.42it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:37,  4.23it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:29,  5.44it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:17,  8.78it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:14, 10.47it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:15,  9.64it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:01<00:13, 11.05it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:01<00:14, 10.48it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:01<00:12, 11.78it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:02<00:11, 12.52it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:02<00:12, 11.08it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:02<00:11, 12.19it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:02<00:19,  7.11it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:03<00:17,  7.76it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:03<00:15,  8.72it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:03<00:11, 11.53it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:03<00:09, 12.91it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:03<00:10, 11.67it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:03<00:09, 12.82it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:04<00:08, 13.69it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:04<00:10, 11.75it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:04<00:09, 12.72it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:04<00:09, 12.59it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:04<00:08, 13.73it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:04<00:07, 14.91it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:05<00:08, 12.78it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:05<00:15,  7.09it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:05<00:12,  8.58it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:05<00:12,  8.53it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:06<00:09, 10.26it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:06<00:07, 12.38it/s]
Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:06<00:08, 11.68it/s]
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:06<00:07, 12.98it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:06<00:06, 14.11it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:06<00:07, 11.58it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:07<00:06, 12.61it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:07<00:05, 15.48it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:07<00:06, 13.31it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:07<00:05, 15.09it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:07<00:06, 12.81it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:07<00:05, 13.62it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:07<00:05, 14.52it/s]
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:08<00:05, 13.29it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:08<00:10,  6.65it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:08<00:08,  8.10it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:09<00:08,  8.15it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:09<00:06,  9.48it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:09<00:05, 11.17it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:09<00:05, 10.42it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:09<00:04, 12.87it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:10<00:04, 11.08it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:10<00:04, 12.10it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:10<00:03, 14.69it/s]
Loading safetensors checkpoint shards:  71% Completed | 115/163 [00:10<00:03, 13.10it/s]
Loading safetensors checkpoint shards:  72% Completed | 118/163 [00:10<00:02, 15.71it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:10<00:02, 16.02it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:10<00:03, 13.21it/s]
Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:11<00:02, 15.32it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:11<00:02, 15.90it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:11<00:02, 13.78it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:11<00:02, 15.00it/s]
Loading safetensors checkpoint shards:  82% Completed | 134/163 [00:11<00:01, 17.49it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:11<00:01, 13.99it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:12<00:03,  6.63it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:12<00:03,  7.11it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:12<00:02,  8.41it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:13<00:01, 10.81it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:13<00:01, 10.12it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:13<00:01, 11.24it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:13<00:01, 10.54it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:13<00:00, 11.79it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:13<00:00, 12.98it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:14<00:00, 11.68it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:14<00:00, 13.51it/s]
Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:14<00:00, 11.77it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00, 11.19it/s]

[2025-11-24 16:17:31 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.19 GB, mem usage=158.02 GB.
[2025-11-24 16:17:32 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-11-24 16:17:32 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-11-24 16:17:34 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.25 GB, mem usage=158.02 GB.
[2025-11-24 16:17:34 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-24 16:17:34 TP3] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-24 16:17:34 TP0] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-24 16:17:34 TP3] Memory pool end. avail mem=21.59 GB
[2025-11-24 16:17:34 TP0] Memory pool end. avail mem=21.54 GB
[2025-11-24 16:17:34 TP2] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-24 16:17:34 TP1] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-24 16:17:34 TP2] Memory pool end. avail mem=21.54 GB
[2025-11-24 16:17:34 TP1] Memory pool end. avail mem=21.53 GB
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1124 16:17:37.757609   161 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1124 16:17:37.757629   161 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1124 16:17:37.757652   161 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16595
I1124 16:17:37.757706   161 transfer_engine.cpp:185] Auto-discovering topology...
W1124 16:17:37.757740   161 topology.cpp:55] No RDMA devices found, check your device installation
I1124 16:17:37.757773   161 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1124 16:17:37.757788   161 tcp_transport.cpp:299] TcpTransport: listen on port 16943
[2025-11-24 16:17:38 TP0] max_total_num_tokens=143003, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=2048, context_len=163840, available_gpu_mem=21.22 GB
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1124 16:17:38.638720   162 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1124 16:17:38.638739   162 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1124 16:17:38.638762   162 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16423
I1124 16:17:38.638810   162 transfer_engine.cpp:185] Auto-discovering topology...
W1124 16:17:38.638844   162 topology.cpp:55] No RDMA devices found, check your device installation
I1124 16:17:38.638878   162 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1124 16:17:38.638895   162 tcp_transport.cpp:299] TcpTransport: listen on port 16067
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1124 16:17:38.643954   160 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1124 16:17:38.643973   160 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1124 16:17:38.643996   160 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16536
I1124 16:17:38.644048   160 transfer_engine.cpp:185] Auto-discovering topology...
W1124 16:17:38.644084   160 topology.cpp:55] No RDMA devices found, check your device installation
I1124 16:17:38.644119   160 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1124 16:17:38.644135   160 tcp_transport.cpp:299] TcpTransport: listen on port 16794
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1124 16:17:38.644732   159 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1124 16:17:38.644752   159 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1124 16:17:38.644774   159 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15900
I1124 16:17:38.644824   159 transfer_engine.cpp:185] Auto-discovering topology...
W1124 16:17:38.644858   159 topology.cpp:55] No RDMA devices found, check your device installation
I1124 16:17:38.644893   159 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1124 16:17:38.644907   159 tcp_transport.cpp:299] TcpTransport: listen on port 15996
[2025-11-24 16:17:38] INFO:     Started server process [1]
[2025-11-24 16:17:38] INFO:     Waiting for application startup.
[2025-11-24 16:17:38] INFO:     Application startup complete.
[2025-11-24 16:17:38] INFO:     Uvicorn running on http://10.194.129.138:30025 (Press CTRL+C to quit)
[2025-11-24 16:17:39] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-24 16:17:39] INFO:     10.194.129.138:58606 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-24 16:17:39] Start of pd disaggregation warmup ...
[2025-11-24 16:17:39 TP0] Prefill batch, #new-seq: 1, #new-token: 4, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.00, 
[2025-11-24 16:17:43] INFO:     10.194.129.138:58634 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-24 16:17:43] WARNING:  Invalid HTTP request received.
[2025-11-24 16:17:43] WARNING:  Invalid HTTP request received.
[2025-11-24 16:17:48] INFO:     10.194.129.138:35074 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-24 16:17:48] WARNING:  Invalid HTTP request received.
[2025-11-24 16:17:48] WARNING:  Invalid HTTP request received.
[2025-11-24 16:17:53] INFO:     10.194.129.138:35104 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-24 16:17:53] WARNING:  Invalid HTTP request received.
[2025-11-24 16:17:53] WARNING:  Invalid HTTP request received.
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-24 16:17:58 TP2] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-24 16:17:58 TP2] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-11-24 16:17:58] INFO:     10.194.129.138:58942 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-24 16:17:58] WARNING:  Invalid HTTP request received.
[2025-11-24 16:17:58] WARNING:  Invalid HTTP request received.
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-24 16:17:58 TP1] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-24 16:17:58 TP1] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-24 16:17:59 TP0] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-24 16:17:59 TP0] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-24 16:18:00 TP3] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-24 16:18:00 TP3] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-11-24 16:18:01] INFO:     10.194.129.138:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-24 16:18:01] End of prefill disaggregation mode warmup with status 200, resp: [{'text': ' ', 'output_ids': [223], 'meta_info': {'id': '8287488fbe2445cdad07aa7ee43729a3', 'finish_reason': {'type': 'length', 'length': 0}, 'prompt_tokens': 4, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 1, 'cached_tokens': 0, 'e2e_latency': 21.35810685157776, 'response_sent_to_client_ts': 1764001081.0466886}}]
[2025-11-24 16:18:01] The server is fired up and ready to roll!
[2025-11-24 16:18:03] WARNING:  Invalid HTTP request received.
[2025-11-24 16:18:03] WARNING:  Invalid HTTP request received.
[2025-11-24 16:18:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.17, 
[2025-11-24 16:18:07] INFO:     10.194.129.138:58948 - "GET /health HTTP/1.1" 200 OK
[2025-11-24 16:18:07] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-11-24 16:18:07] INFO:     10.194.129.138:36762 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-24 16:18:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.03, 
[2025-11-24 16:18:39] INFO:     10.194.129.138:51030 - "GET /health HTTP/1.1" 200 OK
[2025-11-24 16:19:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-24 16:19:39] INFO:     10.194.129.138:50374 - "GET /health HTTP/1.1" 200 OK
[2025-11-24 16:20:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-24 16:20:39] INFO:     10.194.129.138:41224 - "GET /health HTTP/1.1" 200 OK
[2025-11-24 16:21:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-24 16:21:28] INFO:     10.194.129.138:55284 - "GET /health HTTP/1.1" 200 OK
[2025-11-24 16:21:31] WARNING:  Invalid HTTP request received.
[2025-11-24 16:21:31] WARNING:  Invalid HTTP request received.
[2025-11-24 16:21:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.24, 
[2025-11-24 16:21:32] INFO:     10.194.129.138:55292 - "GET /health HTTP/1.1" 200 OK
[2025-11-24 16:21:32] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-11-24 16:21:32] INFO:     10.194.129.138:55330 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-24 16:21:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.08, 
[2025-11-24 16:21:47] INFO:     10.194.129.138:36358 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-24 16:21:48 TP0] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.32, 
[2025-11-24 16:21:48] INFO:     10.194.129.138:36358 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-24 16:21:54 TP0] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.90, 
[2025-11-24 16:21:54] INFO:     10.194.129.138:47708 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-24 16:22:00 TP0] Prefill batch, #new-seq: 2, #new-token: 1429, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 2.15, 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-24 16:22:02 TP2] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-24 16:22:02 TP3] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-24 16:22:02 TP1] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-24 16:22:02 TP0] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-24 16:22:02 TP0] Prefill batch, #new-seq: 14, #new-token: 10264, #cached-token: 14, token usage: 0.01, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 840.84, 
Memory access fault by GPU node-2 (Agent handle: 0x5559a25a3960) on address 0x7eafedb7b000. Reason: Unknown.
Memory access fault by GPU node-4 (Agent handle: 0x557db5f79e30) on address 0x7eb1da709000. Reason: Unknown.
Failed to create GPU coredump: File exists
GPU core dump failed
Fatal Python error: Aborted

Thread 0x00007eb64bfff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eb657fff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 320 in wait
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/common/utils.py", line 24 in get
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 703 in transfer_worker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eb663fff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 320 in wait
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/common/utils.py", line 24 in get
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 703 in transfer_worker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eb66ffff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 320 in wait
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/common/utils.py", line 24 in get
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 703 in transfer_worker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eb67bfff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 320 in wait
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/common/utils.py", line 24 in get
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 703 in transfer_worker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eb693fff640 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py", line 799 in recv_multipart
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 854 in bootstrap_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eb51bfff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eb72cfff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f24db626480 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/streams.py", line 231 in synchronize
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/prefill.py", line 411 in process_batch_result_disagg_prefill
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/prefill.py", line 374 in event_loop_overlap_disagg_prefill
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2655 in run_scheduler_process
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108 in run
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", line 1 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, pybase64._pybase64, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, zmq.backend.cython._zmq, PIL._imaging, cython.cimports.libc.math, sentencepiece._sentencepiece, regex._regex, yaml._yaml, markupsafe._speedups, PIL._imagingft, _cffi_backend, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, pyzstd._c._zstd, sklearn.__check_build._check_build, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, setproctitle._setproctitle, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.Compiler.Scanning, Cython.StringIOTree, Cython.Compiler.Code, hip_utils, hiredis.hiredis, _cbor2, msgspec._core, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, __triton_launcher (total: 211)
[2025-11-24 16:22:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:22:32. last_heartbeat time: 16:21:54
[2025-11-24 16:23:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:23:32. last_heartbeat time: 16:21:54
[2025-11-24 16:24:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:24:32. last_heartbeat time: 16:21:54
[2025-11-24 16:25:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:25:32. last_heartbeat time: 16:21:54
[2025-11-24 16:26:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:26:32. last_heartbeat time: 16:21:54
[2025-11-24 16:27:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:27:32. last_heartbeat time: 16:21:54
[2025-11-24 16:28:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:28:32. last_heartbeat time: 16:21:54
[2025-11-24 16:29:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:29:32. last_heartbeat time: 16:21:54
[2025-11-24 16:30:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:30:32. last_heartbeat time: 16:21:54
[2025-11-24 16:31:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:31:32. last_heartbeat time: 16:21:54
[rank1]:[E1124 16:32:04.902520145 ProcessGroupNCCL.cpp:674] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=110, OpType=ALLREDUCE, NumelIn=73572352, NumelOut=73572352, Timeout(ms)=600000) ran for 600018 milliseconds before timing out.
[rank1]:[E1124 16:32:04.902685364 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 1]  failure detected by watchdog at work sequence id: 110 PG status: last enqueued work: 135, last completed work: 109
[rank1]:[E1124 16:32:04.902695034 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1124 16:32:04.902723559 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 1] First PG on this rank to signal dumping.
[rank3]:[E1124 16:32:04.903192825 ProcessGroupNCCL.cpp:674] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=110, OpType=ALLREDUCE, NumelIn=73572352, NumelOut=73572352, Timeout(ms)=600000) ran for 600019 milliseconds before timing out.
[rank3]:[E1124 16:32:04.903312546 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 3]  failure detected by watchdog at work sequence id: 110 PG status: last enqueued work: 135, last completed work: 109
[rank3]:[E1124 16:32:04.903321087 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E1124 16:32:04.903348449 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 3] First PG on this rank to signal dumping.
[rank0]:[E1124 16:32:04.966491939 ProcessGroupNCCL.cpp:674] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=110, OpType=ALLREDUCE, NumelIn=73572352, NumelOut=73572352, Timeout(ms)=600000) ran for 600083 milliseconds before timing out.
[rank0]:[E1124 16:32:04.966620944 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 0]  failure detected by watchdog at work sequence id: 110 PG status: last enqueued work: 135, last completed work: 109
[rank0]:[E1124 16:32:04.966630649 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E1124 16:32:04.966652312 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 0] First PG on this rank to signal dumping.
[rank1]:[E1124 16:32:04.428193806 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E1124 16:32:04.428352216 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank3]:[E1124 16:32:04.506262241 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[E1124 16:32:04.506411192 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E1124 16:32:05.805361589 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E1124 16:32:05.812145978 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
