merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-27 16:15:56 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-27 16:15:56] WARNING server_args.py:1829: Cuda graph is disabled for prefill server
[2025-11-27 16:15:57] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='10.194.129.138', port=30025, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.88, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=825424895, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=180.0, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_block_size=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=True, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='prefill', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=4, disaggregation_decode_dp=1, disaggregation_prefill_pp=1, disaggregation_ib_device='lo', disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, forward_hooks=None)
[2025-11-27 16:15:57] Using default HuggingFace chat template with detected content format: string
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-27 16:16:06 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-27 16:16:07 TP1] Process 160 gpu_id 1 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-27 16:16:07 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-27 16:16:07 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-27 16:16:07 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-27 16:16:07 [__init__.py:241] Automatically detected platform rocm.
[2025-11-27 16:16:07 TP1] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-27 16:16:08 TP2] Process 161 gpu_id 2 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-27 16:16:08 TP3] Process 162 gpu_id 3 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-11-27 16:16:08 TP0] Process 159 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-11-27 16:16:08 TP2] Init torch distributed begin.
[2025-11-27 16:16:08 TP3] Init torch distributed begin.
[2025-11-27 16:16:08 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-11-27 16:16:09 TP0] sglang is using nccl==2.26.6
[2025-11-27 16:16:17 TP3] Using AiterCustomAllreduce for ROCm.
[2025-11-27 16:16:17 TP2] Using AiterCustomAllreduce for ROCm.
[2025-11-27 16:16:17 TP0] Using AiterCustomAllreduce for ROCm.
[2025-11-27 16:16:17 TP1] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-11-27 16:16:17 TP3] Init torch distributed ends. mem usage=1.16 GB
[2025-11-27 16:16:17 TP2] Init torch distributed ends. mem usage=1.21 GB
[2025-11-27 16:16:17 TP0] Init torch distributed ends. mem usage=1.21 GB
[2025-11-27 16:16:17 TP1] Init torch distributed ends. mem usage=1.22 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-11-27 16:16:18 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-27 16:16:18 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-27 16:16:18 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-27 16:16:18 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-27 16:16:18 TP1] Load weight begin. avail mem=190.21 GB
[2025-11-27 16:16:18 TP3] Load weight begin. avail mem=190.27 GB
[2025-11-27 16:16:18 TP2] Load weight begin. avail mem=190.22 GB
[2025-11-27 16:16:19 TP0] Load weight begin. avail mem=190.22 GB
[2025-11-27 16:16:19 TP0] Detected fp8 checkpoint.
[2025-11-27 16:16:19 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:31,  5.18it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:47,  3.38it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:24,  6.40it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:21,  7.20it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:18,  8.27it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:14, 10.44it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:12, 11.92it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:01<00:11, 12.68it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:01<00:13, 10.95it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:01<00:11, 12.73it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:01<00:10, 13.26it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:02<00:12, 11.52it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:02<00:11, 12.72it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:02<00:10, 13.66it/s]
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:02<00:15,  8.60it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:02<00:15,  8.63it/s]
Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:03<00:11, 11.84it/s]
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:03<00:09, 13.02it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:03<00:09, 13.56it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:03<00:10, 12.44it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:03<00:09, 12.42it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:03<00:09, 13.30it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:03<00:08, 13.80it/s]
Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:04<00:07, 15.01it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:04<00:08, 13.38it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:04<00:08, 14.08it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:04<00:07, 14.69it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:04<00:08, 13.14it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:05<00:12,  8.33it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:05<00:10,  9.93it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:05<00:10,  9.91it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:05<00:08, 11.30it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:05<00:07, 13.82it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:05<00:07, 13.23it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:06<00:06, 14.88it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:06<00:06, 14.61it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:06<00:06, 12.82it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:06<00:05, 15.55it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:06<00:04, 17.17it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:06<00:05, 14.69it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:07<00:05, 15.16it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:07<00:04, 15.90it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:07<00:05, 14.04it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:07<00:08,  8.82it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:07<00:06, 10.22it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:08<00:06, 10.24it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:08<00:05, 11.21it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:08<00:05, 12.26it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:08<00:04, 13.48it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:08<00:04, 12.99it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:08<00:03, 14.94it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:08<00:03, 15.29it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:09<00:03, 13.58it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:09<00:03, 15.63it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:09<00:02, 16.94it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:09<00:02, 15.08it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:09<00:02, 16.50it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:10<00:02, 17.82it/s]
Loading safetensors checkpoint shards:  79% Completed | 128/163 [00:10<00:02, 15.43it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:10<00:02, 16.11it/s]
Loading safetensors checkpoint shards:  81% Completed | 132/163 [00:10<00:01, 16.88it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:10<00:01, 18.41it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:11<00:03,  8.16it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:11<00:02,  9.51it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:11<00:02, 10.81it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:11<00:01, 10.83it/s]
Loading safetensors checkpoint shards:  90% Completed | 146/163 [00:11<00:01, 13.05it/s]
Loading safetensors checkpoint shards:  91% Completed | 148/163 [00:11<00:01, 14.09it/s]
Loading safetensors checkpoint shards:  92% Completed | 150/163 [00:12<00:01, 12.60it/s]
Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:12<00:00, 13.87it/s]
Loading safetensors checkpoint shards:  94% Completed | 154/163 [00:12<00:00, 14.96it/s]
Loading safetensors checkpoint shards:  96% Completed | 156/163 [00:12<00:00, 15.47it/s]
Loading safetensors checkpoint shards:  97% Completed | 158/163 [00:12<00:00, 13.55it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:12<00:00, 14.15it/s]
Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:12<00:00, 14.66it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:12<00:00, 12.65it/s]

[2025-11-27 16:17:38 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-11-27 16:17:40 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.19 GB, mem usage=158.02 GB.
[2025-11-27 16:17:40 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.25 GB, mem usage=158.02 GB.
[2025-11-27 16:17:40 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-11-27 16:17:40 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-27 16:17:40 TP0] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-27 16:17:40 TP0] Memory pool end. avail mem=21.54 GB
[2025-11-27 16:17:40 TP3] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-27 16:17:40 TP3] Memory pool end. avail mem=21.59 GB
[2025-11-27 16:17:40 TP2] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-27 16:17:40 TP2] Memory pool end. avail mem=21.54 GB
[2025-11-27 16:17:40 TP1] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-27 16:17:40 TP1] Memory pool end. avail mem=21.53 GB
[2025-11-27 16:17:46 TP0] max_total_num_tokens=143003, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=2048, context_len=163840, available_gpu_mem=21.22 GB
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1127 16:17:46.817250   159 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1127 16:17:46.817271   159 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1127 16:17:46.817293   159 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15331
I1127 16:17:46.817338   159 transfer_engine.cpp:185] Auto-discovering topology...
W1127 16:17:46.817373   159 topology.cpp:55] No RDMA devices found, check your device installation
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1127 16:17:46.817376   160 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1127 16:17:46.817394   160 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1127 16:17:46.817410   159 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1127 16:17:46.817418   160 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16120
I1127 16:17:46.817425   159 tcp_transport.cpp:299] TcpTransport: listen on port 16260
I1127 16:17:46.817471   160 transfer_engine.cpp:185] Auto-discovering topology...
W1127 16:17:46.817503   160 topology.cpp:55] No RDMA devices found, check your device installation
I1127 16:17:46.817540   160 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1127 16:17:46.817556   160 tcp_transport.cpp:299] TcpTransport: listen on port 15241
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1127 16:17:46.831593   162 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1127 16:17:46.831612   162 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1127 16:17:46.831638   162 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16880
I1127 16:17:46.831686   162 transfer_engine.cpp:185] Auto-discovering topology...
W1127 16:17:46.831719   162 topology.cpp:55] No RDMA devices found, check your device installation
I1127 16:17:46.831754   162 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1127 16:17:46.831771   162 tcp_transport.cpp:299] TcpTransport: listen on port 16497
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1127 16:17:46.832681   161 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1127 16:17:46.832698   161 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1127 16:17:46.832726   161 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16479
I1127 16:17:46.832785   161 transfer_engine.cpp:185] Auto-discovering topology...
W1127 16:17:46.832819   161 topology.cpp:55] No RDMA devices found, check your device installation
I1127 16:17:46.832855   161 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1127 16:17:46.832872   161 tcp_transport.cpp:299] TcpTransport: listen on port 15553
[2025-11-27 16:17:46] INFO:     Started server process [1]
[2025-11-27 16:17:46] INFO:     Waiting for application startup.
[2025-11-27 16:17:46] INFO:     Application startup complete.
[2025-11-27 16:17:46] INFO:     Uvicorn running on http://10.194.129.138:30025 (Press CTRL+C to quit)
[2025-11-27 16:17:47] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-27 16:17:47] INFO:     10.194.129.138:34640 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-27 16:17:47] Start of pd disaggregation warmup ...
[2025-11-27 16:17:47 TP0] Prefill batch, #new-seq: 1, #new-token: 4, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.00, 
[2025-11-27 16:17:48] INFO:     10.194.129.138:34658 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-27 16:17:48] WARNING:  Invalid HTTP request received.
[2025-11-27 16:17:48] WARNING:  Invalid HTTP request received.
[2025-11-27 16:17:53] WARNING:  Invalid HTTP request received.
[2025-11-27 16:17:53] INFO:     10.194.129.138:34684 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-27 16:17:53] WARNING:  Invalid HTTP request received.
[2025-11-27 16:17:58] INFO:     10.194.129.138:49800 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-27 16:17:58] WARNING:  Invalid HTTP request received.
[2025-11-27 16:17:58] WARNING:  Invalid HTTP request received.
[2025-11-27 16:18:03] WARNING:  Invalid HTTP request received.
[2025-11-27 16:18:03] INFO:     10.194.129.138:49842 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-27 16:18:03] WARNING:  Invalid HTTP request received.
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-27 16:18:08 TP0] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-27 16:18:08 TP0] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-11-27 16:18:08] WARNING:  Invalid HTTP request received.
[2025-11-27 16:18:08] INFO:     10.194.129.138:50082 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-27 16:18:08] WARNING:  Invalid HTTP request received.
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-27 16:18:09 TP1] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-27 16:18:09 TP1] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-27 16:18:10 TP3] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-27 16:18:10 TP3] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-27 16:18:10 TP2] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-27 16:18:10 TP2] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-11-27 16:18:11] INFO:     10.194.129.138:34652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-27 16:18:11] End of prefill disaggregation mode warmup with status 200, resp: [{'text': ' capit', 'output_ids': [49524], 'meta_info': {'id': '724a8a111f5c43f887ffa252b0f44ce3', 'finish_reason': {'type': 'length', 'length': 0}, 'prompt_tokens': 4, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 1, 'cached_tokens': 0, 'e2e_latency': 23.438698530197144, 'response_sent_to_client_ts': 1764260291.317791}}]
[2025-11-27 16:18:11] The server is fired up and ready to roll!
[2025-11-27 16:18:13] WARNING:  Invalid HTTP request received.
[2025-11-27 16:18:13] WARNING:  Invalid HTTP request received.
[2025-11-27 16:18:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.16, 
[2025-11-27 16:18:23] WARNING:  Invalid HTTP request received.
[2025-11-27 16:18:23] WARNING:  Invalid HTTP request received.
[2025-11-27 16:18:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.10, 
[2025-11-27 16:18:24] INFO:     10.194.129.138:47556 - "GET /health HTTP/1.1" 200 OK
[2025-11-27 16:18:24] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-11-27 16:18:24] INFO:     10.194.129.138:54678 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-27 16:18:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.07, 
[2025-11-27 16:18:39] INFO:     10.194.129.138:50398 - "GET /health HTTP/1.1" 200 OK
[2025-11-27 16:19:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-27 16:19:39] INFO:     10.194.129.138:57812 - "GET /health HTTP/1.1" 200 OK
[2025-11-27 16:20:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-27 16:20:39] INFO:     10.194.129.138:51242 - "GET /health HTTP/1.1" 200 OK
[2025-11-27 16:21:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-27 16:21:28] INFO:     10.194.129.138:50058 - "GET /health HTTP/1.1" 200 OK
[2025-11-27 16:21:31] WARNING:  Invalid HTTP request received.
[2025-11-27 16:21:31] WARNING:  Invalid HTTP request received.
[2025-11-27 16:21:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.24, 
[2025-11-27 16:21:32] INFO:     10.194.129.138:50072 - "GET /health HTTP/1.1" 200 OK
[2025-11-27 16:21:32] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-11-27 16:21:32] INFO:     10.194.129.138:50100 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-27 16:21:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.08, 
[2025-11-27 16:21:47] INFO:     10.194.129.138:59580 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-27 16:21:48 TP0] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.32, 
[2025-11-27 16:21:48] INFO:     10.194.129.138:59580 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-27 16:21:55 TP0] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.86, 
[2025-11-27 16:21:55] INFO:     10.194.129.138:46028 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-27 16:22:12 TP0] Prefill batch, #new-seq: 7, #new-token: 5062, #cached-token: 7, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.74, 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-27 16:22:14 TP2] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-27 16:22:14 TP3] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-27 16:22:14 TP0] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-27 16:22:14 TP1] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-27 16:22:14 TP0] Prefill batch, #new-seq: 9, #new-token: 6631, #cached-token: 9, token usage: 0.04, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 2743.06, 
Memory access fault by GPU node-2 (Agent handle: 0x55e661544420) on address 0x7efe59b7d000. Reason: Unknown.
Memory access fault by GPU node-3 (Agent handle: 0x55ef822c08c0) on address 0x7ecd08915000. Reason: Unknown.
[2025-11-27 16:22:51] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:22:31. last_heartbeat time: 16:21:55
[2025-11-27 16:23:51] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:23:31. last_heartbeat time: 16:21:55
[2025-11-27 16:24:51] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:24:31. last_heartbeat time: 16:21:55
[2025-11-27 16:25:51] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:25:31. last_heartbeat time: 16:21:55
[2025-11-27 16:26:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:26:31. last_heartbeat time: 16:21:55
[2025-11-27 16:27:51] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:27:31. last_heartbeat time: 16:21:55
[2025-11-27 16:28:51] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:28:31. last_heartbeat time: 16:21:55
[2025-11-27 16:29:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:29:31. last_heartbeat time: 16:21:55
[2025-11-27 16:30:51] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:30:31. last_heartbeat time: 16:21:55
[2025-11-27 16:31:51] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:31:31. last_heartbeat time: 16:21:55
[rank3]:[E1127 16:32:16.167470622 ProcessGroupNCCL.cpp:674] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=256, OpType=ALLREDUCE, NumelIn=47531008, NumelOut=47531008, Timeout(ms)=600000) ran for 600065 milliseconds before timing out.
[rank3]:[E1127 16:32:16.167607862 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 3]  failure detected by watchdog at work sequence id: 256 PG status: last enqueued work: 259, last completed work: 255
[rank3]:[E1127 16:32:16.167615274 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E1127 16:32:16.167653820 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 3] First PG on this rank to signal dumping.
[rank2]:[E1127 16:32:16.171838635 ProcessGroupNCCL.cpp:674] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=256, OpType=ALLREDUCE, NumelIn=47531008, NumelOut=47531008, Timeout(ms)=600000) ran for 600070 milliseconds before timing out.
[rank2]:[E1127 16:32:16.171975063 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 2]  failure detected by watchdog at work sequence id: 256 PG status: last enqueued work: 259, last completed work: 255
[rank2]:[E1127 16:32:16.171984009 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E1127 16:32:16.172006238 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 2] First PG on this rank to signal dumping.
[rank1]:[E1127 16:32:16.173274623 ProcessGroupNCCL.cpp:674] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=256, OpType=ALLREDUCE, NumelIn=47531008, NumelOut=47531008, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
[rank1]:[E1127 16:32:16.173415328 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 1]  failure detected by watchdog at work sequence id: 256 PG status: last enqueued work: 259, last completed work: 255
[rank1]:[E1127 16:32:16.173424601 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1127 16:32:16.173455210 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 1] First PG on this rank to signal dumping.
[rank0]:[E1127 16:32:16.174102172 ProcessGroupNCCL.cpp:674] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=256, OpType=ALLREDUCE, NumelIn=47531008, NumelOut=47531008, Timeout(ms)=600000) ran for 600072 milliseconds before timing out.
[rank0]:[E1127 16:32:16.174230988 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 0]  failure detected by watchdog at work sequence id: 256 PG status: last enqueued work: 259, last completed work: 255
[rank0]:[E1127 16:32:16.174240666 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E1127 16:32:16.174260556 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 0] First PG on this rank to signal dumping.
[rank2]:[E1127 16:32:16.499885307 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank2]:[E1127 16:32:16.500043444 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank3]:[E1127 16:32:16.691289214 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[E1127 16:32:16.691440672 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank1]:[E1127 16:32:17.847623413 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E1127 16:32:17.847707136 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E1127 16:32:17.852187847 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank1]:[E1127 16:32:17.854789167 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
