merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-29 16:15:56 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-29 16:15:56] WARNING server_args.py:1827: Cuda graph is disabled for prefill server when piecewise cuda graph is not enabled.
[2025-11-29 16:15:57] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='10.194.129.138', port=30025, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.88, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=105647306, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=180.0, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_block_size=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=True, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='prefill', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=4, disaggregation_decode_dp=1, disaggregation_prefill_pp=1, disaggregation_ib_device='lo', disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, forward_hooks=None)
[2025-11-29 16:15:57] Using default HuggingFace chat template with detected content format: string
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-29 16:16:06 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-29 16:16:06 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-29 16:16:06 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-29 16:16:07 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-29 16:16:07 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-29 16:16:07 TP0] Process 159 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-11-29 16:16:07 TP1] Process 160 gpu_id 1 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-29 16:16:07 TP3] Process 162 gpu_id 3 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:63: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-29 16:16:07 TP2] Process 161 gpu_id 2 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-29 16:16:07 TP1] Init torch distributed begin.
[2025-11-29 16:16:07 TP3] Init torch distributed begin.
[2025-11-29 16:16:07 TP0] Init torch distributed begin.
[2025-11-29 16:16:07 TP2] Init torch distributed begin.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-11-29 16:16:08 TP0] sglang is using nccl==2.26.6
[2025-11-29 16:16:17 TP3] Using AiterCustomAllreduce for ROCm.
[2025-11-29 16:16:17 TP2] Using AiterCustomAllreduce for ROCm.
[2025-11-29 16:16:17 TP1] Using AiterCustomAllreduce for ROCm.
[2025-11-29 16:16:17 TP0] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-11-29 16:16:17 TP0] Init torch distributed ends. mem usage=1.21 GB
[2025-11-29 16:16:17 TP3] Init torch distributed ends. mem usage=1.16 GB
[2025-11-29 16:16:17 TP2] Init torch distributed ends. mem usage=1.21 GB
[2025-11-29 16:16:17 TP1] Init torch distributed ends. mem usage=1.22 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-11-29 16:16:18 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-29 16:16:18 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-29 16:16:18 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-29 16:16:18 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-11-29 16:16:18 TP0] Load weight begin. avail mem=190.22 GB
[2025-11-29 16:16:18 TP2] Load weight begin. avail mem=190.22 GB
[2025-11-29 16:16:18 TP1] Load weight begin. avail mem=190.21 GB
[2025-11-29 16:16:18 TP0] Detected fp8 checkpoint.
[2025-11-29 16:16:18 TP3] Load weight begin. avail mem=190.27 GB
[2025-11-29 16:16:18 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:22,  7.28it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:54,  2.94it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:26,  6.10it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:00<00:21,  7.33it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:16,  9.38it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:16,  9.19it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:01<00:14, 10.74it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:01<00:12, 11.97it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:01<00:13, 10.87it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:01<00:12, 11.71it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:02<00:11, 12.55it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:02<00:12, 11.40it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:02<00:11, 12.59it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:02<00:18,  7.27it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:03<00:16,  7.95it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:03<00:14,  9.11it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:03<00:10, 12.29it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:03<00:09, 13.50it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:03<00:10, 11.98it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:03<00:09, 12.94it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:03<00:08, 14.06it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:04<00:10, 11.76it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:04<00:09, 12.39it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:04<00:07, 14.73it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:04<00:08, 12.73it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:04<00:08, 13.82it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:04<00:07, 14.99it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:05<00:15,  6.89it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:05<00:12,  8.26it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:05<00:10,  9.80it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:06<00:10,  9.74it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:06<00:08, 12.01it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:06<00:07, 12.08it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:06<00:06, 13.40it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:06<00:06, 13.55it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:06<00:07, 11.92it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:07<00:05, 14.19it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:07<00:05, 15.31it/s]
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:07<00:05, 13.56it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:07<00:05, 14.48it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:07<00:05, 15.34it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:07<00:05, 13.39it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:07<00:05, 14.50it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:08<00:09,  7.76it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:08<00:08,  8.13it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:08<00:07,  9.41it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:09<00:06, 10.65it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:09<00:06, 10.16it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:09<00:04, 12.69it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:09<00:04, 12.20it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:09<00:04, 12.58it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:09<00:03, 13.88it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:10<00:03, 12.82it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:10<00:03, 14.84it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:10<00:02, 17.02it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:10<00:02, 14.03it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:10<00:02, 15.14it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:10<00:02, 17.18it/s]
Loading safetensors checkpoint shards:  79% Completed | 128/163 [00:11<00:02, 14.45it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:11<00:02, 15.46it/s]
Loading safetensors checkpoint shards:  81% Completed | 132/163 [00:11<00:01, 16.45it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:11<00:01, 14.90it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:11<00:01, 15.95it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:12<00:03,  6.96it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:12<00:02,  7.55it/s]
Loading safetensors checkpoint shards:  88% Completed | 144/163 [00:12<00:01,  9.86it/s]
Loading safetensors checkpoint shards:  90% Completed | 146/163 [00:12<00:01, 11.08it/s]
Loading safetensors checkpoint shards:  91% Completed | 148/163 [00:12<00:01, 10.70it/s]
Loading safetensors checkpoint shards:  92% Completed | 150/163 [00:13<00:01, 11.89it/s]
Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:13<00:00, 13.00it/s]
Loading safetensors checkpoint shards:  94% Completed | 154/163 [00:13<00:00, 11.78it/s]
Loading safetensors checkpoint shards:  96% Completed | 156/163 [00:13<00:00, 12.96it/s]
Loading safetensors checkpoint shards:  97% Completed | 158/163 [00:13<00:00, 14.42it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:13<00:00, 12.65it/s]
Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:13<00:00, 13.52it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00, 11.62it/s]

[2025-11-29 16:17:31 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.19 GB, mem usage=158.02 GB.
[2025-11-29 16:17:32 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-11-29 16:17:37 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.25 GB, mem usage=158.02 GB.
[2025-11-29 16:17:38 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.21 GB, mem usage=158.02 GB.
[2025-11-29 16:17:38 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-29 16:17:38 TP2] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-29 16:17:38 TP0] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-29 16:17:38 TP2] Memory pool end. avail mem=21.54 GB
[2025-11-29 16:17:38 TP0] Memory pool end. avail mem=21.54 GB
[2025-11-29 16:17:38 TP1] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-29 16:17:38 TP1] Memory pool end. avail mem=21.53 GB
[2025-11-29 16:17:38 TP3] KV Cache is allocated. #tokens: 143003, KV size: 9.36 GB
[2025-11-29 16:17:38 TP3] Memory pool end. avail mem=21.59 GB
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1129 16:17:42.763562   161 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1129 16:17:42.763581   161 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1129 16:17:42.763602   161 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16678
I1129 16:17:42.763645   161 transfer_engine.cpp:185] Auto-discovering topology...
W1129 16:17:42.763679   161 topology.cpp:55] No RDMA devices found, check your device installation
I1129 16:17:42.763717   161 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1129 16:17:42.763732   161 tcp_transport.cpp:299] TcpTransport: listen on port 16975
[2025-11-29 16:17:43 TP0] max_total_num_tokens=143003, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=2048, context_len=163840, available_gpu_mem=21.22 GB
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1129 16:17:43.720286   160 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1129 16:17:43.720309   160 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1129 16:17:43.720329   160 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:15616
I1129 16:17:43.720388   160 transfer_engine.cpp:185] Auto-discovering topology...
W1129 16:17:43.720424   160 topology.cpp:55] No RDMA devices found, check your device installation
I1129 16:17:43.720463   160 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1129 16:17:43.720479   160 tcp_transport.cpp:299] TcpTransport: listen on port 15306
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1129 16:17:43.723912   162 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1129 16:17:43.723928   162 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1129 16:17:43.723950   162 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16513
I1129 16:17:43.723996   162 transfer_engine.cpp:185] Auto-discovering topology...
W1129 16:17:43.724030   162 topology.cpp:55] No RDMA devices found, check your device installation
I1129 16:17:43.724063   162 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1129 16:17:43.724078   162 tcp_transport.cpp:299] TcpTransport: listen on port 15824
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1129 16:17:43.725505   159 transfer_engine.cpp:486] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)
I1129 16:17:43.725521   159 transfer_engine.cpp:91] Transfer Engine parseHostNameWithPort. server_name: 10.194.129.138 port: 12001
I1129 16:17:43.725548   159 transfer_engine.cpp:146] Transfer Engine RPC using P2P handshake, listening on 10.194.129.138:16271
I1129 16:17:43.725593   159 transfer_engine.cpp:185] Auto-discovering topology...
W1129 16:17:43.725620   159 topology.cpp:55] No RDMA devices found, check your device installation
I1129 16:17:43.725653   159 transfer_engine.cpp:200] Topology discovery complete. Found 0 HCAs.
I1129 16:17:43.725669   159 tcp_transport.cpp:299] TcpTransport: listen on port 15596
[2025-11-29 16:17:43] INFO:     Started server process [1]
[2025-11-29 16:17:43] INFO:     Waiting for application startup.
[2025-11-29 16:17:43] INFO:     Application startup complete.
[2025-11-29 16:17:43] INFO:     Uvicorn running on http://10.194.129.138:30025 (Press CTRL+C to quit)
[2025-11-29 16:17:44] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-11-29 16:17:44] INFO:     10.194.129.138:36078 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-29 16:17:44] Start of pd disaggregation warmup ...
[2025-11-29 16:17:44 TP0] Prefill batch, #new-seq: 1, #new-token: 4, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.00, 
[2025-11-29 16:17:48] INFO:     10.194.129.138:36094 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-29 16:17:48] WARNING:  Invalid HTTP request received.
[2025-11-29 16:17:48] WARNING:  Invalid HTTP request received.
[2025-11-29 16:17:53] INFO:     10.194.129.138:36124 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-29 16:17:53] WARNING:  Invalid HTTP request received.
[2025-11-29 16:17:53] WARNING:  Invalid HTTP request received.
[2025-11-29 16:17:58] INFO:     10.194.129.138:55114 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-29 16:17:58] WARNING:  Invalid HTTP request received.
[2025-11-29 16:17:58] WARNING:  Invalid HTTP request received.
[2025-11-29 16:18:03] INFO:     10.194.129.138:55138 - "GET /health HTTP/1.1" 503 Service Unavailable
[2025-11-29 16:18:03] WARNING:  Invalid HTTP request received.
[2025-11-29 16:18:03] WARNING:  Invalid HTTP request received.
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-29 16:18:03 TP0] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-29 16:18:03 TP0] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-29 16:18:05 TP3] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-29 16:18:05 TP3] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-29 16:18:05 TP2] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-29 16:18:05 TP2] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-29 16:18:05 TP1] [fused_moe] using 1stage default for (304, 16, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-11-29 16:18:05 TP1] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[2025-11-29 16:18:06] INFO:     10.194.129.138:36088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-29 16:18:06] End of prefill disaggregation mode warmup with status 200, resp: [{'text': 'Munic', 'output_ids': [123368], 'meta_info': {'id': 'f275b77b963a4e9189ae28b60b491866', 'finish_reason': {'type': 'length', 'length': 0}, 'prompt_tokens': 4, 'weight_version': 'default', 'total_retractions': 0, 'completion_tokens': 1, 'cached_tokens': 0, 'e2e_latency': 21.72756052017212, 'response_sent_to_client_ts': 1764433086.4981954}}]
[2025-11-29 16:18:06] The server is fired up and ready to roll!
[2025-11-29 16:18:08] WARNING:  Invalid HTTP request received.
[2025-11-29 16:18:08] WARNING:  Invalid HTTP request received.
[2025-11-29 16:18:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.17, 
[2025-11-29 16:18:13] INFO:     10.194.129.138:49682 - "GET /health HTTP/1.1" 200 OK
[2025-11-29 16:18:18] WARNING:  Invalid HTTP request received.
[2025-11-29 16:18:18] WARNING:  Invalid HTTP request received.
[2025-11-29 16:18:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.10, 
[2025-11-29 16:18:19] INFO:     10.194.129.138:57928 - "GET /health HTTP/1.1" 200 OK
[2025-11-29 16:18:19] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-11-29 16:18:19] INFO:     10.194.129.138:57958 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-29 16:18:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.05, 
[2025-11-29 16:18:39] INFO:     10.194.129.138:42644 - "GET /health HTTP/1.1" 200 OK
[2025-11-29 16:19:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-29 16:19:39] INFO:     10.194.129.138:32924 - "GET /health HTTP/1.1" 200 OK
[2025-11-29 16:20:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-29 16:20:39] INFO:     10.194.129.138:35442 - "GET /health HTTP/1.1" 200 OK
[2025-11-29 16:21:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.02, 
[2025-11-29 16:21:27] INFO:     10.194.129.138:56652 - "GET /health HTTP/1.1" 200 OK
[2025-11-29 16:21:31] WARNING:  Invalid HTTP request received.
[2025-11-29 16:21:31] WARNING:  Invalid HTTP request received.
[2025-11-29 16:21:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.24, 
[2025-11-29 16:21:32] INFO:     10.194.129.138:56668 - "GET /health HTTP/1.1" 200 OK
[2025-11-29 16:21:32] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-11-29 16:21:32] INFO:     10.194.129.138:56692 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-29 16:21:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.07, 
[2025-11-29 16:21:46] INFO:     10.194.129.138:41878 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-29 16:21:47 TP0] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.32, 
[2025-11-29 16:21:47] INFO:     10.194.129.138:41878 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-29 16:21:54 TP0] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.90, 
[2025-11-29 16:21:54] INFO:     10.194.129.138:35640 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-11-29 16:22:07 TP0] Prefill batch, #new-seq: 5, #new-token: 3629, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.99, 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-29 16:22:08 TP2] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-29 16:22:08 TP3] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-29 16:22:08 TP0] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-29 16:22:08 TP1] [fused_moe] using 1stage default for (304, 1024, 7168, 512, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-11-29 16:22:09 TP0] Prefill batch, #new-seq: 11, #new-token: 8064, #cached-token: 11, token usage: 0.03, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 2138.62, 
Memory access fault by GPU node-5 (Agent handle: 0x55c1782d7860) on address 0x7ea8dff1f000. Reason: Unknown.
Failed to enable debug interface, debugger might be already attached.
GPU core dump failed
Fatal Python error: Aborted

Thread 0x00007eacbffff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eaccbfff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 320 in wait
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/common/utils.py", line 24 in get
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 703 in transfer_worker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eacd7fff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 320 in wait
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/common/utils.py", line 24 in get
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 703 in transfer_worker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eace3fff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 320 in wait
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/common/utils.py", line 24 in get
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 703 in transfer_worker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eaceffff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 320 in wait
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/common/utils.py", line 24 in get
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 703 in transfer_worker
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eacfbfff640 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/zmq/sugar/socket.py", line 799 in recv_multipart
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/mooncake/conn.py", line 854 in bootstrap_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eab77fff640 (most recent call first):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler_runtime_checker_mixin.py", line 319 in watchdog_thread
  File "/usr/lib/python3.10/threading.py", line 953 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007eadae5ff640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/opt/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007f1b50471480 (most recent call first):
  File "/opt/venv/lib/python3.10/site-packages/torch/cuda/streams.py", line 231 in synchronize
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/prefill.py", line 411 in process_batch_result_disagg_prefill
  File "/sgl-workspace/sglang/python/sglang/srt/disaggregation/prefill.py", line 374 in event_loop_overlap_disagg_prefill
  File "/opt/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120 in decorate_context
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2678 in run_scheduler_process
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108 in run
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
  File "/usr/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
  File "<string>", line 1 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, pybase64._pybase64, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, zmq.backend.cython._zmq, PIL._imaging, cython.cimports.libc.math, sentencepiece._sentencepiece, regex._regex, yaml._yaml, markupsafe._speedups, PIL._imagingft, _cffi_backend, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, pyzstd._c._zstd, sklearn.__check_build._check_build, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, setproctitle._setproctitle, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.Compiler.Scanning, Cython.StringIOTree, Cython.Compiler.Code, hip_utils, hiredis.hiredis, _cbor2, msgspec._core, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, __triton_launcher (total: 211)
[2025-11-29 16:22:53] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:22:32. last_heartbeat time: 16:21:54
[2025-11-29 16:23:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:23:32. last_heartbeat time: 16:21:54
[2025-11-29 16:24:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:24:32. last_heartbeat time: 16:21:54
[2025-11-29 16:25:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:25:32. last_heartbeat time: 16:21:54
[2025-11-29 16:26:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:26:32. last_heartbeat time: 16:21:54
[2025-11-29 16:27:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:27:32. last_heartbeat time: 16:21:54
[2025-11-29 16:28:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:28:32. last_heartbeat time: 16:21:54
[2025-11-29 16:29:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:29:32. last_heartbeat time: 16:21:54
[2025-11-29 16:30:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:30:32. last_heartbeat time: 16:21:54
[2025-11-29 16:31:52] Health check failed. Server couldn't get a response from detokenizer for last 20 seconds. tic start time: 16:31:32. last_heartbeat time: 16:21:54
[rank2]:[E1129 16:32:10.620821350 ProcessGroupNCCL.cpp:674] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=131, OpType=ALLREDUCE, NumelIn=57802752, NumelOut=57802752, Timeout(ms)=600000) ran for 600002 milliseconds before timing out.
[rank2]:[E1129 16:32:10.620952492 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 2]  failure detected by watchdog at work sequence id: 131 PG status: last enqueued work: 136, last completed work: 130
[rank2]:[E1129 16:32:10.620961425 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E1129 16:32:10.620998305 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 2] First PG on this rank to signal dumping.
[rank0]:[E1129 16:32:10.698763466 ProcessGroupNCCL.cpp:674] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=131, OpType=ALLREDUCE, NumelIn=57802752, NumelOut=57802752, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
[rank0]:[E1129 16:32:10.698914473 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 0]  failure detected by watchdog at work sequence id: 131 PG status: last enqueued work: 136, last completed work: 130
[rank0]:[E1129 16:32:10.698923157 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E1129 16:32:10.698946159 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 0] First PG on this rank to signal dumping.
[rank1]:[E1129 16:32:10.706632115 ProcessGroupNCCL.cpp:674] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=131, OpType=ALLREDUCE, NumelIn=57802752, NumelOut=57802752, Timeout(ms)=600000) ran for 600088 milliseconds before timing out.
[rank1]:[E1129 16:32:10.706766785 ProcessGroupNCCL.cpp:2239] [PG ID 2 PG GUID 3 Rank 1]  failure detected by watchdog at work sequence id: 131 PG status: last enqueued work: 136, last completed work: 130
[rank1]:[E1129 16:32:10.706775680 ProcessGroupNCCL.cpp:721] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1129 16:32:10.706801718 ProcessGroupNCCL.cpp:2571] [PG ID 2 PG GUID 3 Rank 1] First PG on this rank to signal dumping.
[rank1]:[E1129 16:32:11.715811877 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E1129 16:32:11.715959784 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank2]:[E1129 16:32:11.798316747 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank2]:[E1129 16:32:11.798493204 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E1129 16:32:11.276281028 ProcessGroupNCCL.cpp:1856] [PG ID 0 PG GUID 0 Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: -1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E1129 16:32:11.276409640 ProcessGroupNCCL.cpp:1573] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
