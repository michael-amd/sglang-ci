[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:40] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-16 17:03:41 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-16 17:03:42] server_args=ServerArgs(model_path='/data/amd--grok-1-W4A8KV8', tokenizer_path='/data/Xenova--grok-1-tokenizer', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], dtype='auto', quantization='fp8', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.85, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=938717264, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/amd--grok-1-W4A8KV8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='fp8', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-12-16 17:03:42] No chat template found, defaulting to 'string' content format
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:47] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:47] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:47] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:47] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:47] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:47] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:47] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:48] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-16 17:03:48] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-16 17:03:48 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 17:03:48 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 17:03:48 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 17:03:48 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 17:03:48 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 17:03:48 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 17:03:48 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
INFO 12-16 17:03:48 [__init__.py:241] Automatically detected platform rocm.
INFO 12-16 17:03:48 [__init__.py:241] Automatically detected platform rocm.
[2025-12-16 17:03:48 TP7] Process 21777 gpu_id 7 is running on CPUs: [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-12-16 17:03:49 TP3] Process 21773 gpu_id 3 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-16 17:03:49 TP2] Process 21772 gpu_id 2 is running on CPUs: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
[2025-12-16 17:03:49 TP7] Init torch distributed begin.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-12-16 17:03:49 TP4] Process 21774 gpu_id 4 is running on CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-12-16 17:03:49 TP6] Process 21776 gpu_id 6 is running on CPUs: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-12-16 17:03:49 TP5] Process 21775 gpu_id 5 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-16 17:03:49 TP3] Init torch distributed begin.
[2025-12-16 17:03:49 TP0] Process 21770 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
[2025-12-16 17:03:49 TP2] Init torch distributed begin.
[2025-12-16 17:03:49 TP4] Init torch distributed begin.
[2025-12-16 17:03:49 TP1] Process 21771 gpu_id 1 is running on CPUs: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-12-16 17:03:49 TP6] Init torch distributed begin.
[2025-12-16 17:03:49 TP5] Init torch distributed begin.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
[2025-12-16 17:03:49 TP0] Init torch distributed begin.
[2025-12-16 17:03:49 TP1] Init torch distributed begin.
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-12-16 17:03:50 TP0] sglang is using nccl==2.26.6
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 17:03:55 TP7] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 17:03:55 TP5] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 17:03:55 TP4] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 17:03:55 TP6] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 17:03:55 TP0] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 17:03:55 TP1] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 17:03:55 TP2] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 17:03:55 TP3] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-16 17:03:55 TP7] Using AiterCustomAllreduce for ROCm.
[2025-12-16 17:03:55 TP6] Using AiterCustomAllreduce for ROCm.
[2025-12-16 17:03:55 TP5] Using AiterCustomAllreduce for ROCm.
[2025-12-16 17:03:55 TP4] Using AiterCustomAllreduce for ROCm.
[2025-12-16 17:03:55 TP0] Using AiterCustomAllreduce for ROCm.
[2025-12-16 17:03:55 TP2] Using AiterCustomAllreduce for ROCm.
[2025-12-16 17:03:55 TP1] Using AiterCustomAllreduce for ROCm.
[2025-12-16 17:03:55 TP3] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-12-16 17:03:56 TP0] Init torch distributed ends. mem usage=2.81 GB
[2025-12-16 17:03:56 TP7] Init torch distributed ends. mem usage=2.68 GB
[2025-12-16 17:03:56 TP6] Init torch distributed ends. mem usage=2.68 GB
[2025-12-16 17:03:56 TP5] Init torch distributed ends. mem usage=2.75 GB
[2025-12-16 17:03:56 TP4] Init torch distributed ends. mem usage=2.68 GB
[2025-12-16 17:03:56 TP3] Init torch distributed ends. mem usage=2.81 GB
[2025-12-16 17:03:56 TP2] Init torch distributed ends. mem usage=2.81 GB
[2025-12-16 17:03:56 TP1] Init torch distributed ends. mem usage=2.37 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-12-16 17:03:56 TP5] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 17:03:56 TP4] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 17:03:56 TP6] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 17:03:56 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 17:03:56 TP7] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 17:03:56 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 17:03:56 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 17:03:56 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'Tensor' is not defined
[2025-12-16 17:03:56 TP5] Load weight begin. avail mem=284.80 GB
[2025-12-16 17:03:56 TP6] Load weight begin. avail mem=284.86 GB
[2025-12-16 17:03:56 TP4] Load weight begin. avail mem=284.86 GB
[2025-12-16 17:03:56 TP1] Load weight begin. avail mem=285.17 GB
[2025-12-16 17:03:56 TP7] Load weight begin. avail mem=284.86 GB
[2025-12-16 17:03:56 TP0] Load weight begin. avail mem=284.74 GB
[2025-12-16 17:03:56 TP0] Detected fp8 checkpoint.
[2025-12-16 17:03:56 TP2] Load weight begin. avail mem=284.74 GB
[2025-12-16 17:03:56 TP3] Load weight begin. avail mem=284.74 GB
Loading pt checkpoint shards:   0% Completed | 0/33 [00:00<?, ?it/s]
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:   3% Completed | 1/33 [00:00<00:08,  3.90it/s]
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:   6% Completed | 2/33 [00:00<00:06,  4.68it/s]
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:   9% Completed | 3/33 [00:00<00:06,  4.86it/s]
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  12% Completed | 4/33 [00:00<00:06,  4.43it/s]
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP3] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP2] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP0] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  15% Completed | 5/33 [00:01<00:06,  4.43it/s]
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP6] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP1] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP7] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP5] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:57 TP4] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP3] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP3] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP3] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP3] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP0] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP0] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP1] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP1] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP1] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP1] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP0] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP0] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  18% Completed | 6/33 [00:01<00:08,  3.11it/s]
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP7] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP7] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP7] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP7] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP0] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP0] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP0] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP0] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  21% Completed | 7/33 [00:01<00:07,  3.42it/s]
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP1] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP1] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP1] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP1] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP2] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP7] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP7] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP7] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP7] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP5] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP4] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:58 TP6] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP3] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP3] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP3] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP3] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP7] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP2] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP6] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP5] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP4] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:03:59 TP1] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP4] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP4] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP4] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP4] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP7] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP4] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP4] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP4] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:00 TP4] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP3] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP3] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP3] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP3] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP1] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP1] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP1] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP1] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP0] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP0] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP0] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP0] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP3] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP3] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP1] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP1] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP3] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP1] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP3] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP1] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP5] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP5] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP5] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP5] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP4] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:01 TP7] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP6] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP6] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP6] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP6] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP2] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP2] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP2] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP2] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  24% Completed | 8/33 [00:05<00:35,  1.43s/it]
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP3] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP0] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP0] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP0] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP0] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  27% Completed | 9/33 [00:06<00:26,  1.10s/it]
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP1] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:02 TP7] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP7] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP7] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  30% Completed | 10/33 [00:06<00:19,  1.20it/s]
[2025-12-16 17:04:03 TP7] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP7] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  33% Completed | 11/33 [00:06<00:14,  1.57it/s]
[2025-12-16 17:04:03 TP4] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP4] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP4] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP4] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP2] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP2] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP2] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP2] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
Loading pt checkpoint shards:  36% Completed | 12/33 [00:06<00:11,  1.79it/s]
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP3] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP2] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP2] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP2] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP2] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP6] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP1] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP0] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  39% Completed | 13/33 [00:07<00:09,  2.15it/s]
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:03 TP5] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.51.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.51.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.52.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.52.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP2] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP2] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP2] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP2] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP1] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP1] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP0] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP0] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP1] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP0] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP0] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP1] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP7] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP7] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP7] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP7] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  42% Completed | 14/33 [00:07<00:10,  1.87it/s]
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP2] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP2] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP2] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP2] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP0] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP0] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP4] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP0] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP0] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  45% Completed | 15/33 [00:08<00:08,  2.13it/s]
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP5] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP6] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:04 TP3] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.27.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.27.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.20.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.20.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.28.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.28.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  48% Completed | 16/33 [00:08<00:06,  2.58it/s]
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  52% Completed | 17/33 [00:08<00:05,  2.94it/s]
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  55% Completed | 18/33 [00:08<00:04,  3.59it/s]
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.25.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.25.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.26.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.26.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  58% Completed | 19/33 [00:08<00:03,  4.09it/s]
[2025-12-16 17:04:05 TP7] Skipping name='model.layers.21.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP7] Skipping name='model.layers.21.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP4] #all_names: 1347, #hit_names: 1219, #missing_exclude_scales: 0
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.61.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.61.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.62.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] Skipping name='model.layers.62.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP6] #all_names: 1347, #hit_names: 1219, #missing_exclude_scales: 0
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP7] Skipping name='model.layers.22.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP7] Skipping name='model.layers.22.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP2] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.10.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.10.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.11.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP3] Skipping name='model.layers.11.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP0] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  61% Completed | 20/33 [00:09<00:02,  4.73it/s]
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:05 TP5] #all_names: 1347, #hit_names: 1219, #missing_exclude_scales: 0
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 17:04:05 TP6] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 17:04:05 TP5] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:05 TP6] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 17:04:05 TP4] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:05 TP5] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:05 TP4] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.59.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.59.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.60.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.53.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.60.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.53.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.54.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.54.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.43.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.43.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.44.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.44.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  64% Completed | 21/33 [00:09<00:02,  4.55it/s]
[2025-12-16 17:04:06 TP4] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=265.66 GB, mem usage=19.21 GB.
[2025-12-16 17:04:06 TP5] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=265.59 GB, mem usage=19.21 GB.
[2025-12-16 17:04:06 TP6] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=265.66 GB, mem usage=19.21 GB.
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP1] Skipping name='model.layers.0.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP1] Skipping name='model.layers.0.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP1] Skipping name='model.layers.1.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP1] Skipping name='model.layers.1.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  67% Completed | 22/33 [00:09<00:03,  3.25it/s]
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP7] #all_names: 1347, #hit_names: 1219, #missing_exclude_scales: 0
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.39.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.39.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.40.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] Skipping name='model.layers.40.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP3] #all_names: 1347, #hit_names: 1219, #missing_exclude_scales: 0
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  70% Completed | 23/33 [00:10<00:02,  3.57it/s]
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 17:04:06 TP7] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:06 TP7] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:06 TP3] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:06 TP3] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.8.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.8.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.9.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP2] Skipping name='model.layers.9.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:06 TP0] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  73% Completed | 24/33 [00:10<00:02,  4.13it/s]
[2025-12-16 17:04:07 TP7] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=265.66 GB, mem usage=19.21 GB.
[2025-12-16 17:04:07 TP3] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=265.53 GB, mem usage=19.21 GB.
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.29.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.29.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.33.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.33.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.30.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.34.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.30.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  76% Completed | 25/33 [00:10<00:01,  4.74it/s]
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.34.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.37.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.37.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.38.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.38.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  79% Completed | 26/33 [00:10<00:01,  5.39it/s]
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  82% Completed | 27/33 [00:10<00:01,  5.60it/s]
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.55.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.55.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.56.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.56.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  85% Completed | 28/33 [00:10<00:00,  6.19it/s]
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.41.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.41.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.42.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.42.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  88% Completed | 29/33 [00:10<00:00,  6.24it/s]
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.6.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.6.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.7.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.7.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP0] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  91% Completed | 30/33 [00:11<00:00,  6.30it/s]
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.12.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.12.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.13.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP2] Skipping name='model.layers.13.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.16.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.16.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.17.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:07 TP1] Skipping name='model.layers.17.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.14.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.14.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.15.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.15.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  94% Completed | 31/33 [00:11<00:00,  5.78it/s]
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.31.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.31.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.32.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.32.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.18.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.18.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.19.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.19.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards:  97% Completed | 32/33 [00:11<00:00,  5.93it/s]
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.2.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.2.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.3.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.3.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.57.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.57.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.58.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP0] Skipping name='model.layers.58.self_attn.v_scale' in load_weights_wrapper
Loading pt checkpoint shards: 100% Completed | 33/33 [00:11<00:00,  5.50it/s]
Loading pt checkpoint shards: 100% Completed | 33/33 [00:11<00:00,  2.85it/s]

[2025-12-16 17:04:08 TP0] #all_names: 1347, #hit_names: 1219, #missing_exclude_scales: 0
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.23.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.23.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.24.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.24.self_attn.v_scale' in load_weights_wrapper
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 17:04:08 TP0] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:08 TP0] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.45.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.45.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.46.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.46.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.63.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Skipping name='model.layers.63.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] #all_names: 1347, #hit_names: 1219, #missing_exclude_scales: 0
[2025-12-16 17:04:08 TP0] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=265.53 GB, mem usage=19.21 GB.
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 17:04:08 TP2] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:08 TP2] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.49.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.49.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.50.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.50.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP2] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=265.53 GB, mem usage=19.21 GB.
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.4.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.4.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.5.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.5.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.35.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.35.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.36.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:08 TP1] Skipping name='model.layers.36.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:09 TP1] Skipping name='model.layers.47.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:09 TP1] Skipping name='model.layers.47.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:09 TP1] Skipping name='model.layers.48.self_attn.k_scale' in load_weights_wrapper
[2025-12-16 17:04:09 TP1] Skipping name='model.layers.48.self_attn.v_scale' in load_weights_wrapper
[2025-12-16 17:04:09 TP1] #all_names: 1347, #hit_names: 1219, #missing_exclude_scales: 0
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-16 17:04:09 TP1] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:09 TP1] type hints mismatch, override to --> static_per_tensor_quant(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None
[2025-12-16 17:04:09 TP1] Load weight end. type=Grok1ModelForCausalLM, dtype=torch.bfloat16, avail mem=265.97 GB, mem usage=19.21 GB.
[2025-12-16 17:04:09 TP0] Using KV cache dtype: torch.bfloat16
[2025-12-16 17:04:09 TP0] KV Cache is allocated. #tokens: 7301392, K size: 111.41 GB, V size: 111.41 GB
[2025-12-16 17:04:09 TP1] KV Cache is allocated. #tokens: 7301392, K size: 111.41 GB, V size: 111.41 GB
[2025-12-16 17:04:09 TP0] Memory pool end. avail mem=42.35 GB
[2025-12-16 17:04:09 TP1] Memory pool end. avail mem=42.79 GB
[2025-12-16 17:04:09 TP7] KV Cache is allocated. #tokens: 7301392, K size: 111.41 GB, V size: 111.41 GB
[2025-12-16 17:04:09 TP2] KV Cache is allocated. #tokens: 7301392, K size: 111.41 GB, V size: 111.41 GB
[2025-12-16 17:04:09 TP7] Memory pool end. avail mem=42.47 GB
[2025-12-16 17:04:09 TP2] Memory pool end. avail mem=42.35 GB
[2025-12-16 17:04:09 TP5] KV Cache is allocated. #tokens: 7301392, K size: 111.41 GB, V size: 111.41 GB
[2025-12-16 17:04:09 TP5] Memory pool end. avail mem=42.41 GB
[2025-12-16 17:04:09 TP4] KV Cache is allocated. #tokens: 7301392, K size: 111.41 GB, V size: 111.41 GB
[2025-12-16 17:04:09 TP4] Memory pool end. avail mem=42.47 GB
[2025-12-16 17:04:09 TP3] KV Cache is allocated. #tokens: 7301392, K size: 111.41 GB, V size: 111.41 GB
[2025-12-16 17:04:09 TP3] Memory pool end. avail mem=42.35 GB
[2025-12-16 17:04:09 TP6] KV Cache is allocated. #tokens: 7301392, K size: 111.41 GB, V size: 111.41 GB
[2025-12-16 17:04:09 TP6] Memory pool end. avail mem=42.47 GB
[2025-12-16 17:04:09 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=42.28 GB
[2025-12-16 17:04:09 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=41.84 GB
[2025-12-16 17:04:09 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=41.90 GB
[2025-12-16 17:04:09 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=41.96 GB
[2025-12-16 17:04:09 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=41.96 GB
[2025-12-16 17:04:09 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=41.84 GB
[2025-12-16 17:04:09 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=41.96 GB
[2025-12-16 17:04:09 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=41.84 GB
[2025-12-16 17:04:09 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=41.55 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 17:04:12 TP1] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:12 TP1] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 17:04:12 TP1] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[2025-12-16 17:04:12 TP1] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:12 TP1] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:12 TP1] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 17:04:13 TP2] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:13 TP2] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 17:04:13 TP2] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[2025-12-16 17:04:13 TP2] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:13 TP2] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:13 TP2] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 17:04:14 TP0] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:14 TP0] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 17:04:14 TP0] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[2025-12-16 17:04:14 TP0] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:14 TP0] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:14 TP0] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 17:04:14 TP5] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:15 TP5] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 17:04:15 TP5] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[2025-12-16 17:04:15 TP5] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:15 TP5] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:15 TP5] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 17:04:15 TP7] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:15 TP7] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 17:04:15 TP7] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[2025-12-16 17:04:16 TP7] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:16 TP7] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:16 TP7] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 17:04:16 TP4] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:16 TP4] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 17:04:16 TP4] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[2025-12-16 17:04:16 TP4] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:16 TP4] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:16 TP4] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 17:04:17 TP3] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:17 TP3] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 17:04:17 TP3] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[2025-12-16 17:04:17 TP3] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:17 TP3] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:17 TP3] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-16 17:04:18 TP6] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:18 TP6] [fused_moe] using 2stage default for (256, 512, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-16 17:04:18 TP6] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[2025-12-16 17:04:18 TP6] import [module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2] under /sgl-workspace/aiter/aiter/jit/module_moe_ck2stages_f8_i4_preshuffle_off_b16_gelu_per_token_mulWeightStage2.so
[aiter] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:18 TP6] type hints mismatch, override to --> ck_moe_stage1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w1_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[aiter] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
[2025-12-16 17:04:18 TP6] type hints mismatch, override to --> ck_moe_stage2(inter_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, out: torch.Tensor, topk: int, kernelName: str = None, w2_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_m: Optional[int] = 32, sorted_weights: Optional[torch.Tensor] = None, quant_type: int = 0, activation: int = 0) -> None
Capturing batches (bs=512 avail_mem=41.55 GB):   2%|         | 1/52 [00:08<07:32,  8.87s/it]Capturing batches (bs=496 avail_mem=40.93 GB):   2%|         | 1/52 [00:08<07:32,  8.87s/it]Capturing batches (bs=496 avail_mem=40.93 GB):   4%|         | 2/52 [00:09<03:10,  3.81s/it]Capturing batches (bs=480 avail_mem=40.93 GB):   4%|         | 2/52 [00:09<03:10,  3.81s/it]Capturing batches (bs=480 avail_mem=40.93 GB):   6%|         | 3/52 [00:09<01:45,  2.15s/it]Capturing batches (bs=464 avail_mem=40.93 GB):   6%|         | 3/52 [00:09<01:45,  2.15s/it]Capturing batches (bs=464 avail_mem=40.93 GB):   8%|         | 4/52 [00:09<01:05,  1.37s/it]Capturing batches (bs=448 avail_mem=40.93 GB):   8%|         | 4/52 [00:09<01:05,  1.37s/it]Capturing batches (bs=448 avail_mem=40.93 GB):  10%|         | 5/52 [00:09<00:44,  1.07it/s]Capturing batches (bs=432 avail_mem=40.92 GB):  10%|         | 5/52 [00:09<00:44,  1.07it/s]Capturing batches (bs=432 avail_mem=40.92 GB):  12%|        | 6/52 [00:09<00:31,  1.48it/s]Capturing batches (bs=416 avail_mem=40.92 GB):  12%|        | 6/52 [00:09<00:31,  1.48it/s]Capturing batches (bs=416 avail_mem=40.92 GB):  13%|        | 7/52 [00:10<00:23,  1.95it/s]Capturing batches (bs=400 avail_mem=40.92 GB):  13%|        | 7/52 [00:10<00:23,  1.95it/s]Capturing batches (bs=400 avail_mem=40.92 GB):  15%|        | 8/52 [00:10<00:17,  2.48it/s]Capturing batches (bs=384 avail_mem=40.92 GB):  15%|        | 8/52 [00:10<00:17,  2.48it/s]Capturing batches (bs=384 avail_mem=40.92 GB):  17%|        | 9/52 [00:10<00:14,  3.03it/s]Capturing batches (bs=368 avail_mem=40.92 GB):  17%|        | 9/52 [00:10<00:14,  3.03it/s]Capturing batches (bs=368 avail_mem=40.92 GB):  19%|        | 10/52 [00:10<00:11,  3.57it/s]Capturing batches (bs=352 avail_mem=40.92 GB):  19%|        | 10/52 [00:10<00:11,  3.57it/s]Capturing batches (bs=352 avail_mem=40.92 GB):  21%|        | 11/52 [00:10<00:10,  4.06it/s]Capturing batches (bs=336 avail_mem=40.92 GB):  21%|        | 11/52 [00:10<00:10,  4.06it/s]Capturing batches (bs=336 avail_mem=40.92 GB):  23%|       | 12/52 [00:10<00:08,  4.51it/s]Capturing batches (bs=320 avail_mem=40.92 GB):  23%|       | 12/52 [00:10<00:08,  4.51it/s]Capturing batches (bs=320 avail_mem=40.92 GB):  25%|       | 13/52 [00:11<00:07,  4.88it/s]Capturing batches (bs=304 avail_mem=40.92 GB):  25%|       | 13/52 [00:11<00:07,  4.88it/s]Capturing batches (bs=304 avail_mem=40.92 GB):  27%|       | 14/52 [00:11<00:07,  5.19it/s]Capturing batches (bs=288 avail_mem=40.92 GB):  27%|       | 14/52 [00:11<00:07,  5.19it/s]Capturing batches (bs=288 avail_mem=40.92 GB):  29%|       | 15/52 [00:11<00:06,  5.44it/s]Capturing batches (bs=272 avail_mem=40.92 GB):  29%|       | 15/52 [00:11<00:06,  5.44it/s]Capturing batches (bs=272 avail_mem=40.92 GB):  31%|       | 16/52 [00:11<00:06,  5.62it/s]Capturing batches (bs=256 avail_mem=40.92 GB):  31%|       | 16/52 [00:11<00:06,  5.62it/s][aiter] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:21 TP4] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:21 TP7] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:21 TP3] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:21 TP5] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:21 TP6] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:21 TP0] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:21 TP2] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:21 TP1] [fused_moe] using 2stage default for (256, 256, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=256 avail_mem=40.92 GB):  33%|      | 17/52 [00:11<00:05,  5.98it/s]Capturing batches (bs=248 avail_mem=40.92 GB):  33%|      | 17/52 [00:11<00:05,  5.98it/s]Capturing batches (bs=248 avail_mem=40.92 GB):  35%|      | 18/52 [00:11<00:05,  6.28it/s]Capturing batches (bs=240 avail_mem=40.92 GB):  35%|      | 18/52 [00:11<00:05,  6.28it/s]Capturing batches (bs=240 avail_mem=40.92 GB):  37%|      | 19/52 [00:11<00:05,  6.51it/s]Capturing batches (bs=232 avail_mem=40.92 GB):  37%|      | 19/52 [00:11<00:05,  6.51it/s]Capturing batches (bs=232 avail_mem=40.92 GB):  38%|      | 20/52 [00:12<00:04,  6.69it/s]Capturing batches (bs=224 avail_mem=40.91 GB):  38%|      | 20/52 [00:12<00:04,  6.69it/s]Capturing batches (bs=224 avail_mem=40.91 GB):  40%|      | 21/52 [00:12<00:04,  6.83it/s]Capturing batches (bs=216 avail_mem=40.91 GB):  40%|      | 21/52 [00:12<00:04,  6.83it/s]Capturing batches (bs=216 avail_mem=40.91 GB):  42%|     | 22/52 [00:12<00:04,  6.93it/s]Capturing batches (bs=208 avail_mem=40.91 GB):  42%|     | 22/52 [00:12<00:04,  6.93it/s]Capturing batches (bs=208 avail_mem=40.91 GB):  44%|     | 23/52 [00:12<00:04,  7.00it/s]Capturing batches (bs=200 avail_mem=40.91 GB):  44%|     | 23/52 [00:12<00:04,  7.00it/s]Capturing batches (bs=200 avail_mem=40.91 GB):  46%|     | 24/52 [00:12<00:03,  7.05it/s]Capturing batches (bs=192 avail_mem=40.91 GB):  46%|     | 24/52 [00:12<00:03,  7.05it/s]Capturing batches (bs=192 avail_mem=40.91 GB):  48%|     | 25/52 [00:12<00:03,  7.13it/s]Capturing batches (bs=184 avail_mem=40.91 GB):  48%|     | 25/52 [00:12<00:03,  7.13it/s]Capturing batches (bs=184 avail_mem=40.91 GB):  50%|     | 26/52 [00:12<00:03,  7.16it/s]Capturing batches (bs=176 avail_mem=40.91 GB):  50%|     | 26/52 [00:12<00:03,  7.16it/s]Capturing batches (bs=176 avail_mem=40.91 GB):  52%|    | 27/52 [00:13<00:03,  7.20it/s]Capturing batches (bs=168 avail_mem=40.91 GB):  52%|    | 27/52 [00:13<00:03,  7.20it/s]Capturing batches (bs=168 avail_mem=40.91 GB):  54%|    | 28/52 [00:13<00:03,  7.21it/s]Capturing batches (bs=160 avail_mem=40.91 GB):  54%|    | 28/52 [00:13<00:03,  7.21it/s]Capturing batches (bs=160 avail_mem=40.91 GB):  56%|    | 29/52 [00:13<00:03,  7.25it/s]Capturing batches (bs=152 avail_mem=40.91 GB):  56%|    | 29/52 [00:13<00:03,  7.25it/s]Capturing batches (bs=152 avail_mem=40.91 GB):  58%|    | 30/52 [00:13<00:03,  7.27it/s]Capturing batches (bs=144 avail_mem=40.91 GB):  58%|    | 30/52 [00:13<00:03,  7.27it/s]Capturing batches (bs=144 avail_mem=40.91 GB):  60%|    | 31/52 [00:13<00:02,  7.31it/s]Capturing batches (bs=136 avail_mem=40.91 GB):  60%|    | 31/52 [00:13<00:02,  7.31it/s]Capturing batches (bs=136 avail_mem=40.91 GB):  62%|   | 32/52 [00:13<00:02,  7.34it/s]Capturing batches (bs=128 avail_mem=40.91 GB):  62%|   | 32/52 [00:13<00:02,  7.34it/s][aiter] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:23 TP4] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:23 TP5] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:23 TP3] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:23 TP7] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:23 TP6] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:23 TP0] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:23 TP2] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:23 TP1] [fused_moe] using 2stage default for (256, 128, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=128 avail_mem=40.91 GB):  63%|   | 33/52 [00:13<00:02,  7.07it/s]Capturing batches (bs=120 avail_mem=40.90 GB):  63%|   | 33/52 [00:13<00:02,  7.07it/s]Capturing batches (bs=120 avail_mem=40.90 GB):  65%|   | 34/52 [00:14<00:02,  7.22it/s]Capturing batches (bs=112 avail_mem=40.90 GB):  65%|   | 34/52 [00:14<00:02,  7.22it/s]Capturing batches (bs=112 avail_mem=40.90 GB):  67%|   | 35/52 [00:14<00:02,  7.38it/s]Capturing batches (bs=104 avail_mem=40.90 GB):  67%|   | 35/52 [00:14<00:02,  7.38it/s]Capturing batches (bs=104 avail_mem=40.90 GB):  69%|   | 36/52 [00:14<00:02,  7.52it/s]Capturing batches (bs=96 avail_mem=40.90 GB):  69%|   | 36/52 [00:14<00:02,  7.52it/s] Capturing batches (bs=96 avail_mem=40.90 GB):  71%|   | 37/52 [00:14<00:01,  7.61it/s]Capturing batches (bs=88 avail_mem=40.90 GB):  71%|   | 37/52 [00:14<00:01,  7.61it/s]Capturing batches (bs=88 avail_mem=40.90 GB):  73%|  | 38/52 [00:14<00:01,  7.68it/s]Capturing batches (bs=80 avail_mem=40.90 GB):  73%|  | 38/52 [00:14<00:01,  7.68it/s]Capturing batches (bs=80 avail_mem=40.90 GB):  75%|  | 39/52 [00:14<00:01,  7.72it/s]Capturing batches (bs=72 avail_mem=40.89 GB):  75%|  | 39/52 [00:14<00:01,  7.72it/s]Capturing batches (bs=72 avail_mem=40.89 GB):  77%|  | 40/52 [00:14<00:01,  7.72it/s]Capturing batches (bs=64 avail_mem=40.89 GB):  77%|  | 40/52 [00:14<00:01,  7.72it/s][aiter] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:24 TP5] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:24 TP4] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:24 TP6] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:24 TP7] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:24 TP3] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:24 TP0] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:24 TP2] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:24 TP1] [fused_moe] using 2stage default for (256, 64, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=64 avail_mem=40.89 GB):  79%|  | 41/52 [00:14<00:01,  7.10it/s]Capturing batches (bs=56 avail_mem=40.89 GB):  79%|  | 41/52 [00:14<00:01,  7.10it/s]Capturing batches (bs=56 avail_mem=40.89 GB):  81%|  | 42/52 [00:15<00:01,  7.32it/s]Capturing batches (bs=48 avail_mem=40.89 GB):  81%|  | 42/52 [00:15<00:01,  7.32it/s]Capturing batches (bs=48 avail_mem=40.89 GB):  83%| | 43/52 [00:15<00:01,  7.47it/s]Capturing batches (bs=40 avail_mem=40.89 GB):  83%| | 43/52 [00:15<00:01,  7.47it/s]Capturing batches (bs=40 avail_mem=40.89 GB):  85%| | 44/52 [00:15<00:01,  7.62it/s]Capturing batches (bs=32 avail_mem=40.89 GB):  85%| | 44/52 [00:15<00:01,  7.62it/s][aiter] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP5] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP4] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP7] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP3] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP6] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP0] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP1] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP2] [fused_moe] using 2stage default for (256, 32, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=32 avail_mem=40.89 GB):  87%| | 45/52 [00:15<00:00,  7.68it/s]Capturing batches (bs=24 avail_mem=40.89 GB):  87%| | 45/52 [00:15<00:00,  7.68it/s]Capturing batches (bs=24 avail_mem=40.89 GB):  88%| | 46/52 [00:15<00:00,  7.76it/s]Capturing batches (bs=16 avail_mem=40.89 GB):  88%| | 46/52 [00:15<00:00,  7.76it/s][aiter] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP4] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP0] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP2] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP6] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP7] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP5] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP3] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:25 TP1] [fused_moe] using 2stage default for (256, 16, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
Capturing batches (bs=16 avail_mem=40.89 GB):  90%| | 47/52 [00:15<00:00,  7.82it/s]Capturing batches (bs=12 avail_mem=40.88 GB):  90%| | 47/52 [00:15<00:00,  7.82it/s]Capturing batches (bs=12 avail_mem=40.88 GB):  92%|| 48/52 [00:15<00:00,  7.86it/s]Capturing batches (bs=8 avail_mem=40.88 GB):  92%|| 48/52 [00:15<00:00,  7.86it/s] Capturing batches (bs=8 avail_mem=40.88 GB):  94%|| 49/52 [00:15<00:00,  7.88it/s]Capturing batches (bs=4 avail_mem=40.88 GB):  94%|| 49/52 [00:15<00:00,  7.88it/s]Capturing batches (bs=4 avail_mem=40.88 GB):  96%|| 50/52 [00:16<00:00,  7.89it/s]Capturing batches (bs=2 avail_mem=40.88 GB):  96%|| 50/52 [00:16<00:00,  7.89it/s]Capturing batches (bs=2 avail_mem=40.88 GB):  98%|| 51/52 [00:16<00:00,  7.92it/s]Capturing batches (bs=1 avail_mem=40.88 GB):  98%|| 51/52 [00:16<00:00,  7.92it/s]Capturing batches (bs=1 avail_mem=40.88 GB): 100%|| 52/52 [00:16<00:00,  7.20it/s]Capturing batches (bs=1 avail_mem=40.88 GB): 100%|| 52/52 [00:16<00:00,  3.18it/s]
[aiter] Registering 6708 cuda graph addresses
[2025-12-16 17:04:26 TP7] Registering 6708 cuda graph addresses
[aiter] Registering 6708 cuda graph addresses
[aiter] Registering 6708 cuda graph addresses
[aiter] Registering 6708 cuda graph addresses
[2025-12-16 17:04:26 TP6] Registering 6708 cuda graph addresses
[aiter] Registering 6708 cuda graph addresses
[2025-12-16 17:04:26 TP3] Registering 6708 cuda graph addresses
[aiter] Registering 6708 cuda graph addresses
[2025-12-16 17:04:26 TP4] Registering 6708 cuda graph addresses
[aiter] Registering 6708 cuda graph addresses
[2025-12-16 17:04:26 TP0] Registering 6708 cuda graph addresses
[2025-12-16 17:04:26 TP5] Registering 6708 cuda graph addresses
[aiter] Registering 6708 cuda graph addresses
[2025-12-16 17:04:26 TP2] Registering 6708 cuda graph addresses
[2025-12-16 17:04:26 TP1] Registering 6708 cuda graph addresses
[2025-12-16 17:04:26 TP6] Capture cuda graph end. Time elapsed: 17.09 s. mem usage=0.96 GB. avail mem=41.00 GB.
[2025-12-16 17:04:26 TP5] Capture cuda graph end. Time elapsed: 17.12 s. mem usage=0.96 GB. avail mem=40.94 GB.
[2025-12-16 17:04:26 TP4] Capture cuda graph end. Time elapsed: 17.13 s. mem usage=0.96 GB. avail mem=41.00 GB.
[2025-12-16 17:04:26 TP7] Capture cuda graph end. Time elapsed: 17.14 s. mem usage=0.96 GB. avail mem=41.00 GB.
[2025-12-16 17:04:26 TP0] Capture cuda graph end. Time elapsed: 17.14 s. mem usage=0.96 GB. avail mem=40.88 GB.
[2025-12-16 17:04:26 TP1] Capture cuda graph end. Time elapsed: 17.21 s. mem usage=0.96 GB. avail mem=41.32 GB.
[2025-12-16 17:04:26 TP3] Capture cuda graph end. Time elapsed: 17.32 s. mem usage=0.96 GB. avail mem=40.88 GB.
[2025-12-16 17:04:26 TP2] Capture cuda graph end. Time elapsed: 17.35 s. mem usage=0.96 GB. avail mem=40.88 GB.
[2025-12-16 17:04:27 TP0] max_total_num_tokens=7301392, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4096, context_len=8192, available_gpu_mem=40.88 GB
[2025-12-16 17:04:27] INFO:     Started server process [21617]
[2025-12-16 17:04:27] INFO:     Waiting for application startup.
[2025-12-16 17:04:27] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-12-16 17:04:27] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-12-16 17:04:27] INFO:     Application startup complete.
[2025-12-16 17:04:27] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-12-16 17:04:28] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-16 17:04:28] INFO:     127.0.0.1:50222 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-16 17:04:28 TP0] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[2025-12-16 17:04:28 TP1] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[2025-12-16 17:04:28 TP1] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[2025-12-16 17:04:28 TP0] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[2025-12-16 17:04:28 TP0] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[2025-12-16 17:04:28 TP3] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[2025-12-16 17:04:28 TP3] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[2025-12-16 17:04:28 TP2] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[2025-12-16 17:04:28 TP2] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[2025-12-16 17:04:28 TP4] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[2025-12-16 17:04:28 TP4] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[2025-12-16 17:04:28 TP5] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[2025-12-16 17:04:28 TP5] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[2025-12-16 17:04:28 TP6] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[2025-12-16 17:04:28 TP6] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[2025-12-16 17:04:28 TP7] import [mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout] under /sgl-workspace/aiter/aiter/jit/mha_batch_prefill_bf16_logits_nbias_mask_nlse_ndropout.so
[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[2025-12-16 17:04:28 TP7] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[2025-12-16 17:04:31] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:31] The server is fired up and ready to roll!
[2025-12-16 17:04:34] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-16 17:04:34] INFO:     127.0.0.1:49714 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-16 17:04:34 TP0] Prefill batch, #new-seq: 1, #new-token: 796, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:34 TP6] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:34 TP4] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:34 TP7] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:34 TP0] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:34 TP5] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:34 TP1] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:34 TP2] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[aiter] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:34 TP3] [fused_moe] using 2stage default for (256, 1024, 6144, 4096, 8, 2, 'ActivationType.Gelu', 'torch.bfloat16', 'torch.float8_e4m3fn', 'torch.uint32', 'QuantType.per_Token', True, False) 
[2025-12-16 17:04:35] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:35 TP0] Prefill batch, #new-seq: 1, #new-token: 68, #cached-token: 796, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-16 17:04:35 TP0] Prefill batch, #new-seq: 2, #new-token: 93, #cached-token: 1592, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-12-16 17:04:35 TP0] Prefill batch, #new-seq: 64, #new-token: 4164, #cached-token: 51072, token usage: 0.00, #running-req: 3, #queue-req: 0, 
[2025-12-16 17:04:35 TP0] Prefill batch, #new-seq: 10, #new-token: 613, #cached-token: 7982, token usage: 0.00, #running-req: 67, #queue-req: 0, 
[2025-12-16 17:04:35 TP0] Prefill batch, #new-seq: 268, #new-token: 16370, #cached-token: 213961, token usage: 0.00, #running-req: 77, #queue-req: 106, 
[2025-12-16 17:04:35 TP0] Prefill batch, #new-seq: 195, #new-token: 12181, #cached-token: 155692, token usage: 0.00, #running-req: 345, #queue-req: 0, 
[2025-12-16 17:04:37 TP0] Prefill batch, #new-seq: 263, #new-token: 16356, #cached-token: 210044, token usage: 0.00, #running-req: 540, #queue-req: 516, 
[2025-12-16 17:04:38 TP0] Prefill batch, #new-seq: 257, #new-token: 16357, #cached-token: 205300, token usage: 0.01, #running-req: 803, #queue-req: 259, 
[2025-12-16 17:04:40 TP0] Prefill batch, #new-seq: 256, #new-token: 16342, #cached-token: 204505, token usage: 0.01, #running-req: 1060, #queue-req: 3, 
[2025-12-16 17:04:41 TP0] Prefill batch, #new-seq: 3, #new-token: 178, #cached-token: 2400, token usage: 0.01, #running-req: 1316, #queue-req: 0, 
[2025-12-16 17:04:49 TP0] Decode batch, #running-req: 1319, #token: 126787, token usage: 0.02, cuda graph: False, gen throughput (token/s): 1895.46, #queue-req: 0, 
[2025-12-16 17:04:50] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:51] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:51] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:52] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:52] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:52] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:52] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:52] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:53] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:53] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:53] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:53] INFO:     127.0.0.1:59702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:53] INFO:     127.0.0.1:60804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:53] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:53] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:54] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:54] INFO:     127.0.0.1:60774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:54] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:54] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:54] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:54] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:54] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:54] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:54] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:51244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:55] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:52250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:52398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:56] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:60698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:50516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57 TP0] Decode batch, #running-req: 1252, #token: 171157, token usage: 0.02, cuda graph: False, gen throughput (token/s): 6392.06, #queue-req: 0, 
[2025-12-16 17:04:57] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:56610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:57] INFO:     127.0.0.1:33450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:51386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:58416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:52362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:52094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:33170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:33364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:58] INFO:     127.0.0.1:33420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:50372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:60632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:53428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:60876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:33188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:50974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:60388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:54510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:53762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:04:59] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:60384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:51976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:58652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:56466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:00] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:50598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:32994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:01] INFO:     127.0.0.1:33392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:50670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:51876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:60694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:54506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:58636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:60520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:59242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:60126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:02] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:50250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:54466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:59476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:55788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:55106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:59292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:52270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:60652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:03] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:49806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:54442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:53044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:52568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:53682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:53788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04 TP0] Decode batch, #running-req: 882, #token: 156016, token usage: 0.02, cuda graph: False, gen throughput (token/s): 5871.82, #queue-req: 0, 
[2025-12-16 17:05:04] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:55242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:60848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:04] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:50640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:32782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:50382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:52266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:05] INFO:     127.0.0.1:33408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:55764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:51478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:59322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:50834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:52670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:57294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:54816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:51758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:52766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:54598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:32874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:33274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:06] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:59694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:33164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:50006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:51650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:60564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:32898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:07] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:59662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:52116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:60790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:54000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:33240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:08] INFO:     127.0.0.1:33180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:58862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:54890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:59524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:09] INFO:     127.0.0.1:32980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:33260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:53280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10 TP0] Decode batch, #running-req: 504, #token: 110517, token usage: 0.02, cuda graph: True, gen throughput (token/s): 4484.05, #queue-req: 0, 
[2025-12-16 17:05:10] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:10] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:51482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:33158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:33378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:50156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:54664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:53712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:11] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:56226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:33218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:51634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:50490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:51610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:52376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:33200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:12] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:51306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:57862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:60966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:58930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:60902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:32866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13 TP0] Decode batch, #running-req: 235, #token: 61236, token usage: 0.01, cuda graph: True, gen throughput (token/s): 4635.13, #queue-req: 0, 
[2025-12-16 17:05:13] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:13] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:32786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:33038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:50710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:60746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:53414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:14] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:59540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:49854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:51434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:60012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:51298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:15] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16 TP0] Decode batch, #running-req: 101, #token: 32255, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2743.01, #queue-req: 0, 
[2025-12-16 17:05:16] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:56358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:56808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:16] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:32894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:57016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:53748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:33476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:59446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:51816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:17] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18 TP0] Decode batch, #running-req: 43, #token: 14736, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1532.68, #queue-req: 0, 
[2025-12-16 17:05:18] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:33044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:59932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:52486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:18] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:19] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:19] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:19] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:19] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:19] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:19] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:19] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:19 TP0] Decode batch, #running-req: 17, #token: 7614, token usage: 0.00, cuda graph: True, gen throughput (token/s): 669.07, #queue-req: 0, 
[2025-12-16 17:05:20] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:20] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:20] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:20] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:20] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:21 TP0] Decode batch, #running-req: 12, #token: 6042, token usage: 0.00, cuda graph: True, gen throughput (token/s): 356.39, #queue-req: 0, 
[2025-12-16 17:05:21] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:21] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:22] INFO:     127.0.0.1:52450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:22 TP0] Decode batch, #running-req: 9, #token: 5081, token usage: 0.00, cuda graph: True, gen throughput (token/s): 241.41, #queue-req: 0, 
[2025-12-16 17:05:23] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:24] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:24] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:24] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:24 TP0] Decode batch, #running-req: 7, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 204.93, #queue-req: 0, 
[2025-12-16 17:05:26 TP0] Decode batch, #running-req: 5, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 125.45, #queue-req: 0, 
[2025-12-16 17:05:26] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:27] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:27] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:27] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:27] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-16 17:05:27 TP0] Decode batch, #running-req: 4, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 111.97, #queue-req: 0, 
[2025-12-16 17:05:28] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
