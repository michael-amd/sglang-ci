merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:09 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-17 17:36:10] INFO model_config.py:143: Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:10] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', tokenizer_path='/mnt/raid/models/huggingface/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.7327, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=936416639, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/mnt/raid/models/huggingface/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, hooks=None)
[2025-11-17 17:36:10] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:11] Using default HuggingFace chat template with detected content format: openai
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:24 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:24 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:24 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:24 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:24 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:24 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:24 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:24 [__init__.py:241] Automatically detected platform rocm.
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_gemm_ds_v3.csv
merge tuned file under model_configs/ and configs/  /sgl-workspace/aiter/aiter/configs/a8w8_blockscale_bpreshuffle_tuned_gemm.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_bpreshuffle_tuned_gemm_dsv3.csv
INFO 11-17 17:36:24 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-17 17:36:25 TP5] Process 132050 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-17 17:36:25 TP5] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:68: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-17 17:36:25 TP1] Process 132046 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-11-17 17:36:25 TP1] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:25 TP6] Process 132051 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-11-17 17:36:25 TP6] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-17 17:36:25 TP0] Process 132045 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-11-17 17:36:25 TP2] Process 132047 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-11-17 17:36:25 TP0] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:25 TP2] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-17 17:36:25 TP4] Process 132049 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-11-17 17:36:25 TP4] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:25 TP3] Process 132048 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-17 17:36:25 TP7] Process 132052 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-11-17 17:36:25 TP3] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:25 TP7] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:26 TP5] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:26 TP5] Init torch distributed begin.
[2025-11-17 17:36:26 TP1] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:26 TP1] Init torch distributed begin.
[2025-11-17 17:36:26 TP2] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:26 TP2] Init torch distributed begin.
[2025-11-17 17:36:26 TP0] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:26 TP0] Init torch distributed begin.
[2025-11-17 17:36:26 TP6] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:26 TP6] Init torch distributed begin.
[2025-11-17 17:36:26 TP3] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:26 TP3] Init torch distributed begin.
[2025-11-17 17:36:26 TP4] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:26 TP4] Init torch distributed begin.
[2025-11-17 17:36:26 TP7] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
[2025-11-17 17:36:26 TP7] Init torch distributed begin.
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-11-17 17:36:27 TP0] sglang is using nccl==2.26.6
[2025-11-17 17:36:33 TP5] Using AiterCustomAllreduce for ROCm.
[2025-11-17 17:36:33 TP7] Using AiterCustomAllreduce for ROCm.
[2025-11-17 17:36:33 TP1] Using AiterCustomAllreduce for ROCm.
[2025-11-17 17:36:33 TP2] Using AiterCustomAllreduce for ROCm.
[2025-11-17 17:36:33 TP4] Using AiterCustomAllreduce for ROCm.
[2025-11-17 17:36:33 TP3] Using AiterCustomAllreduce for ROCm.
[2025-11-17 17:36:33 TP6] Using AiterCustomAllreduce for ROCm.
[2025-11-17 17:36:33 TP0] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-11-17 17:36:34 TP0] Init torch distributed ends. mem usage=2.95 GB
[2025-11-17 17:36:34 TP7] Init torch distributed ends. mem usage=3.23 GB
[2025-11-17 17:36:34 TP5] Init torch distributed ends. mem usage=3.22 GB
[2025-11-17 17:36:34 TP6] Init torch distributed ends. mem usage=3.24 GB
[2025-11-17 17:36:34 TP4] Init torch distributed ends. mem usage=3.31 GB
[2025-11-17 17:36:34 TP3] Init torch distributed ends. mem usage=3.36 GB
[2025-11-17 17:36:34 TP2] Init torch distributed ends. mem usage=3.37 GB
[2025-11-17 17:36:34 TP1] Init torch distributed ends. mem usage=3.37 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-11-17 17:36:34 TP2] Load weight begin. avail mem=73.11 GB
[2025-11-17 17:36:34 TP1] Load weight begin. avail mem=73.05 GB
[2025-11-17 17:36:34 TP5] Load weight begin. avail mem=73.64 GB
[2025-11-17 17:36:34 TP3] Load weight begin. avail mem=73.39 GB
[2025-11-17 17:36:34 TP4] Load weight begin. avail mem=73.43 GB
[2025-11-17 17:36:34 TP7] Load weight begin. avail mem=73.42 GB
[2025-11-17 17:36:34 TP0] Load weight begin. avail mem=74.07 GB
[2025-11-17 17:36:34 TP6] Load weight begin. avail mem=73.54 GB
Loading safetensors checkpoint shards:   0% Completed | 0/84 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/84 [00:00<01:08,  1.22it/s]
Loading safetensors checkpoint shards:   2% Completed | 2/84 [00:01<01:21,  1.00it/s]
Loading safetensors checkpoint shards:   4% Completed | 3/84 [00:02<01:22,  1.02s/it]
Loading safetensors checkpoint shards:   5% Completed | 4/84 [00:04<01:29,  1.12s/it]
Loading safetensors checkpoint shards:   6% Completed | 5/84 [00:05<01:39,  1.26s/it]
Loading safetensors checkpoint shards:   7% Completed | 6/84 [00:07<01:43,  1.33s/it]
Loading safetensors checkpoint shards:   8% Completed | 7/84 [00:08<01:36,  1.25s/it]
Loading safetensors checkpoint shards:  10% Completed | 8/84 [00:09<01:35,  1.26s/it]
Loading safetensors checkpoint shards:  11% Completed | 9/84 [00:10<01:31,  1.22s/it]
Loading safetensors checkpoint shards:  12% Completed | 10/84 [00:11<01:25,  1.16s/it]
Loading safetensors checkpoint shards:  13% Completed | 11/84 [00:12<01:21,  1.12s/it]
Loading safetensors checkpoint shards:  14% Completed | 12/84 [00:13<01:16,  1.07s/it]
Loading safetensors checkpoint shards:  15% Completed | 13/84 [00:14<01:09,  1.02it/s]
Loading safetensors checkpoint shards:  17% Completed | 14/84 [00:15<01:05,  1.06it/s]
Loading safetensors checkpoint shards:  18% Completed | 15/84 [00:16<01:05,  1.05it/s]
Loading safetensors checkpoint shards:  19% Completed | 16/84 [00:17<01:02,  1.09it/s]
Loading safetensors checkpoint shards:  20% Completed | 17/84 [00:18<01:02,  1.08it/s]
Loading safetensors checkpoint shards:  21% Completed | 18/84 [00:19<01:01,  1.07it/s]
Loading safetensors checkpoint shards:  23% Completed | 19/84 [00:20<01:05,  1.01s/it]
Loading safetensors checkpoint shards:  24% Completed | 20/84 [00:21<01:11,  1.12s/it]
Loading safetensors checkpoint shards:  25% Completed | 21/84 [00:22<01:12,  1.16s/it]
Loading safetensors checkpoint shards:  26% Completed | 22/84 [00:24<01:11,  1.15s/it]
Loading safetensors checkpoint shards:  27% Completed | 23/84 [00:25<01:09,  1.14s/it]
Loading safetensors checkpoint shards:  29% Completed | 24/84 [00:26<01:08,  1.14s/it]
Loading safetensors checkpoint shards:  30% Completed | 25/84 [00:27<01:07,  1.15s/it]
Loading safetensors checkpoint shards:  31% Completed | 26/84 [00:28<01:03,  1.09s/it]
Loading safetensors checkpoint shards:  32% Completed | 27/84 [00:29<01:03,  1.11s/it]
Loading safetensors checkpoint shards:  33% Completed | 28/84 [00:30<01:02,  1.12s/it]
Loading safetensors checkpoint shards:  35% Completed | 29/84 [00:32<01:06,  1.21s/it]
Loading safetensors checkpoint shards:  36% Completed | 30/84 [00:33<01:06,  1.24s/it]
Loading safetensors checkpoint shards:  37% Completed | 31/84 [00:34<01:09,  1.30s/it]
Loading safetensors checkpoint shards:  38% Completed | 32/84 [00:36<01:06,  1.29s/it]
Loading safetensors checkpoint shards:  39% Completed | 33/84 [00:37<01:00,  1.19s/it]
Loading safetensors checkpoint shards:  40% Completed | 34/84 [00:38<00:57,  1.16s/it]
Loading safetensors checkpoint shards:  42% Completed | 35/84 [00:39<00:57,  1.18s/it]
Loading safetensors checkpoint shards:  43% Completed | 36/84 [00:40<00:57,  1.19s/it]
Loading safetensors checkpoint shards:  44% Completed | 37/84 [00:41<00:55,  1.18s/it]
Loading safetensors checkpoint shards:  45% Completed | 38/84 [00:42<00:52,  1.15s/it]
Loading safetensors checkpoint shards:  46% Completed | 39/84 [00:43<00:48,  1.09s/it]
Loading safetensors checkpoint shards:  48% Completed | 40/84 [00:44<00:47,  1.08s/it]
Loading safetensors checkpoint shards:  49% Completed | 41/84 [00:45<00:46,  1.09s/it]
Loading safetensors checkpoint shards:  50% Completed | 42/84 [00:47<00:49,  1.17s/it]
Loading safetensors checkpoint shards:  51% Completed | 43/84 [00:48<00:48,  1.17s/it]
Loading safetensors checkpoint shards:  52% Completed | 44/84 [00:49<00:47,  1.19s/it]
Loading safetensors checkpoint shards:  54% Completed | 45/84 [00:50<00:47,  1.22s/it]
Loading safetensors checkpoint shards:  55% Completed | 46/84 [00:52<00:45,  1.19s/it]
Loading safetensors checkpoint shards:  56% Completed | 47/84 [00:53<00:44,  1.21s/it]
Loading safetensors checkpoint shards:  57% Completed | 48/84 [00:54<00:42,  1.17s/it]
Loading safetensors checkpoint shards:  58% Completed | 49/84 [00:55<00:34,  1.01it/s]
Loading safetensors checkpoint shards:  60% Completed | 50/84 [00:55<00:31,  1.07it/s]
Loading safetensors checkpoint shards:  61% Completed | 51/84 [00:56<00:30,  1.08it/s]
Loading safetensors checkpoint shards:  62% Completed | 52/84 [00:57<00:29,  1.09it/s]
Loading safetensors checkpoint shards:  63% Completed | 53/84 [00:58<00:29,  1.07it/s]
Loading safetensors checkpoint shards:  64% Completed | 54/84 [00:59<00:28,  1.05it/s]
Loading safetensors checkpoint shards:  65% Completed | 55/84 [01:00<00:27,  1.05it/s]
Loading safetensors checkpoint shards:  67% Completed | 56/84 [01:01<00:25,  1.08it/s]
Loading safetensors checkpoint shards:  68% Completed | 57/84 [01:02<00:24,  1.11it/s]
Loading safetensors checkpoint shards:  69% Completed | 58/84 [01:03<00:22,  1.15it/s]
Loading safetensors checkpoint shards:  70% Completed | 59/84 [01:04<00:23,  1.08it/s]
Loading safetensors checkpoint shards:  71% Completed | 60/84 [01:05<00:22,  1.05it/s]
Loading safetensors checkpoint shards:  73% Completed | 61/84 [01:06<00:21,  1.07it/s]
Loading safetensors checkpoint shards:  74% Completed | 62/84 [01:07<00:21,  1.02it/s]
Loading safetensors checkpoint shards:  75% Completed | 63/84 [01:08<00:22,  1.08s/it]
Loading safetensors checkpoint shards:  76% Completed | 64/84 [01:09<00:23,  1.18s/it]
Loading safetensors checkpoint shards:  77% Completed | 65/84 [01:11<00:22,  1.20s/it]
Loading safetensors checkpoint shards:  79% Completed | 66/84 [01:12<00:20,  1.13s/it]
Loading safetensors checkpoint shards:  80% Completed | 67/84 [01:13<00:18,  1.09s/it]
Loading safetensors checkpoint shards:  81% Completed | 68/84 [01:13<00:16,  1.04s/it]
Loading safetensors checkpoint shards:  82% Completed | 69/84 [01:14<00:14,  1.03it/s]
Loading safetensors checkpoint shards:  83% Completed | 70/84 [01:15<00:12,  1.08it/s]
Loading safetensors checkpoint shards:  85% Completed | 71/84 [01:16<00:11,  1.12it/s]
Loading safetensors checkpoint shards:  86% Completed | 72/84 [01:17<00:11,  1.04it/s]
Loading safetensors checkpoint shards:  87% Completed | 73/84 [01:18<00:11,  1.03s/it]
Loading safetensors checkpoint shards:  88% Completed | 74/84 [01:20<00:11,  1.18s/it]
Loading safetensors checkpoint shards:  89% Completed | 75/84 [01:21<00:10,  1.18s/it]
Loading safetensors checkpoint shards:  90% Completed | 76/84 [01:22<00:09,  1.16s/it]
Loading safetensors checkpoint shards:  92% Completed | 77/84 [01:23<00:07,  1.07s/it]
Loading safetensors checkpoint shards:  93% Completed | 78/84 [01:24<00:06,  1.01s/it]
Loading safetensors checkpoint shards:  94% Completed | 79/84 [01:25<00:04,  1.04it/s]
Loading safetensors checkpoint shards:  95% Completed | 80/84 [01:26<00:03,  1.03it/s]
Loading safetensors checkpoint shards:  96% Completed | 81/84 [01:26<00:02,  1.09it/s]
Loading safetensors checkpoint shards:  98% Completed | 82/84 [01:27<00:01,  1.18it/s]
Loading safetensors checkpoint shards:  99% Completed | 83/84 [01:28<00:00,  1.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 84/84 [01:28<00:00,  1.38it/s]
Loading safetensors checkpoint shards: 100% Completed | 84/84 [01:28<00:00,  1.06s/it]

[2025-11-17 17:38:04 TP0] Setting sliding_window_size to be attention_chunk_size: 8192
[2025-11-17 17:38:04 TP0] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=25.29 GB, mem usage=48.79 GB.
[2025-11-17 17:38:04 TP2] Setting sliding_window_size to be attention_chunk_size: 8192
[2025-11-17 17:38:04 TP2] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=24.32 GB, mem usage=48.79 GB.
[2025-11-17 17:38:04 TP1] Setting sliding_window_size to be attention_chunk_size: 8192
[2025-11-17 17:38:04 TP1] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=24.26 GB, mem usage=48.79 GB.
[2025-11-17 17:38:04 TP3] Setting sliding_window_size to be attention_chunk_size: 8192
[2025-11-17 17:38:04 TP3] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=24.60 GB, mem usage=48.79 GB.
[2025-11-17 17:38:04 TP7] Setting sliding_window_size to be attention_chunk_size: 8192
[2025-11-17 17:38:04 TP7] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=24.64 GB, mem usage=48.79 GB.
[2025-11-17 17:38:04 TP5] Setting sliding_window_size to be attention_chunk_size: 8192
[2025-11-17 17:38:04 TP5] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=24.85 GB, mem usage=48.79 GB.
[2025-11-17 17:38:04 TP6] Setting sliding_window_size to be attention_chunk_size: 8192
[2025-11-17 17:38:04 TP4] Setting sliding_window_size to be attention_chunk_size: 8192
[2025-11-17 17:38:04 TP6] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=24.75 GB, mem usage=48.79 GB.
[2025-11-17 17:38:04 TP4] Load weight end. type=Llama4ForConditionalGeneration, dtype=torch.bfloat16, avail mem=24.64 GB, mem usage=48.79 GB.
[2025-11-17 17:38:04 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-17 17:38:04 TP0] KV Cache is allocated. #tokens: 206971, K size: 2.37 GB, V size: 2.37 GB
[2025-11-17 17:38:04 TP2] KV Cache is allocated. #tokens: 206971, K size: 2.37 GB, V size: 2.37 GB
[2025-11-17 17:38:04 TP7] KV Cache is allocated. #tokens: 206971, K size: 2.37 GB, V size: 2.37 GB
[2025-11-17 17:38:04 TP0] Memory pool end. avail mem=12.41 GB
[2025-11-17 17:38:04 TP2] Memory pool end. avail mem=11.44 GB
[2025-11-17 17:38:04 TP6] KV Cache is allocated. #tokens: 206971, K size: 2.37 GB, V size: 2.37 GB
[2025-11-17 17:38:04 TP7] Memory pool end. avail mem=11.76 GB
[2025-11-17 17:38:04 TP4] KV Cache is allocated. #tokens: 206971, K size: 2.37 GB, V size: 2.37 GB
[2025-11-17 17:38:04 TP1] KV Cache is allocated. #tokens: 206971, K size: 2.37 GB, V size: 2.37 GB
[2025-11-17 17:38:04 TP6] Memory pool end. avail mem=11.87 GB
[2025-11-17 17:38:04 TP4] Memory pool end. avail mem=11.76 GB
[2025-11-17 17:38:04 TP3] KV Cache is allocated. #tokens: 206971, K size: 2.37 GB, V size: 2.37 GB
[2025-11-17 17:38:04 TP5] KV Cache is allocated. #tokens: 206971, K size: 2.37 GB, V size: 2.37 GB
[2025-11-17 17:38:04 TP1] Memory pool end. avail mem=11.39 GB
[2025-11-17 17:38:04 TP3] Memory pool end. avail mem=11.72 GB
[2025-11-17 17:38:04 TP5] Memory pool end. avail mem=11.97 GB
[2025-11-17 17:38:06 TP5] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2712, in run_scheduler_process
    scheduler = Scheduler(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 312, in __init__
    self.tp_worker = TpModelWorker(
  File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 237, in __init__
    self._model_runner = ModelRunner(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 325, in __init__
    self.initialize(min_per_gpu_memory)
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 491, in initialize
    self.init_attention_backend()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1940, in init_attention_backend
    self.attn_backend = self._get_attention_backend()
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1974, in _get_attention_backend
    attn_backend = self._get_attention_backend_from_str(
  File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1991, in _get_attention_backend_from_str
    full_attention_backend = ATTENTION_BACKENDS[backend_str](self)
  File "/sgl-workspace/sglang/python/sglang/srt/layers/attention/attention_registry.py", line 61, in create_aiter_backend
    return AiterAttnBackend(runner)
  File "/sgl-workspace/sglang/python/sglang/srt/layers/attention/aiter_backend.py", line 133, in __init__
    self.workspace_buffer = torch.empty(
torch.OutOfMemoryError: HIP out of memory. Tried to allocate 20.31 GiB. GPU 5 has a total capacity of 191.98 GiB of which 11.65 GiB is free. Of the allocated memory 61.54 GiB is allocated by PyTorch, and 237.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-11-17 17:38:06] Received sigquit from a child process. It usually means the child failed.
Killed
