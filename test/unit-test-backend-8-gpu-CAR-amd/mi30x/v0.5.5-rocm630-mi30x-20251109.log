==========================================
SGL Unit Test Log
==========================================
Test: test_custom_allreduce.TestCustomAllReduce
Image: rocm/sgl-dev:v0.5.5-rocm630-mi30x-20251109
Container: sgl-dev_v0.5.5-rocm630-mi30x-20251109
Hardware: mi30x
Start time: 2025-11-10 10:36:02 CST
Test directory: /sgl-workspace/sglang/test/srt
Command: CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python3 -m unittest test_custom_allreduce.TestCustomAllReduce
==========================================

[Test Method] test_eager_allreduce
/usr/local/lib/python3.12/dist-packages/ray/_private/node.py:1362: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2025-11-10_16-36-09_815475_19/logs/gcs_server.out' mode='a' encoding='utf-8'>
  self.start_gcs_server()
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/usr/local/lib/python3.12/dist-packages/ray/_private/node.py:1362: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2025-11-10_16-36-09_815475_19/logs/gcs_server.err' mode='a' encoding='utf-8'>
  self.start_gcs_server()
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/usr/local/lib/python3.12/dist-packages/ray/_private/node.py:1367: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2025-11-10_16-36-09_815475_19/logs/monitor.out' mode='a' encoding='utf-8'>
  self.start_monitor()
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/usr/local/lib/python3.12/dist-packages/ray/_private/node.py:1367: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2025-11-10_16-36-09_815475_19/logs/monitor.err' mode='a' encoding='utf-8'>
  self.start_monitor()
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/usr/local/lib/python3.12/dist-packages/ray/_private/node.py:1378: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2025-11-10_16-36-09_815475_19/logs/dashboard.err' mode='a' encoding='utf-8'>
  self.start_api_server(
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/usr/local/lib/python3.12/dist-packages/ray/_private/utils.py:505: ResourceWarning: unclosed file <_io.TextIOWrapper name='/sys/fs/cgroup/cpu.max' mode='r' encoding='utf-8'>
  max_file = open(cpu_max_file_name).read()
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/usr/local/lib/python3.12/dist-packages/ray/_private/node.py:1420: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2025-11-10_16-36-09_815475_19/logs/raylet.out' mode='a' encoding='utf-8'>
  self.start_raylet(plasma_directory, object_store_memory)
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/usr/local/lib/python3.12/dist-packages/ray/_private/node.py:1420: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2025-11-10_16-36-09_815475_19/logs/raylet.err' mode='a' encoding='utf-8'>
  self.start_raylet(plasma_directory, object_store_memory)
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/usr/local/lib/python3.12/dist-packages/ray/_private/node.py:1422: ResourceWarning: unclosed file <_io.TextIOWrapper name='/tmp/ray/session_2025-11-10_16-36-09_815475_19/logs/log_monitor.err' mode='a' encoding='utf-8'>
  self.start_log_monitor()
ResourceWarning: Enable tracemalloc to get the object allocation traceback
2025-11-10 16:36:10,868	INFO worker.py:1821 -- Started a local Ray instance.
Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 2466, in retry
    return fn()
           ^^^^
  File "/sgl-workspace/sglang/python/sglang/test/test_utils.py", line 1683, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/unittest/case.py", line 589, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 85, in test_eager_allreduce
    multi_process_parallel(world_size, self, self.eager_allreduce)
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 53, in multi_process_parallel
    ray.get(refs)
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2755, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 906, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ValueError): [36mray::eager_allreduce()[39m (pid=479, ip=10.194.129.138)
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 168, in eager_allreduce
    out1 = tensor_model_parallel_all_reduce(inp1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
    return get_tp_group().all_reduce(input_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 582, in all_reduce
    if self.pynccl_comm is not None and self.is_symmetric_memory_enabled():
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_allocator.py", line 69, in is_symmetric_memory_enabled
    return get_global_server_args().enable_symm_mem
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/server_args.py", line 3991, in get_global_server_args
    raise ValueError("Global server args is not set yet!")
ValueError: Global server args is not set yet!
E[Test Method] test_graph_allreduce
Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 2466, in retry
    return fn()
           ^^^^
  File "/sgl-workspace/sglang/python/sglang/test/test_utils.py", line 1683, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/unittest/case.py", line 589, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 79, in test_graph_allreduce
    multi_process_parallel(world_size, self, self.graph_allreduce)
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 47, in multi_process_parallel
    ray.init(log_to_driver=True)
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 1658, in init
    raise RuntimeError(
RuntimeError: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.
E
======================================================================
ERROR: test_eager_allreduce (test_custom_allreduce.TestCustomAllReduce.test_eager_allreduce)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 2466, in retry
    return fn()
           ^^^^
  File "/sgl-workspace/sglang/python/sglang/test/test_utils.py", line 1683, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/unittest/case.py", line 589, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 85, in test_eager_allreduce
    multi_process_parallel(world_size, self, self.eager_allreduce)
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 53, in multi_process_parallel
    ray.get(refs)
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2755, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 906, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ValueError): [36mray::eager_allreduce()[39m (pid=479, ip=10.194.129.138)
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 168, in eager_allreduce
    out1 = tensor_model_parallel_all_reduce(inp1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
    return get_tp_group().all_reduce(input_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 582, in all_reduce
    if self.pynccl_comm is not None and self.is_symmetric_memory_enabled():
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_allocator.py", line 69, in is_symmetric_memory_enabled
    return get_global_server_args().enable_symm_mem
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/server_args.py", line 3991, in get_global_server_args
    raise ValueError("Global server args is not set yet!")
ValueError: Global server args is not set yet!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/test/test_utils.py", line 1682, in _callTestMethod
    retry(
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 2471, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

======================================================================
ERROR: test_graph_allreduce (test_custom_allreduce.TestCustomAllReduce.test_graph_allreduce)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 2466, in retry
    return fn()
           ^^^^
  File "/sgl-workspace/sglang/python/sglang/test/test_utils.py", line 1683, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/unittest/case.py", line 589, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 79, in test_graph_allreduce
    multi_process_parallel(world_size, self, self.graph_allreduce)
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 47, in multi_process_parallel
    ray.init(log_to_driver=True)
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 1658, in init
    raise RuntimeError(
RuntimeError: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/test/test_utils.py", line 1682, in _callTestMethod
    retry(
  File "/sgl-workspace/sglang/python/sglang/srt/utils/common.py", line 2471, in retry
    raise Exception(f"retry() exceed maximum number of retries.")
Exception: retry() exceed maximum number of retries.

----------------------------------------------------------------------
Ran 2 tests in 11.090s

FAILED (errors=2)
2025-11-10 16:36:20,913	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::eager_allreduce()[39m (pid=477, ip=10.194.129.138)
  File "/sgl-workspace/sglang/test/srt/test_custom_allreduce.py", line 168, in eager_allreduce
    out1 = tensor_model_parallel_all_reduce(inp1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce
    return get_tp_group().all_reduce(input_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py", line 582, in all_reduce
    if self.pynccl_comm is not None and self.is_symmetric_memory_enabled():
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_allocator.py", line 69, in is_symmetric_memory_enabled
    return get_global_server_args().enable_symm_mem
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/server_args.py", line 3991, in get_global_server_args
    raise ValueError("Global server args is not set yet!")
ValueError: Global server args is not set yet!

==========================================
End time: 2025-11-10 10:36:28 CST
Exit code: 1
Result: FAILED
==========================================
