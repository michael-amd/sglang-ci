INFO 10-26 11:30:40 __init__.py:179] Automatically detected platform rocm.
WARNING 10-26 11:30:40 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-26 11:30:42] WARNING server_args.py:1104: Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-26 11:30:42] WARNING server_args.py:1293: DP attention is enabled. The chunked prefill size is adjusted to 16384 to avoid MoE kernel issues. 
[2025-10-26 11:30:42] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-26 11:30:42] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.7200000000000001, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=0.3, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=875452334, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=8, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=True, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-26 11:30:42] Using default HuggingFace chat template with detected content format: string
INFO 10-26 11:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-26 11:30:50 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-26 11:30:51] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-26 11:30:51] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
INFO 10-26 11:30:59 __init__.py:179] Automatically detected platform rocm.
INFO 10-26 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-26 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-26 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-26 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-26 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-26 11:31:00 __init__.py:179] Automatically detected platform rocm.
INFO 10-26 11:31:00 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-26 11:31:01] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-26 11:31:01 DP7 TP7] Process 822 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-26 11:31:02 DP7 TP7] Init torch distributed begin.
[2025-10-26 11:31:02] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-26 11:31:02] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-26 11:31:02 DP0 TP0] Process 815 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-10-26 11:31:02 DP3 TP3] Process 818 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-10-26 11:31:02 DP0 TP0] Init torch distributed begin.
[2025-10-26 11:31:02 DP3 TP3] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-26 11:31:02] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-26 11:31:02] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-26 11:31:02 DP1 TP1] Process 816 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-10-26 11:31:02 DP2 TP2] Process 817 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-26 11:31:02] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-26 11:31:02] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-26 11:31:02 DP4 TP4] Process 819 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-10-26 11:31:02 DP6 TP6] Process 821 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-26 11:31:03] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-26 11:31:03 DP1 TP1] Init torch distributed begin.
[2025-10-26 11:31:03 DP2 TP2] Init torch distributed begin.
[2025-10-26 11:31:03 DP5 TP5] Process 820 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-10-26 11:31:03 DP4 TP4] Init torch distributed begin.
[2025-10-26 11:31:03 DP6 TP6] Init torch distributed begin.
[2025-10-26 11:31:03 DP5 TP5] Init torch distributed begin.
[2025-10-26 11:31:03 DP0 TP0] sglang is using nccl==2.21.5
[2025-10-26 11:31:05 DP0 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-10-26 11:31:05 DP7 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-10-26 11:31:05 DP6 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-10-26 11:31:05 DP5 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-10-26 11:31:05 DP4 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-10-26 11:31:05 DP3 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-10-26 11:31:05 DP1 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-10-26 11:31:05 DP2 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-10-26 11:31:07 DP7 TP7] Load weight begin. avail mem=187.32 GB
[2025-10-26 11:31:07 DP6 TP6] Load weight begin. avail mem=187.31 GB
[2025-10-26 11:31:07 DP4 TP4] Load weight begin. avail mem=187.25 GB
[2025-10-26 11:31:07 DP3 TP3] Load weight begin. avail mem=187.20 GB
[2025-10-26 11:31:07 DP2 TP2] Load weight begin. avail mem=187.19 GB
[2025-10-26 11:31:07 DP1 TP1] Load weight begin. avail mem=187.19 GB
[2025-10-26 11:31:07 DP5 TP5] Load weight begin. avail mem=187.33 GB
[2025-10-26 11:31:07 DP0 TP0] Load weight begin. avail mem=187.61 GB
[2025-10-26 11:31:07 DP0 TP0] Detected fp8 checkpoint.
[2025-10-26 11:31:07 DP0 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:30,  5.29it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:31,  5.13it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:25,  6.23it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:27,  5.84it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:24,  6.39it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:28,  5.55it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:25,  6.12it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:25,  5.96it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:23,  6.56it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:22,  6.91it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:02<00:47,  3.23it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:02<00:38,  3.89it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:02<00:36,  4.12it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:02<00:31,  4.77it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:02<00:31,  4.70it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:03<00:27,  5.39it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:03<00:24,  6.01it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:03<00:22,  6.44it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:03<00:31,  4.63it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:03<00:27,  5.25it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:03<00:23,  5.95it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:04<00:23,  5.89it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:04<00:22,  6.30it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:04<00:27,  5.08it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:04<00:26,  5.27it/s]
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:04<00:17,  7.90it/s]
Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:04<00:11, 11.59it/s]
Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:05<00:10, 12.09it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:05<00:08, 14.38it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:06<00:22,  5.66it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:06<00:17,  7.06it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:06<00:14,  8.43it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:06<00:11, 10.08it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:06<00:10, 11.73it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:06<00:07, 15.23it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:06<00:07, 14.72it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:07<00:07, 15.44it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:07<00:06, 16.89it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:07<00:05, 18.23it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:07<00:06, 16.50it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:07<00:05, 19.09it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:07<00:04, 21.23it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:08<00:10,  9.07it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:08<00:09,  9.65it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:08<00:08, 10.92it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:08<00:06, 13.66it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:09<00:05, 15.67it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:09<00:04, 16.33it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:09<00:04, 18.08it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:09<00:04, 16.43it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:09<00:03, 18.15it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:09<00:03, 18.93it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:09<00:03, 18.21it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:10<00:03, 18.98it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:10<00:03, 17.50it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:11<00:08,  6.83it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:11<00:06,  8.02it/s]
Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:11<00:04, 10.58it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:11<00:04, 11.63it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:11<00:03, 11.76it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:11<00:03, 13.99it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:12<00:02, 14.42it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:12<00:02, 16.92it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:12<00:01, 18.47it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:12<00:01, 20.83it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:12<00:01, 22.09it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:12<00:01, 15.47it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:12<00:01, 16.62it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:13<00:01, 16.59it/s]
Loading safetensors checkpoint shards:  88% Completed | 144/163 [00:13<00:01, 18.03it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:13<00:00, 18.44it/s]
Loading safetensors checkpoint shards:  92% Completed | 150/163 [00:13<00:00, 18.78it/s]
Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:13<00:00, 16.10it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:13<00:00, 15.93it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:14<00:00,  6.05it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:15<00:00,  8.02it/s]
Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:15<00:00,  9.29it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:15<00:00, 10.73it/s]

[2025-10-26 11:32:03 DP3 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.73 GB, mem usage=90.47 GB.
[2025-10-26 11:32:03 DP2 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.72 GB, mem usage=90.47 GB.
[2025-10-26 11:32:03 DP1 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.72 GB, mem usage=90.47 GB.
[2025-10-26 11:32:04 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=97.14 GB, mem usage=90.47 GB.
[2025-10-26 11:32:14 DP5 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.86 GB, mem usage=90.47 GB.
[2025-10-26 11:32:14 DP4 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.78 GB, mem usage=90.47 GB.
[2025-10-26 11:32:14 DP6 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.85 GB, mem usage=90.47 GB.
[2025-10-26 11:32:14 DP7 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.86 GB, mem usage=90.47 GB.
[2025-10-26 11:32:14 DP0 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-26 11:32:14 DP6 TP6] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-26 11:32:14 DP6 TP6] Memory pool end. avail mem=51.23 GB
[2025-10-26 11:32:14 DP7 TP7] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-26 11:32:14 DP7 TP7] Memory pool end. avail mem=51.24 GB
[2025-10-26 11:32:14 DP4 TP4] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-26 11:32:14 DP4 TP4] Memory pool end. avail mem=51.17 GB
[2025-10-26 11:32:14 DP3 TP3] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-26 11:32:14 DP5 TP5] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-26 11:32:14 DP3 TP3] Memory pool end. avail mem=51.12 GB
[2025-10-26 11:32:14 DP2 TP2] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-26 11:32:14 DP5 TP5] Memory pool end. avail mem=51.25 GB
[2025-10-26 11:32:14 DP2 TP2] Memory pool end. avail mem=51.11 GB
[2025-10-26 11:32:14 DP0 TP0] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-26 11:32:14 DP0 TP0] Memory pool end. avail mem=51.53 GB
[2025-10-26 11:32:14 DP1 TP1] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-26 11:32:14 DP1 TP1] Memory pool end. avail mem=51.11 GB
[2025-10-26 11:32:16 DP7 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=51.04 GB
[2025-10-26 11:32:16 DP2 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=50.90 GB
[2025-10-26 11:32:16 DP5 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=51.05 GB
[2025-10-26 11:32:16 DP3 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=50.91 GB
[2025-10-26 11:32:16 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=51.32 GB
[2025-10-26 11:32:16 DP0 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-26 11:32:16 DP1 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=50.91 GB
[2025-10-26 11:32:16 DP6 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=51.03 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=50.68 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-10-26 11:32:17 DP4 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=50.96 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-26 11:32:17 DP3 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-26 11:32:17 DP0 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-26 11:32:17 DP1 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-26 11:32:17 DP4 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-26 11:32:17 DP2 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-26 11:32:17 DP7 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-26 11:32:17 DP5 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-26 11:32:17 DP6 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-26 11:32:19 DP5 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-26 11:32:19 DP2 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-26 11:32:19 DP3 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-26 11:32:19 DP7 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-26 11:32:19 DP1 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-26 11:32:19 DP0 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP5 TP5] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP5 TP5] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP3 TP3] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP3 TP3] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP2 TP2] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP2 TP2] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP0 TP0] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP0 TP0] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP1 TP1] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP1 TP1] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP7 TP7] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:20 DP7 TP7] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-26 11:32:20 DP4 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-26 11:32:20 DP6 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:21 DP4 TP4] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:21 DP4 TP4] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:21 DP6 TP6] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:21 DP6 TP6] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP5 TP5] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP3 TP3] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP6 TP6] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP2 TP2] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP7 TP7] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP0 TP0] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP4 TP4] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP1 TP1] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP5 TP5] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP3 TP3] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP2 TP2] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP6 TP6] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP7 TP7] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP0 TP0] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP4 TP4] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:22 DP1 TP1] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:22 DP3 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:22 DP0 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:22 DP2 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:22 DP7 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:22 DP5 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:22 DP6 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:22 DP4 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:22 DP1 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank3]:[W1026 11:32:23.619152873 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank0]:[W1026 11:32:23.619179361 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank5]:[W1026 11:32:23.619288867 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank7]:[W1026 11:32:23.619313828 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank6]:[W1026 11:32:23.619290177 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank4]:[W1026 11:32:23.619338212 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Capturing batches (bs=512 avail_mem=50.68 GB):   2%|         | 1/52 [00:06<05:56,  6.99s/it]Capturing batches (bs=496 avail_mem=42.68 GB):   2%|         | 1/52 [00:06<05:56,  6.99s/it][rank2]:[W1026 11:32:24.353680410 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank1]:[W1026 11:32:24.353822371 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Capturing batches (bs=496 avail_mem=42.68 GB):   4%|         | 2/52 [00:07<02:44,  3.28s/it]Capturing batches (bs=480 avail_mem=42.68 GB):   4%|         | 2/52 [00:07<02:44,  3.28s/it]Capturing batches (bs=480 avail_mem=42.68 GB):   6%|         | 3/52 [00:08<01:43,  2.10s/it]Capturing batches (bs=464 avail_mem=42.67 GB):   6%|         | 3/52 [00:08<01:43,  2.10s/it]Capturing batches (bs=464 avail_mem=42.67 GB):   8%|         | 4/52 [00:09<01:14,  1.55s/it]Capturing batches (bs=448 avail_mem=42.66 GB):   8%|         | 4/52 [00:09<01:14,  1.55s/it]Capturing batches (bs=448 avail_mem=42.66 GB):  10%|         | 5/52 [00:09<00:58,  1.25s/it]Capturing batches (bs=432 avail_mem=42.65 GB):  10%|         | 5/52 [00:09<00:58,  1.25s/it]Capturing batches (bs=432 avail_mem=42.65 GB):  12%|        | 6/52 [00:10<00:48,  1.06s/it]Capturing batches (bs=416 avail_mem=42.65 GB):  12%|        | 6/52 [00:10<00:48,  1.06s/it]Capturing batches (bs=416 avail_mem=42.65 GB):  13%|        | 7/52 [00:11<00:41,  1.09it/s]Capturing batches (bs=400 avail_mem=42.64 GB):  13%|        | 7/52 [00:11<00:41,  1.09it/s]Capturing batches (bs=400 avail_mem=42.64 GB):  15%|        | 8/52 [00:11<00:37,  1.19it/s]Capturing batches (bs=384 avail_mem=42.63 GB):  15%|        | 8/52 [00:11<00:37,  1.19it/s]Capturing batches (bs=384 avail_mem=42.63 GB):  17%|        | 9/52 [00:12<00:31,  1.36it/s]Capturing batches (bs=368 avail_mem=42.62 GB):  17%|        | 9/52 [00:12<00:31,  1.36it/s]Capturing batches (bs=368 avail_mem=42.62 GB):  19%|        | 10/52 [00:12<00:29,  1.44it/s]Capturing batches (bs=352 avail_mem=42.62 GB):  19%|        | 10/52 [00:12<00:29,  1.44it/s]Capturing batches (bs=352 avail_mem=42.62 GB):  21%|        | 11/52 [00:13<00:28,  1.44it/s]Capturing batches (bs=336 avail_mem=42.61 GB):  21%|        | 11/52 [00:13<00:28,  1.44it/s]Capturing batches (bs=336 avail_mem=42.61 GB):  23%|       | 12/52 [00:14<00:26,  1.50it/s]Capturing batches (bs=320 avail_mem=42.60 GB):  23%|       | 12/52 [00:14<00:26,  1.50it/s]Capturing batches (bs=320 avail_mem=42.60 GB):  25%|       | 13/52 [00:14<00:25,  1.55it/s]Capturing batches (bs=304 avail_mem=42.60 GB):  25%|       | 13/52 [00:14<00:25,  1.55it/s]Capturing batches (bs=304 avail_mem=42.60 GB):  27%|       | 14/52 [00:15<00:22,  1.67it/s]Capturing batches (bs=288 avail_mem=42.59 GB):  27%|       | 14/52 [00:15<00:22,  1.67it/s]Capturing batches (bs=288 avail_mem=42.59 GB):  29%|       | 15/52 [00:15<00:20,  1.85it/s]Capturing batches (bs=272 avail_mem=42.58 GB):  29%|       | 15/52 [00:15<00:20,  1.85it/s]Capturing batches (bs=272 avail_mem=42.58 GB):  31%|       | 16/52 [00:16<00:20,  1.78it/s]Capturing batches (bs=256 avail_mem=42.58 GB):  31%|       | 16/52 [00:16<00:20,  1.78it/s][aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP6 TP6] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP6 TP6] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP6 TP6] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP6 TP6] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP3 TP3] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP3 TP3] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP3 TP3] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP3 TP3] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP5 TP5] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP5 TP5] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP5 TP5] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP5 TP5] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP1 TP1] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP1 TP1] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP1 TP1] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP1 TP1] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP7 TP7] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP7 TP7] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP7 TP7] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP7 TP7] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP4 TP4] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP4 TP4] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP4 TP4] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP4 TP4] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP2 TP2] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP2 TP2] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP2 TP2] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP2 TP2] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP0 TP0] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:33 DP0 TP0] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP0 TP0] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:33 DP0 TP0] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=256 avail_mem=42.58 GB):  33%|      | 17/52 [00:16<00:20,  1.74it/s]Capturing batches (bs=248 avail_mem=42.57 GB):  33%|      | 17/52 [00:16<00:20,  1.74it/s]Capturing batches (bs=248 avail_mem=42.57 GB):  35%|      | 18/52 [00:17<00:19,  1.72it/s]Capturing batches (bs=240 avail_mem=42.56 GB):  35%|      | 18/52 [00:17<00:19,  1.72it/s]Capturing batches (bs=240 avail_mem=42.56 GB):  37%|      | 19/52 [00:18<00:19,  1.70it/s]Capturing batches (bs=232 avail_mem=42.55 GB):  37%|      | 19/52 [00:18<00:19,  1.70it/s]Capturing batches (bs=232 avail_mem=42.55 GB):  38%|      | 20/52 [00:18<00:19,  1.68it/s]Capturing batches (bs=224 avail_mem=42.55 GB):  38%|      | 20/52 [00:18<00:19,  1.68it/s]Capturing batches (bs=224 avail_mem=42.55 GB):  40%|      | 21/52 [00:19<00:18,  1.69it/s]Capturing batches (bs=216 avail_mem=42.54 GB):  40%|      | 21/52 [00:19<00:18,  1.69it/s]Capturing batches (bs=216 avail_mem=42.54 GB):  42%|     | 22/52 [00:19<00:16,  1.77it/s]Capturing batches (bs=208 avail_mem=42.53 GB):  42%|     | 22/52 [00:19<00:16,  1.77it/s]Capturing batches (bs=208 avail_mem=42.53 GB):  44%|     | 23/52 [00:20<00:15,  1.84it/s]Capturing batches (bs=200 avail_mem=42.53 GB):  44%|     | 23/52 [00:20<00:15,  1.84it/s]Capturing batches (bs=200 avail_mem=42.53 GB):  46%|     | 24/52 [00:20<00:14,  1.88it/s]Capturing batches (bs=192 avail_mem=42.52 GB):  46%|     | 24/52 [00:20<00:14,  1.88it/s][aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP2 TP2] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP0 TP0] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP3 TP3] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP1 TP1] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP5 TP5] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP6 TP6] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP7 TP7] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP4 TP4] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:37 DP2 TP2] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:37 DP0 TP0] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:37 DP1 TP1] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:37 DP3 TP3] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:37 DP5 TP5] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:37 DP6 TP6] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:37 DP7 TP7] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:37 DP4 TP4] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP2 TP2] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP0 TP0] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP1 TP1] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP5 TP5] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP6 TP6] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP2 TP2] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP3 TP3] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP4 TP4] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP0 TP0] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP7 TP7] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP1 TP1] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP5 TP5] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP6 TP6] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP3 TP3] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP4 TP4] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:37 DP7 TP7] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=192 avail_mem=42.52 GB):  48%|     | 25/52 [00:21<00:14,  1.90it/s]Capturing batches (bs=184 avail_mem=42.52 GB):  48%|     | 25/52 [00:21<00:14,  1.90it/s]Capturing batches (bs=184 avail_mem=42.52 GB):  50%|     | 26/52 [00:21<00:12,  2.07it/s]Capturing batches (bs=176 avail_mem=42.51 GB):  50%|     | 26/52 [00:21<00:12,  2.07it/s]Capturing batches (bs=176 avail_mem=42.51 GB):  52%|    | 27/52 [00:22<00:11,  2.17it/s]Capturing batches (bs=168 avail_mem=42.51 GB):  52%|    | 27/52 [00:22<00:11,  2.17it/s]Capturing batches (bs=168 avail_mem=42.51 GB):  54%|    | 28/52 [00:22<00:12,  1.99it/s]Capturing batches (bs=160 avail_mem=42.50 GB):  54%|    | 28/52 [00:22<00:12,  1.99it/s]Capturing batches (bs=160 avail_mem=42.50 GB):  56%|    | 29/52 [00:23<00:10,  2.10it/s]Capturing batches (bs=152 avail_mem=42.49 GB):  56%|    | 29/52 [00:23<00:10,  2.10it/s]Capturing batches (bs=152 avail_mem=42.49 GB):  58%|    | 30/52 [00:23<00:11,  1.97it/s]Capturing batches (bs=144 avail_mem=42.49 GB):  58%|    | 30/52 [00:23<00:11,  1.97it/s]Capturing batches (bs=144 avail_mem=42.49 GB):  60%|    | 31/52 [00:24<00:10,  2.10it/s]Capturing batches (bs=136 avail_mem=42.48 GB):  60%|    | 31/52 [00:24<00:10,  2.10it/s]Capturing batches (bs=136 avail_mem=42.48 GB):  62%|   | 32/52 [00:24<00:09,  2.20it/s]Capturing batches (bs=128 avail_mem=42.47 GB):  62%|   | 32/52 [00:24<00:09,  2.20it/s][aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP7 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP2 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP1 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP3 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP0 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP4 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP5 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP6 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP7 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP2 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP1 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP3 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP0 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP4 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP5 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-26 11:32:41 DP6 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP2 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP1 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP7 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP5 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP0 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP4 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP3 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP2 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP6 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP1 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP7 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP5 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP0 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP4 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP3 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:41 DP6 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=128 avail_mem=42.47 GB):  63%|   | 33/52 [00:24<00:08,  2.27it/s]Capturing batches (bs=120 avail_mem=42.47 GB):  63%|   | 33/52 [00:24<00:08,  2.27it/s][aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:41 DP1 TP1] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:41 DP2 TP2] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:41 DP3 TP3] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:41 DP7 TP7] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:41 DP4 TP4] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:41 DP0 TP0] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:41 DP6 TP6] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:41 DP5 TP5] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.47 GB):  65%|   | 34/52 [00:25<00:08,  2.06it/s]Capturing batches (bs=112 avail_mem=42.46 GB):  65%|   | 34/52 [00:25<00:08,  2.06it/s][aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP2 TP2] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP5 TP5] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP0 TP0] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP4 TP4] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP6 TP6] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP1 TP1] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP7 TP7] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP3 TP3] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.46 GB):  67%|   | 35/52 [00:25<00:07,  2.17it/s]Capturing batches (bs=104 avail_mem=42.46 GB):  67%|   | 35/52 [00:25<00:07,  2.17it/s][aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP1 TP1] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP2 TP2] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP0 TP0] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP3 TP3] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP6 TP6] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP7 TP7] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP4 TP4] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:42 DP5 TP5] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.46 GB):  69%|   | 36/52 [00:26<00:08,  1.99it/s]Capturing batches (bs=96 avail_mem=42.45 GB):  69%|   | 36/52 [00:26<00:08,  1.99it/s] [aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP2 TP2] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP0 TP0] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP5 TP5] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP1 TP1] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP3 TP3] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP7 TP7] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP4 TP4] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP6 TP6] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.45 GB):  71%|   | 37/52 [00:26<00:07,  2.11it/s]Capturing batches (bs=88 avail_mem=42.45 GB):  71%|   | 37/52 [00:26<00:07,  2.11it/s][aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP3 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP2 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP5 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP1 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP0 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP4 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP7 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:43 DP6 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.45 GB):  73%|  | 38/52 [00:27<00:07,  1.95it/s]Capturing batches (bs=80 avail_mem=42.44 GB):  73%|  | 38/52 [00:27<00:07,  1.95it/s][aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP0 TP0] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP2 TP2] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP3 TP3] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP1 TP1] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP5 TP5] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP6 TP6] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP7 TP7] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP4 TP4] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.44 GB):  75%|  | 39/52 [00:27<00:06,  2.10it/s]Capturing batches (bs=72 avail_mem=42.43 GB):  75%|  | 39/52 [00:27<00:06,  2.10it/s][aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP3 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP7 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP1 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP4 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP5 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP0 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP6 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:44 DP2 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.43 GB):  77%|  | 40/52 [00:28<00:06,  1.95it/s]Capturing batches (bs=64 avail_mem=42.43 GB):  77%|  | 40/52 [00:28<00:06,  1.95it/s][aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP7 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP6 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP5 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP0 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP4 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP2 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP1 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP3 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP7 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP5 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP6 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP0 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP4 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP2 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP1 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP3 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP5 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP1 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP0 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP2 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP6 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP4 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP7 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP3 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP5 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP0 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP1 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP2 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP6 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP4 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP7 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:32:45 DP3 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP5 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP0 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP6 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP2 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP1 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP4 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP3 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP7 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.43 GB):  79%|  | 41/52 [00:28<00:05,  2.09it/s]Capturing batches (bs=56 avail_mem=42.42 GB):  79%|  | 41/52 [00:28<00:05,  2.09it/s][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP0 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP7 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP5 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP1 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP3 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP6 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP2 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:45 DP4 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.42 GB):  81%|  | 42/52 [00:29<00:05,  1.94it/s]Capturing batches (bs=48 avail_mem=42.41 GB):  81%|  | 42/52 [00:29<00:05,  1.94it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP2 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP1 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP4 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP6 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP0 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP3 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP5 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP7 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.41 GB):  83%| | 43/52 [00:29<00:04,  2.07it/s]Capturing batches (bs=40 avail_mem=42.41 GB):  83%| | 43/52 [00:29<00:04,  2.07it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP3 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP0 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP6 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP2 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP7 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP5 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP4 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:32:46 DP1 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.41 GB):  85%| | 44/52 [00:30<00:04,  1.92it/s]Capturing batches (bs=32 avail_mem=42.40 GB):  85%| | 44/52 [00:30<00:04,  1.92it/s][rank2]:W1026 11:32:49.488000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:32:49.498000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:32:49.506000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1026 11:32:49.533000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:32:49.541000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:32:49.550000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1026 11:32:49.564000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1026 11:32:49.573000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1026 11:32:49.581000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:32:49.608000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:32:49.616000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:32:49.625000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:32:49.661000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:32:49.671000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:32:49.678000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:32:49.705000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:32:49.712000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:32:49.721000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1026 11:32:49.960000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:32:49.969000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1026 11:32:50.037000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:32:50.045000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:32:50.134000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:32:50.142000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1026 11:32:50.226000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1026 11:32:50.233000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:32:50.241000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:32:50.269000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:32:50.278000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:32:50.284000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:32:50.340000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:32:50.346000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:32:50.354000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:32:50.391000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:32:50.399000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:32:50.413000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:32:50.425000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:32:50.431000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:32:50.438000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:32:50.467000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:32:50.475000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:32:50.480000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:32:50.492000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:32:50.498000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:32:50.505000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:32:50.534000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:32:50.541000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:32:50.547000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:32:50.698000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:32:50.705000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:32:50.826000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:32:50.833000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:32:50.894000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:32:50.900000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:32:50.960000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:32:50.971000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:32:51.020000 819 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank6]:W1026 11:32:51.026000 821 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank2]:W1026 11:32:51.037000 817 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank7]:W1026 11:32:51.061000 822 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank1]:W1026 11:32:51.079000 816 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank0]:W1026 11:32:51.089000 815 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank5]:W1026 11:32:51.663000 820 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank3]:W1026 11:32:51.718000 818 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0105 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0108 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0108 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_25 0.0109 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0112 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0114 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0115 ms 90.6% 
  triton_bmm_9 0.0115 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2700 seconds and 0.5350 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0105 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0106 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0107 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0108 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0109 ms 95.2% 
  triton_bmm_15 0.0110 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_9 0.0113 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0113 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0113 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3327 seconds and 0.5445 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_23 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_17 0.0107 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0109 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0109 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0111 ms 95.3% 
  triton_bmm_15 0.0115 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0116 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0117 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0119 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3336 seconds and 0.6272 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0107 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0109 ms 98.2% 
  triton_bmm_25 0.0110 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0110 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0112 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0114 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0117 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0117 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0118 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8059 seconds and 0.5378 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0107 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0107 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0107 ms 97.8% 
  triton_bmm_19 0.0107 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0108 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0111 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0115 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0116 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8700 seconds and 0.6437 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_23 0.0106 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_17 0.0108 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0109 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0109 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0110 ms 96.3% 
  triton_bmm_19 0.0111 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_22 0.0115 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_15 0.0116 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0118 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0120 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4046 seconds and 0.4670 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0106 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0108 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_25 0.0108 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0109 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0111 ms 94.2% 
  triton_bmm_15 0.0113 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0117 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_20 0.0117 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.1661 seconds and 0.5733 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0106 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0108 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_25 0.0109 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0110 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0112 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0114 ms 93.3% 
  triton_bmm_22 0.0115 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0117 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.0007 seconds and 0.4632 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1026 11:33:02.417000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:02.941000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:03.076000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:03.086000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:03.161000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:03.429000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:03.575000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:03.592000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:03.675000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1026 11:33:03.879000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:03.928000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:04.019000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:04.387000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1026 11:33:04.444000 815 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1026 11:33:04.532000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1026 11:33:05.060000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:05.074000 819 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank2]:W1026 11:33:05.076000 817 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank1]:W1026 11:33:05.103000 816 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1026 11:33:05.459000 821 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank5]:W1026 11:33:05.604000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:05.896000 822 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank3]:W1026 11:33:05.975000 818 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1026 11:33:07.613000 820 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_48 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0105 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0105 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0106 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0109 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_51 0.0109 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0109 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0109 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0111 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.4002 seconds and 0.6159 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0104 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0106 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0108 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0108 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0109 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0111 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0115 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3105 seconds and 0.6306 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0105 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0106 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0107 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0110 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0110 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0110 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0114 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_31 0.0115 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3443 seconds and 0.6530 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0105 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0105 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0105 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0107 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0111 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0115 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3502 seconds and 0.6487 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0104 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0107 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0107 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0108 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0108 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0112 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0115 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3684 seconds and 0.6171 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_46 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0104 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0105 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0105 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_49 0.0109 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0109 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0109 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0109 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0115 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0117 ms 88.7% 
SingleProcess AUTOTUNE benchmarking takes 5.2750 seconds and 0.6109 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0105 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0107 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0108 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0109 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0112 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0113 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3865 seconds and 0.6164 seconds precompiling for 27 choices
[rank0]:W1026 11:33:12.150000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:12.225000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:12.334000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:12.411000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:12.487000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:12.586000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:12.721000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:12.731000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:12.773000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:12.795000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:12.818000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:12.847000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:12.891000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:12.943000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:12.952000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:13.236000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:13.310000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:13.407000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:13.479000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:13.554000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_46 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0103 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0104 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0104 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0105 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0108 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0109 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0109 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0111 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0113 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2556 seconds and 0.6176 seconds precompiling for 27 choices
[rank3]:W1026 11:33:13.651000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:14.819000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:14.892000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:14.989000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:16.346000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:16.421000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:16.519000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:16 DP0 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1026 11:33:16.944000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:17.018000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:17.115000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:17 DP6 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1026 11:33:17.372000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:17.448000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:17.546000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:17 DP1 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1026 11:33:17.623000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:17.668000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:17.697000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:17.698000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:17.743000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:17.772000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:17.794000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:17.841000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:17 DP4 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1026 11:33:17.869000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:17 DP2 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:17 DP3 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1026 11:33:18.141000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:18.214000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:18.310000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:18 DP7 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1026 11:33:18.939000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:19.012000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:19.108000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:19 DP5 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1026 11:33:19.697000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:19.712000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:19.733000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:19.765000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:19.773000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:19.785000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:19.807000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:19.839000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:19.881000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:19.882000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:19.883000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:19.904000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:19 DP1 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1026 11:33:19.937000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:19.960000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:19 DP7 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:19 DP6 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:19 DP4 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1026 11:33:20.061000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:20 DP2 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1026 11:33:20.159000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:20.190000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:20.233000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:20.263000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:20.302000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:20.330000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:20.360000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:20.374000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:20 DP0 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:20 DP3 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1026 11:33:20.471000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:33:20 DP5 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1026 11:33:21.071000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:21.080000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:21.088000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:21.098000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:21.110000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:21.119000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:21.126000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:21.155000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:21.307000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:21.319000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:21.329000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:21.339000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:21.350000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:21.359000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:21.367000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:21.391000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:21.543000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:21.555000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:21.574000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:21.585000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:21.595000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:21.604000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:21.623000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:21.637000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:33:21 DP7 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:33:21 DP6 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:33:21 DP4 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:33:21 DP3 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:33:21 DP1 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:33:21 DP0 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:33:21 DP5 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:33:21 DP2 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1026 11:33:23.219000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:23.228000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:23.246000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:23.254000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:23.263000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:23.277000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:23.294000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:23.303000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:23.321000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:23.329000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:23.339000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:23.352000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:23.371000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:23.379000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:23.382000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:23.396000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:23.404000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:23.414000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:33:23 DP7 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank5]:W1026 11:33:23.427000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:33:23 DP1 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:33:23 DP4 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:33:23 DP3 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:33:23 DP2 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:33:23 DP5 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank0]:W1026 11:33:23.498000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1026 11:33:23.575000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:33:23 DP0 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank6]:W1026 11:33:23.659000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1026 11:33:23.732000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:23.807000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:33:23 DP6 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0145 ms 100.0% 
  triton_mm_55 0.0274 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0584 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0771 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1870 seconds and 0.6701 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0141 ms 100.0% 
  triton_mm_55 0.0276 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0423 ms 33.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0583 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0584 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0611 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0758 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2186 seconds and 0.4286 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0139 ms 100.0% 
  triton_mm_55 0.0274 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 33.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0504 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0581 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0778 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2114 seconds and 0.6541 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0150 ms 100.0% 
  triton_mm_55 0.0278 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 35.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0586 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0611 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0758 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2153 seconds and 0.6241 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0150 ms 100.0% 
  triton_mm_55 0.0275 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0422 ms 35.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0760 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1757 seconds and 0.4954 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0142 ms 100.0% 
  triton_mm_55 0.0275 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0423 ms 33.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0507 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0537 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0584 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0610 ms 23.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0752 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2301 seconds and 0.6827 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0147 ms 100.0% 
  triton_mm_55 0.0277 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0582 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0609 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0766 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2267 seconds and 0.3770 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0148 ms 100.0% 
  triton_mm_55 0.0312 ms 47.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0422 ms 35.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0504 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0533 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0581 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 24.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0829 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2692 seconds and 0.4319 seconds precompiling for 39 choices
[rank5]:W1026 11:33:34.301000 820 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81a730>
[rank4]:W1026 11:33:34.330000 819 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446df0>
[rank5]:W1026 11:33:34.333000 820 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81ae50>
[rank3]:W1026 11:33:34.341000 818 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e80>
[rank2]:W1026 11:33:34.353000 817 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f62539b6430>
[rank4]:W1026 11:33:34.362000 819 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446f40>
[rank3]:W1026 11:33:34.372000 818 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e20>
[rank2]:W1026 11:33:34.431000 817 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6252c5abe0>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:33:34 DP5 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:33:34 DP4 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:33:34 DP3 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1026 11:33:34.495000 815 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e670>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1026 11:33:34.506000 816 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba310>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1026 11:33:34.527000 815 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e820>
[rank1]:W1026 11:33:34.538000 816 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba8e0>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:33:34 DP2 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1026 11:33:34.590000 822 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6880>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1026 11:33:34.622000 822 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6790>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:33:34 DP0 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:33:34 DP1 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1026 11:33:34.689000 821 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b2fd0>
[rank6]:W1026 11:33:34.720000 821 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b3180>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:33:34 DP7 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:33:34 DP6 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1026 11:33:36.448000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:36.456000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:36.464000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:36.474000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:36.481000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:36.491000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:36.501000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:36.559000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:36.694000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:36.702000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:36.710000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:36.720000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:36.728000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:36.740000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:36.751000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:36.795000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:36.938000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:36.946000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:36.954000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:36.964000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:36.971000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:36.983000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:36.994000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:37.031000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:37.181000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:37.190000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:37.198000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:37.208000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:37.217000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:37.229000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:37.239000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:37.268000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:37.446000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:37.455000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:37.463000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:37.479000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:37.489000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:37.499000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:37.509000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:37.520000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:37.695000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:37.706000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:37.716000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:37.724000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:37.733000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:37.744000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:37.755000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:37.765000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:37.935000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:37.949000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:37.959000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:37.968000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:37.976000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:37.988000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:37.998000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:38.010000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:38.204000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:38.212000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:38.220000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:38.232000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:38.241000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:38.253000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:38.261000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:38.272000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:38.443000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:38.453000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:38.461000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:38.473000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:38.482000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:38.493000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:38.505000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:38.516000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:38.683000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:38.697000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:38.705000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:38.716000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:38.725000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:38.735000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:38.747000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:38.758000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:38.923000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:38.949000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:38.959000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:38.978000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:38.998000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:39.006000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:39.019000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:39.052000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:39.183000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:39.193000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:39.203000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:39.222000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:39.242000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:39.252000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:39.263000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:39.292000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:39.447000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:39.466000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:39.478000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:39.491000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:39.499000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:39.508000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:39.519000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:39.531000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:39.687000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:39.719000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:39.727000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:39.736000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:39.743000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:39.753000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:39.765000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:39.776000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:39.927000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:39.959000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:39.969000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:39.978000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:39.987000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:39.996000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:40.007000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:40.019000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:40.167000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:40.199000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:40.214000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:40.222000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:40.230000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:40.240000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:40.251000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:40.263000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:40.407000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:40.439000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:40.457000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:40.467000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:40.474000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:40.484000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:40.495000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:40.507000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:40.647000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:40.679000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:40.702000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:40.711000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:40.718000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:40.728000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:40.740000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:40.752000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:40.888000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:40.919000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:40.958000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:40.965000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:40.975000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:40.986000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:40.998000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:41.007000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:41.142000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:41.158000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:41.203000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:41.213000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:41.224000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:41.234000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:41.244000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:41.255000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:41.390000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:41.397000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:41.459000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:41.469000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:41.479000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:41.489000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:41.500000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:41.511000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:41.644000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:41.691000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:41.709000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:41.724000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:41.732000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:41.744000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:41.754000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:41.764000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:41.899000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:41.935000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:41.949000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:41.963000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:41.977000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:41.989000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:42.000000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:42.010000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:42.143000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:42.175000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:42.201000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:42.210000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:42.225000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:42.236000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:42.245000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:42.255000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:42.392000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:42.419000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:42.441000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:42.490000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:42.502000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:42.515000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:42.526000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:42.540000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:42.675000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:42.685000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:42.693000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:42.734000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:42.746000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:42.760000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:42.770000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:42.782000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:42.931000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:42.975000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:42.983000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:42.993000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:43.001000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:43.012000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:43.021000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:43.033000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:43.174000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:43.218000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:43.231000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:43.242000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:43.252000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:43.262000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:43.273000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:43.284000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:43.417000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:43.461000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:43.475000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:43.486000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:43.496000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:43.507000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:43.517000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:43.528000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:43.661000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:43.708000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:43.718000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:43.727000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:43.735000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:43.752000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:43.761000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:43.773000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:43.907000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:43.951000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:43.961000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:43.971000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:43.978000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:43.991000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:44.002000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:44.014000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:44.147000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:44.191000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:44.213000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:44.226000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:44.235000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:44.243000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:44.256000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:44.264000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:44.399000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:44.431000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:44.457000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:44.470000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:44.479000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:44.488000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:44.500000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:44.510000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:44.644000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:44.671000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:44.718000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:44.728000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:44.736000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:44.748000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:44.757000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:44.767000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:44.903000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:44.912000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:44.965000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:44.975000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:44.983000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:44.994000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:45.004000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:45.013000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:45.150000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:45.158000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:45.214000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:45.223000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:45.234000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:45.244000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:45.255000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:45.267000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:45.402000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:45.457000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:45.465000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:45.475000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:45.486000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:45.496000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:45.511000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:45.531000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:45.665000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:45.702000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:45.710000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:45.719000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:45.731000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:45.740000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:45.752000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:45.771000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:45.913000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:45.946000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:45.954000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:45.963000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:45.975000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:45.984000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:45.996000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:46.011000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:46.166000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:46.202000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:46.211000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:46.252000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:46.261000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:46.271000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:46.286000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:46.299000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:46.434000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:46.446000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:46.454000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:46.495000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:46.506000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:46.516000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:46.531000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:46.540000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:46.677000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:46.689000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:46.698000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:46.739000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:46.750000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:46.761000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:46.776000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:46.786000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:46.937000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:46.987000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:46.995000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:47.005000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:47.014000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:47.025000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:47.035000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:47.046000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:47.181000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:47.232000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:47.242000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:47.250000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:47.259000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:47.271000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:47.282000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:47.292000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:47.425000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:47.476000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:47.486000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:47.494000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:47.503000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:47.515000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:47.526000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:47.536000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:47.669000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:47.731000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:47.740000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:47.748000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:47.757000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:47.770000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:47.779000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:47.790000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:47.921000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:47.975000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:47.985000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:47.994000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:48.004000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:48.015000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:48.026000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:48.037000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:48.169000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:48.230000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:48.240000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:48.251000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:48.258000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:48.268000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:48.281000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:48.289000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:48.427000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:48.478000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:48.488000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:48.498000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:48.506000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:48.518000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:48.528000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:48.538000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:48.676000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:48.731000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:48.742000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:48.750000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:48.766000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:48.776000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:48.785000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:48.796000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:48.935000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:48.975000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:48.985000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:48.994000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:49.014000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:49.024000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:49.033000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:49.043000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:49.179000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:49.219000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:49.229000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:49.237000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:49.258000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:49.269000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:49.278000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:49.289000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:49.468000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:49.478000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:49.485000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:49.496000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:49.516000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:49.524000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:49.537000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:49.574000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:49.715000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:49.725000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:49.733000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:49.744000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:49.759000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:49.768000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:49.781000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:49.821000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:49.963000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:49.973000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:49.987000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:49.994000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:50.005000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:50.021000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:50.037000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:50.065000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:50.211000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:50.221000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:50.232000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:50.239000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:50.251000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:50.265000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:50.281000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:50.318000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:50.460000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:50.480000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:50.493000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:50.538000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:50.548000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:50.566000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:50.580000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:50.590000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:33:51.332000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:33:51.341000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:33:51.351000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:33:51.371000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:33:51.381000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:33:51.389000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:33:51.406000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:33:51.432000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1928 ms 100.0% 
  triton_mm_127 0.2481 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2564 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2611 ms 73.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2659 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2675 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2693 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2740 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2824 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2989 ms 64.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9484 seconds and 0.4039 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1828 ms 100.0% 
  triton_mm_127 0.2483 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2561 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2628 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2665 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2674 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2691 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2745 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2835 ms 64.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2996 ms 61.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9715 seconds and 0.5458 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1824 ms 100.0% 
  triton_mm_127 0.2488 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2571 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2624 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2653 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2666 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2695 ms 67.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2746 ms 66.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2871 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2983 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9501 seconds and 0.4477 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1822 ms 100.0% 
  triton_mm_127 0.2464 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2541 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2621 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2625 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2672 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2673 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2737 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2825 ms 64.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2958 ms 61.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9695 seconds and 0.4886 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1749 ms 100.0% 
  triton_mm_127 0.2465 ms 71.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2536 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2586 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2646 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_124 0.2649 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2650 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_125 0.2675 ms 65.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2749 ms 63.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2943 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9375 seconds and 0.4349 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1814 ms 100.0% 
  triton_mm_127 0.2462 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2543 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2606 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2635 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2656 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2683 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2719 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2823 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2960 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8830 seconds and 0.4049 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1799 ms 100.0% 
  triton_mm_127 0.2436 ms 73.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2517 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2585 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2627 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2630 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2639 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2660 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2723 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2928 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8188 seconds and 0.4195 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1745 ms 100.0% 
  triton_mm_127 0.2543 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2570 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2614 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2663 ms 65.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2667 ms 65.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2703 ms 64.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2748 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2851 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2986 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9359 seconds and 0.4075 seconds precompiling for 39 choices
Capturing batches (bs=32 avail_mem=42.40 GB):  87%| | 45/52 [01:50<02:50, 24.30s/it]Capturing batches (bs=24 avail_mem=41.77 GB):  87%| | 45/52 [01:50<02:50, 24.30s/it][rank4]:W1026 11:34:08.320000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:08.354000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:08.375000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:08.376000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:08.387000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:08.390000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:08.401000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:08.408000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:08.421000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:08.422000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:08.442000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:08.443000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:08.457000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:08.458000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:08.469000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:08.475000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:08.489000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:08.492000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:08.514000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:08.515000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:08.528000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:08.540000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:08.546000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:08.561000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:08.949000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:08.977000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:09.005000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:09.012000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:09.020000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:09.029000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:09.030000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:09.037000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:09.049000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:09.055000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:09.083000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:09.095000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:09.097000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:09.100000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:09.108000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:09.118000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:09.122000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:09.132000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:09.151000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:09.163000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:09.164000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:09.168000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:09.177000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:09.185000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:09.189000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:09.200000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:09.218000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:09.230000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:09.235000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:09.244000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:09.252000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:09.267000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:09.628000 819 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank7]:W1026 11:34:09.647000 822 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank5]:W1026 11:34:09.680000 820 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank1]:W1026 11:34:09.702000 816 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank0]:W1026 11:34:09.706000 815 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank6]:W1026 11:34:09.709000 821 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank3]:W1026 11:34:09.739000 818 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank2]:W1026 11:34:09.758000 817 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_144 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0100 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_153 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2396 seconds and 0.4447 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_148 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_151 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2712 seconds and 0.3474 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0104 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0104 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0105 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0105 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_149 0.0105 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0106 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0106 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3083 seconds and 0.4439 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0102 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_147 0.0102 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0102 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0105 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2914 seconds and 0.3168 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0102 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0102 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0102 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0109 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2944 seconds and 0.5150 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0104 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0104 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_153 0.0104 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0104 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0105 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0106 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2668 seconds and 0.3466 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_151 0.0103 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0103 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0103 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0104 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0106 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0109 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3127 seconds and 0.3574 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_144 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0099 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0100 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0100 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0100 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0101 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0101 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0106 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2643 seconds and 0.3676 seconds precompiling for 27 choices
[rank5]:W1026 11:34:20.738000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:20.893000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:20.927000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:20.951000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:20.986000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:21.033000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:21.132000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:21.243000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:21.390000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:21.425000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:21.452000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:21.483000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:21.527000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:21.628000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:21.679000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:22.175000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:22.534000 820 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank2]:W1026 11:34:22.709000 817 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank0]:W1026 11:34:22.817000 815 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank1]:W1026 11:34:22.930000 816 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank7]:W1026 11:34:22.933000 822 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank3]:W1026 11:34:22.945000 818 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank4]:W1026 11:34:23.017000 819 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank6]:W1026 11:34:23.669000 821 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0100 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0101 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0102 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0106 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0106 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0107 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0109 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0114 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2951 seconds and 0.5394 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0100 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_174 0.0102 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0103 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0103 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_165 0.0105 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0108 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_173 0.0113 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_159 0.0113 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3141 seconds and 0.5302 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0100 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_176 0.0103 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0103 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_177 0.0106 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0107 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0108 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0110 ms 89.5% 
SingleProcess AUTOTUNE benchmarking takes 5.2559 seconds and 0.5361 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0103 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0105 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_179 0.0108 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0109 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0109 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0109 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0115 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2509 seconds and 0.5581 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0099 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_176 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0105 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0105 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0107 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0109 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0111 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0111 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0115 ms 86.1% 
SingleProcess AUTOTUNE benchmarking takes 5.2797 seconds and 0.5486 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0100 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_179 0.0103 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0104 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_165 0.0105 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0105 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0107 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0110 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2714 seconds and 0.5870 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0100 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_176 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_174 0.0102 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0105 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0107 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0107 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_159 0.0110 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0111 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3529 seconds and 0.5668 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0100 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_176 0.0102 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_174 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0105 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0107 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0107 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_165 0.0108 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0111 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0113 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2792 seconds and 0.5398 seconds precompiling for 27 choices
[rank2]:W1026 11:34:29.783000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:29.857000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:29.901000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:29.954000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:29.970000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:29.977000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:30.014000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:30.030000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:30.045000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:30.077000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:30.089000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:30.105000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:30.132000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:30.144000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:30.172000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:30.187000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:30.203000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:30.206000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:30.248000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:30.305000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:30.345000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:30.684000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:30.760000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:30.859000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:34.497000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:34.572000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:34.590000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:34.661000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:34.666000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:34.670000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:34.709000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:34.716000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:34.727000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:34.736000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:34.753000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:34.766000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:34.784000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:34.791000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:34.802000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:34.828000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:34.836000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:34.883000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:34.889000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:34.901000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:34.926000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:35.206000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:35.282000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:35.380000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:36.015000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:36.086000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:36.093000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:36.094000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:36.109000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:36.164000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:36.172000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:36.176000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:36.186000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:36.191000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:36.198000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:36.253000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:36.268000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:36.268000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:36.273000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:36.276000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:36.286000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:36.310000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:36.354000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:36.369000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:36.374000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:36.665000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:36.740000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:36.839000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:37.402000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:37.411000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:37.421000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:37.432000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:37.442000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:37.453000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:37.464000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:37.482000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:37.654000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:37.662000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:37.673000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:37.683000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:37.694000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:37.704000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:37.715000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:37.725000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:37.902000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:37.910000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:37.920000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:37.931000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:37.943000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:37.953000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:37.965000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:37.973000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:39.513000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:39.532000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:39.577000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:39.586000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:39.589000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:39.593000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:39.602000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:39.606000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:39.611000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:39.637000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:39.653000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:39.654000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:39.661000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:39.668000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:39.676000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:39.686000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:39.701000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:39.709000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:39.716000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:39.716000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:39.724000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:39.734000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:39.791000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:39.839000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0129 ms 100.0% 
  triton_mm_183 0.0276 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0532 ms 24.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0597 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2292 seconds and 0.6953 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0132 ms 100.0% 
  triton_mm_183 0.0276 ms 47.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 22.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1816 seconds and 0.6414 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0133 ms 100.0% 
  triton_mm_183 0.0276 ms 48.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0498 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0755 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1710 seconds and 0.7780 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0134 ms 100.0% 
  triton_mm_183 0.0276 ms 48.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0581 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0597 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1582 seconds and 0.5287 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0131 ms 100.0% 
  triton_mm_183 0.0275 ms 47.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2765 seconds and 0.7007 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0128 ms 100.0% 
  triton_mm_183 0.0275 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0410 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0498 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0752 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1266 seconds and 0.4824 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0130 ms 100.0% 
  triton_mm_183 0.0275 ms 47.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0410 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0498 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0753 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2023 seconds and 0.4306 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0134 ms 100.0% 
  triton_mm_183 0.0276 ms 48.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0410 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0457 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0499 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0536 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0584 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0599 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2089 seconds and 0.4262 seconds precompiling for 39 choices
[rank6]:W1026 11:34:50.374000 821 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b2fd0>
[rank5]:W1026 11:34:50.391000 820 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81a730>
[rank6]:W1026 11:34:50.397000 821 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b3180>
[rank4]:W1026 11:34:50.409000 819 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446df0>
[rank5]:W1026 11:34:50.414000 820 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81ae50>
[rank7]:W1026 11:34:50.417000 822 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6880>
[rank4]:W1026 11:34:50.431000 819 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446f40>
[rank7]:W1026 11:34:50.440000 822 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6790>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:34:50 DP6 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:34:50 DP5 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:34:50 DP4 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:34:50 DP7 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1026 11:34:50.517000 816 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba310>
[rank1]:W1026 11:34:50.540000 816 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba8e0>
[rank0]:W1026 11:34:50.551000 815 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e670>
[rank0]:W1026 11:34:50.574000 815 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e820>
[rank3]:W1026 11:34:50.575000 818 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e80>
[rank3]:W1026 11:34:50.598000 818 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e20>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:34:50 DP0 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:34:50 DP3 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1026 11:34:50.688000 817 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f62539b6430>
[rank2]:W1026 11:34:50.712000 817 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6252c5abe0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:34:50 DP1 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:34:50 DP2 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1026 11:34:51.938000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:51.946000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:51.954000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:51.963000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:52.008000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:52.031000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:52.043000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:52.176000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:52.314000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:52.322000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:52.330000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:52.338000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:52.347000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:52.358000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:52.369000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:52.437000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:52.575000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:52.583000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:52.592000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:52.602000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:52.614000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:52.626000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:52.680000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:52.695000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:52.834000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:52.844000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:52.853000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:52.865000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:52.877000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:52.928000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:52.946000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:52.955000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:53.093000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:53.102000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:53.111000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:53.122000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:53.133000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:53.171000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:53.196000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:53.204000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:53.342000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:53.350000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:53.359000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:53.371000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:53.382000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:53.415000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:53.442000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:53.452000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:53.590000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:53.598000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:53.608000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:53.619000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:53.631000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:53.660000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:53.690000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:53.700000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:53.840000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:53.851000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:53.859000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:53.869000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:53.878000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:53.902000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:53.940000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:53.952000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:54.092000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:54.102000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:54.109000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:54.122000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:54.131000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:54.150000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:54.184000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:54.196000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:54.338000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:54.347000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:54.367000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:54.384000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:54.395000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:54.411000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:54.426000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:54.438000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:54.586000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:54.594000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:54.615000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:54.632000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:54.643000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:54.655000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:54.675000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:54.684000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:54.836000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:54.847000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:54.857000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:54.879000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:54.887000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:54.898000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:54.927000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:54.940000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:55.083000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:55.095000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:55.117000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:55.142000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:55.150000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:55.160000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:55.172000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:55.184000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:55.331000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:55.343000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:55.365000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:55.394000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:55.402000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:55.412000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:55.424000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:55.436000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:55.579000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:55.591000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:55.613000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:55.646000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:55.654000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:55.666000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:55.677000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:55.688000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:55.826000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:55.838000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:55.863000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:55.900000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:55.910000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:55.923000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:55.933000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:55.942000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:56.082000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:56.090000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:56.111000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:56.148000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:56.159000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:56.171000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:56.182000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:56.192000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:56.334000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:56.342000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:56.355000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:56.396000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:56.407000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:56.419000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:56.430000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:56.440000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:56.582000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:56.590000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:56.600000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:56.652000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:56.663000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:56.676000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:56.685000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:56.695000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:56.834000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:56.842000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:56.852000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:56.900000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:56.911000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:56.923000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:56.934000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:56.944000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:57.082000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:57.090000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:57.099000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:57.148000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:57.159000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:57.170000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:57.182000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:57.192000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:57.330000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:57.338000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:57.347000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:57.396000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:57.407000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:57.419000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:57.431000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:57.440000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:57.582000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:57.590000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:57.599000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:57.644000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:57.656000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:57.667000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:57.680000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:57.688000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:57.834000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:57.842000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:57.851000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:57.892000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:57.904000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:57.916000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:57.927000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:57.936000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:58.182000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:58.190000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:58.200000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:58.208000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:58.219000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:58.227000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:58.255000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:58.292000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:58.434000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:58.442000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:58.458000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:58.468000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:58.478000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:58.489000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:58.515000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:58.553000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:58.715000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:58.730000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:58.779000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:58.824000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:58.846000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:58.855000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:58.888000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:58.901000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:59.042000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:59.050000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:59.059000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:59.079000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:59.098000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:59.108000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:59.140000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:59.154000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:59.330000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:59.340000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:59.351000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:59.360000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:59.370000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:59.392000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:59.404000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:59.439000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:59.611000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:59.622000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:59.631000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:59.640000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:59.651000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:59.663000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:59.690000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:59.718000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:34:59.863000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:34:59.876000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:34:59.885000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:34:59.894000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:34:59.906000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:34:59.916000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:34:59.942000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:34:59.966000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:00.115000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:00.130000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:00.138000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:00.147000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:00.158000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:00.170000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:00.194000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:00.214000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:00.375000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:00.388000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:00.397000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:00.406000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:00.416000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:00.430000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:00.446000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:00.462000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:00.634000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:00.652000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:00.661000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:00.671000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:00.681000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:00.691000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:00.703000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:00.715000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:00.885000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:00.904000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:00.918000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:00.928000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:00.939000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:00.949000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:00.959000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:00.971000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:01.137000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:01.156000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:01.174000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:01.184000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:01.195000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:01.205000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:01.216000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:01.227000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:01.389000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:01.408000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:01.430000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:01.440000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:01.450000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:01.460000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:01.471000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:01.482000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:01.644000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:01.658000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:01.688000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:01.697000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:01.709000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:01.719000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:01.730000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:01.740000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:01.895000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:01.910000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:01.948000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:01.957000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:01.972000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:01.985000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:01.995000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:02.004000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:02.147000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:02.162000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:02.200000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:02.210000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:02.225000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:02.236000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:02.246000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:02.257000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:02.399000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:02.414000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:02.452000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:02.462000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:02.477000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:02.487000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:02.500000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:02.508000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:02.651000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:02.666000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:02.704000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:02.714000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:02.729000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:02.740000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:02.750000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:02.760000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:02.903000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:02.918000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:02.956000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:02.966000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:02.980000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:02.992000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:03.004000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:03.012000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:03.155000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:03.170000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:03.208000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:03.218000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:03.232000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:03.244000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:03.254000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:03.264000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:03.407000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:03.422000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:03.460000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:03.470000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:03.485000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:03.495000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:03.506000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:03.516000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:03.659000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:03.674000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:03.712000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:03.722000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:03.736000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:03.747000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:03.759000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:03.768000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:03.911000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:03.926000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:03.964000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:03.974000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:03.988000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:04.000000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:04.010000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:04.020000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:04.163000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:04.178000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:04.220000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:04.229000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:04.240000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:04.252000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:04.264000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:04.272000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:04.415000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:04.434000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:04.472000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:04.483000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:04.494000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:04.505000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:04.515000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:04.525000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:04.665000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:04.692000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:04.722000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:04.736000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:04.756000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:04.766000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:04.777000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:04.789000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:05.032000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:05.042000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:05.051000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:05.061000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:05.070000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:05.080000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:05.097000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:05.138000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:05.284000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:05.294000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:05.311000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:05.320000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:05.330000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:05.340000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:05.349000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:05.394000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:05.541000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:05.552000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:05.563000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:05.578000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:05.587000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:05.597000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:05.607000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:05.659000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:05.906000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:05.916000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:05.927000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:05.936000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:05.988000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:05.998000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:06.021000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:06.033000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:06.178000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:06.188000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:06.196000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:06.207000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:06.238000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:06.250000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:06.280000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:06.293000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:06.502000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:06.513000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:06.521000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:06.531000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:06.540000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:06.552000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:06.564000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:06.611000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:06.822000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:06.833000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:06.843000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:06.852000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:06.860000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:06.870000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:06.882000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:06.928000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:07.611000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:07.624000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:07.633000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:07.649000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:07.659000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:07.671000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:07.679000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:07.686000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1464 ms 100.0% 
  triton_mm_235 0.2158 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2166 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2392 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2439 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2454 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_241 0.2475 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2499 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2512 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2600 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8046 seconds and 0.7193 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1483 ms 100.0% 
  triton_mm_235 0.2254 ms 65.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2255 ms 65.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2420 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2487 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2504 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2533 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2564 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2567 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2656 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9030 seconds and 0.7401 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1476 ms 100.0% 
  triton_mm_234 0.2171 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_235 0.2190 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_233 0.2396 ms 61.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2449 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2452 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_241 0.2485 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2511 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2530 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2626 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8709 seconds and 0.7576 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1489 ms 100.0% 
  triton_mm_235 0.2228 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2241 ms 66.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2432 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2477 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2506 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2524 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2554 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2570 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2651 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9043 seconds and 0.6564 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1466 ms 100.0% 
  triton_mm_235 0.2210 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2233 ms 65.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2434 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2448 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2490 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2513 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2521 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2527 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2608 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8417 seconds and 0.3197 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1486 ms 100.0% 
  triton_mm_235 0.2249 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2251 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2424 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2475 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2516 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_227 0.2533 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2535 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2567 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2653 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9146 seconds and 0.6369 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1486 ms 100.0% 
  triton_mm_235 0.2243 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2251 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2434 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2477 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2498 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2529 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2557 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2560 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2663 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9018 seconds and 0.5687 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1476 ms 100.0% 
  triton_mm_235 0.2214 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2226 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2413 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2462 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2496 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2522 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2523 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2540 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2628 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9178 seconds and 0.3731 seconds precompiling for 39 choices
Capturing batches (bs=24 avail_mem=41.77 GB):  88%| | 46/52 [03:06<03:59, 39.93s/it]Capturing batches (bs=16 avail_mem=41.15 GB):  88%| | 46/52 [03:06<03:59, 39.93s/it][rank1]:W1026 11:35:24.746000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:24.756000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:24.767000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:24.778000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:24.788000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:24.799000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:24.814000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:24.815000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:24.821000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:24.823000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:24.834000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:24.845000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:24.865000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:24.866000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:24.893000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:24.895000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:24.896000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:24.898000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:24.906000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:24.916000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:24.937000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:24.938000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:24.965000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:24.970000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:25.382000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:25.389000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:25.398000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:25.410000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:25.430000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:25.437000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:25.457000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:25.461000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:25.466000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:25.474000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:25.477000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:25.489000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:25.509000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:25.519000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:25.529000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:25.539000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:25.544000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:25.545000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:25.550000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:25.557000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:25.577000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:25.588000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:25.596000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:25.607000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:25.613000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:25.613000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:25.619000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:25.624000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:25.645000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:25.655000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:25.675000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:25.687000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:26.048000 820 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank7]:W1026 11:35:26.064000 822 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank1]:W1026 11:35:26.070000 816 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank4]:W1026 11:35:26.077000 819 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank6]:W1026 11:35:26.098000 821 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank0]:W1026 11:35:26.108000 815 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank3]:W1026 11:35:26.129000 818 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank2]:W1026 11:35:26.144000 817 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_276 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_275 0.0093 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0093 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_277 0.0093 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7937 seconds and 0.2426 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_276 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_277 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_278 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_279 0.0093 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8053 seconds and 0.2068 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_274 0.0091 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0091 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0092 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_270 0.0092 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0093 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0093 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_277 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8737 seconds and 0.2426 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0091 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0091 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0094 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0094 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0094 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_277 0.0094 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8971 seconds and 0.1616 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0089 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0093 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_268 0.0093 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_269 0.0093 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9133 seconds and 0.1175 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0091 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0091 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0092 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0092 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0092 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0092 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0092 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_276 0.0093 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9034 seconds and 0.2127 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_272 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_276 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0090 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_277 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8538 seconds and 0.1221 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_277 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_276 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0094 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8296 seconds and 0.1372 seconds precompiling for 25 choices
[rank0]:W1026 11:35:36.316000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:36.462000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:36.543000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:36.601000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:36.808000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:36.817000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:36.838000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:36.884000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:36.959000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:37.039000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:37.100000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:37.313000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:37.336000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:37.391000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:37.553000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:38.057000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:38.128000 815 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank5]:W1026 11:35:38.414000 820 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank6]:W1026 11:35:38.423000 821 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank7]:W1026 11:35:38.489000 822 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank3]:W1026 11:35:38.805000 818 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank1]:W1026 11:35:38.833000 816 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank4]:W1026 11:35:38.854000 819 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank2]:W1026 11:35:39.373000 817 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0092 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0097 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0098 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0099 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8162 seconds and 0.4045 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0095 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0096 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_284 0.0096 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_298 0.0098 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0098 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0099 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8984 seconds and 0.3981 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0095 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0095 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0095 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0095 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_284 0.0098 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0098 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9022 seconds and 0.4082 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0089 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0096 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0097 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0099 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0099 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_300 0.0101 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_301 0.0102 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0921 seconds and 0.4041 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0094 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0095 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0097 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0101 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0101 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8848 seconds and 0.4061 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_284 0.0098 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_298 0.0098 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0098 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0098 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_299 0.0099 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0099 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0520 seconds and 0.4039 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_302 0.0095 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0095 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0096 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0098 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0099 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1013 seconds and 0.4041 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_293 0.0091 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0094 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0095 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0095 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0096 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0098 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0098 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9874 seconds and 0.3967 seconds precompiling for 25 choices
[rank0]:W1026 11:35:44.867000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:45.133000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:45.155000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:45.230000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:45.231000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:45.233000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:45.332000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:45.356000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:45.432000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:45.474000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:45.487000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:45.550000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:45.589000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:45.660000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:45.672000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:45.707000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:45.750000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:45.861000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:45.878000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:45.955000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:46.056000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:46.276000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:46.353000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:46.454000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:49.844000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:49.881000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:49.920000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:49.958000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:50.020000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:50.059000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:50 DP0 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:50 DP6 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1026 11:35:50.124000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:50.200000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:50.211000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:50.239000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:50.286000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:50.301000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:50.315000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:50 DP5 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1026 11:35:50.396000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:50.416000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:50 DP3 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:50 DP7 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1026 11:35:50.607000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:50.615000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:50.684000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:50.692000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:50.784000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:50.792000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:50 DP1 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:50 DP4 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1026 11:35:51.008000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:51.087000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:51.186000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:51 DP2 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1026 11:35:51.771000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:51.781000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:51.791000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:51.800000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:51.807000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:51.846000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:51.856000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:51.866000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:51.876000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:51.882000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:51.945000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:51.957000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:51.966000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:51.973000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:51.978000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:51.981000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:51.981000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:52 DP3 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:52 DP5 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:52 DP0 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:52 DP6 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:52 DP7 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1026 11:35:52.049000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:52.056000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:52.156000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:52.159000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:52 DP4 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:52 DP1 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1026 11:35:52.371000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:52.446000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:52.545000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:35:52 DP2 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1026 11:35:53.106000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:53.128000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:53.138000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:53.148000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:53.158000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:53.169000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:53.179000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:53.191000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:53.366000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:53.384000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:53.394000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:53.404000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:53.415000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:53.426000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:53.436000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:53.448000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:53.622000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:53.636000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:53.646000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:53.659000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:53.669000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:53.681000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:53.690000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:53.702000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:35:53 DP6 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:35:53 DP3 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:35:54 DP0 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:35:54 DP7 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:35:54 DP1 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:35:54 DP5 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:35:54 DP4 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:35:54 DP2 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1026 11:35:55.261000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:55.272000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:55.278000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:55.291000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:55.301000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:55.312000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:55.323000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:55.333000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:55.336000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:55.349000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:55.354000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:55.367000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:55.377000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:55.387000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:35:55.400000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:35:55.409000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:35:55.412000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:35:55.425000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:35:55.431000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:35:55.443000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:35:55.453000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:35:55.462000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:35:55 DP3 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank4]:W1026 11:35:55.476000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:35:55 DP6 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank2]:W1026 11:35:55.485000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:35:55 DP0 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:35:55 DP7 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:35:55 DP1 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:35:55 DP5 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:35:55 DP4 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:35:55 DP2 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0127 ms 100.0% 
  triton_mm_307 0.0271 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0581 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0611 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1471 seconds and 0.7341 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0126 ms 100.0% 
  triton_mm_307 0.0270 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0598 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0611 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2038 seconds and 0.6669 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0123 ms 100.0% 
  triton_mm_307 0.0272 ms 45.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0742 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2105 seconds and 0.7928 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0123 ms 100.0% 
  triton_mm_307 0.0271 ms 45.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0497 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0533 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0601 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2013 seconds and 0.7671 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0124 ms 100.0% 
  triton_mm_307 0.0272 ms 45.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0494 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0581 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0611 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2311 seconds and 0.6674 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0128 ms 100.0% 
  triton_mm_307 0.0271 ms 47.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0531 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0581 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2221 seconds and 0.6308 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0128 ms 100.0% 
  triton_mm_307 0.0271 ms 47.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0421 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0455 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0498 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0534 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0584 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0744 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2050 seconds and 0.4883 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0128 ms 100.0% 
  triton_mm_307 0.0272 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0455 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0533 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0601 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0744 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2496 seconds and 0.4356 seconds precompiling for 39 choices
[rank6]:W1026 11:36:05.971000 821 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b2fd0>
[rank6]:W1026 11:36:05.994000 821 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b3180>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:36:06 DP6 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1026 11:36:06.150000 817 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f62539b6430>
[rank2]:W1026 11:36:06.173000 817 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6252c5abe0>
[rank0]:W1026 11:36:06.174000 815 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e670>
[rank0]:W1026 11:36:06.197000 815 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e820>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:36:06 DP2 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1026 11:36:06.259000 816 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba310>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:36:06 DP0 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1026 11:36:06.266000 819 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446df0>
[rank7]:W1026 11:36:06.279000 822 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6880>
[rank1]:W1026 11:36:06.282000 816 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba8e0>
[rank4]:W1026 11:36:06.289000 819 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446f40>
[rank7]:W1026 11:36:06.302000 822 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6790>
[rank5]:W1026 11:36:06.343000 820 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81a730>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:36:06 DP1 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:36:06 DP4 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1026 11:36:06.366000 820 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81ae50>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:36:06 DP7 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1026 11:36:06.389000 818 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e80>
[rank3]:W1026 11:36:06.412000 818 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e20>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:36:06 DP5 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:36:06 DP3 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1026 11:36:07.853000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:07.862000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:07.870000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:07.879000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:07.890000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:07.900000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:07.911000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:07.965000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:08.113000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:08.130000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:08.139000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:08.153000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:08.163000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:08.175000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:08.221000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:08.332000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:08.487000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:08.496000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:08.507000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:08.518000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:08.530000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:08.591000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:08.601000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:08.632000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:08.855000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:08.866000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:08.877000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:08.888000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:08.898000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:08.907000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:08.918000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:08.967000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:09.227000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:09.236000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:09.244000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:09.255000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:09.266000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:09.275000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:09.284000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:09.336000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:09.483000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:09.494000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:09.503000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:09.517000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:09.529000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:09.540000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:09.548000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:09.591000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:09.738000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:09.756000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:09.767000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:09.776000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:09.786000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:09.799000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:09.810000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:09.848000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:10.005000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:10.016000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:10.027000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:10.046000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:10.058000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:10.068000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:10.080000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:10.117000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:10.270000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:10.280000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:10.290000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:10.310000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:10.321000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:10.333000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:10.343000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:10.388000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:10.535000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:10.544000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:10.553000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:10.566000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:10.578000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:10.588000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:10.601000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:10.644000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:10.791000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:10.802000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:10.811000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:10.823000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:10.833000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:10.844000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:10.856000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:10.900000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:11.043000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:11.062000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:11.071000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:11.081000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:11.091000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:11.102000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:11.114000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:11.152000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:11.299000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:11.318000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:11.327000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:11.338000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:11.350000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:11.360000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:11.372000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:11.408000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:11.553000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:11.574000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:11.583000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:11.595000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:11.605000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:11.616000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:11.628000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:11.660000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:11.815000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:11.834000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:11.843000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:11.852000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:11.863000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:11.875000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:11.885000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:11.912000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:12.075000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:12.090000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:12.099000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:12.108000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:12.130000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:12.141000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:12.153000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:12.176000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:12.327000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:12.346000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:12.355000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:12.364000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:12.382000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:12.396000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:12.409000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:12.428000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:12.579000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:12.606000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:12.614000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:12.624000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:12.634000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:12.652000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:12.664000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:12.680000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:12.831000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:12.866000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:12.874000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:12.884000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:12.893000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:12.908000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:12.921000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:12.933000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:13.083000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:13.126000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:13.135000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:13.145000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:13.153000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:13.165000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:13.178000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:13.190000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:13.335000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:13.386000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:13.395000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:13.404000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:13.414000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:13.426000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:13.439000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:13.452000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:13.598000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:13.648000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:13.659000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:13.671000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:13.682000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:13.692000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:13.703000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:13.714000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:13.862000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:13.904000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:13.916000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:13.927000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:13.939000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:13.951000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:13.960000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:13.971000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:14.118000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:14.161000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:14.172000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:14.184000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:14.195000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:14.210000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:14.219000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:14.230000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:14.378000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:14.416000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:14.428000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:14.441000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:14.451000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:14.470000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:14.479000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:14.490000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:14.640000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:14.671000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:14.682000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:14.694000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:14.704000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:14.732000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:14.743000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:14.756000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:14.903000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:14.930000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:14.939000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:14.950000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:14.960000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:14.988000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:15.001000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:15.013000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:15.159000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:15.190000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:15.199000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:15.209000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:15.217000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:15.243000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:15.254000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:15.266000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:15.414000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:15.453000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:15.463000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:15.475000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:15.486000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:15.502000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:15.511000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:15.522000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:15.672000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:15.718000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:15.727000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:15.742000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:15.752000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:15.764000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:15.776000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:15.789000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:15.935000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:15.978000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:15.987000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:15.998000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:16.011000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:16.022000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:16.034000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:16.047000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:16.203000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:16.238000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:16.247000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:16.256000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:16.266000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:16.280000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:16.293000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:16.305000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:16.459000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:16.498000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:16.507000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:16.516000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:16.526000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:16.537000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:16.550000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:16.563000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:16.715000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:16.759000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:16.767000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:16.777000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:16.787000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:16.798000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:16.811000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:16.824000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:16.984000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:17.018000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:17.027000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:17.036000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:17.046000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:17.058000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:17.071000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:17.084000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:17.239000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:17.278000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:17.287000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:17.296000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:17.307000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:17.319000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:17.332000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:17.344000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:17.738000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:17.749000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:17.760000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:17.771000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:17.782000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:17.791000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:17.846000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:17.856000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:18.184000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:18.193000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:18.201000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:18.209000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:18.219000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:18.231000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:18.269000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:18.293000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:18.571000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:18.580000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:18.589000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:18.599000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:18.610000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:18.621000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:18.633000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:18.681000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:18.991000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:19.002000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:19.011000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:19.022000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:19.034000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:19.043000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:19.079000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:19.099000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:19.255000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:19.269000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:19.278000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:19.302000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:19.311000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:19.339000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:19.359000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:19.533000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:19.682000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:19.691000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:19.702000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:19.711000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:19.721000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:19.731000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:19.740000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:19.792000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:19.947000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:19.957000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:19.967000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:19.978000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:19.988000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:20.000000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:20.009000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:20.052000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:20.204000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:20.219000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:20.229000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:20.240000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:20.251000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:20.261000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:20.272000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:20.312000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:20.463000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:20.483000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:20.493000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:20.504000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:20.514000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:20.526000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:20.535000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:20.572000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:20.723000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:20.747000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:20.757000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:20.768000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:20.779000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:20.790000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:20.799000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:20.832000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:20.983000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:21.011000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:21.021000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:21.032000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:21.042000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:21.053000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:21.064000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:21.104000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:21.255000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:21.274000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:21.285000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:21.296000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:21.306000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:21.317000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:21.327000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:21.368000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:21.528000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:21.539000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:21.560000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:21.569000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:21.582000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:21.591000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:21.602000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:21.632000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:21.788000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:21.803000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:21.820000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:21.831000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:21.842000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:21.852000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:21.863000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:21.892000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:22.047000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:22.067000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:22.080000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:22.091000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:22.102000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:22.112000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:22.123000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:22.152000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:22.307000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:22.331000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:22.341000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:22.352000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:22.362000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:22.373000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:22.383000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:22.412000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:22.568000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:22.594000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:22.605000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:22.616000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:22.626000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:22.638000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:22.647000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:22.672000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:22.826000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:22.865000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:22.876000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:22.887000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:22.897000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:22.910000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:22.922000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:22.932000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:23.098000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:23.125000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:23.142000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:23.153000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:23.163000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:23.175000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:23.188000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:23.198000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:23.357000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:23.384000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:23.406000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:23.417000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:23.428000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:23.439000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:23.451000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:23.462000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:23.618000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:23.644000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:23.670000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:23.681000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:23.692000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:23.703000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:23.716000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:23.726000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:24.427000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:24.438000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:24.448000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:24.458000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:24.468000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:24.477000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:24.486000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:24.496000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1033 ms 100.0% 
  triton_mm_359 0.1437 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1466 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1623 ms 63.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1660 ms 62.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1758 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1758 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1794 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1834 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2020 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6460 seconds and 0.5374 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1063 ms 100.0% 
  triton_mm_359 0.1483 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1542 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1669 ms 63.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1687 ms 63.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1775 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1775 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1826 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1843 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2074 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6886 seconds and 0.7414 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1045 ms 100.0% 
  triton_mm_359 0.1454 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1491 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1640 ms 63.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1658 ms 63.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1767 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1767 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1803 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1840 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2024 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6535 seconds and 0.6093 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1076 ms 100.0% 
  triton_mm_359 0.1505 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1541 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1665 ms 64.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1696 ms 63.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1774 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1777 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1838 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1851 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2080 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6553 seconds and 0.4505 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1066 ms 100.0% 
  triton_mm_359 0.1506 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1552 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1681 ms 63.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1692 ms 63.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1785 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1787 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1840 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1862 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2076 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7463 seconds and 0.3943 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1063 ms 100.0% 
  triton_mm_359 0.1507 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1541 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1690 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1700 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1783 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1787 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1843 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1858 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2084 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6611 seconds and 0.7285 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1073 ms 100.0% 
  triton_mm_359 0.1487 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1530 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1681 ms 63.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1703 ms 63.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1781 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1781 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1858 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_351 0.1863 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_345 0.2091 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.5965 seconds and 0.3594 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1057 ms 100.0% 
  triton_mm_359 0.1491 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1518 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1650 ms 64.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1656 ms 63.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1770 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1775 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1817 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1853 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2080 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6536 seconds and 0.3774 seconds precompiling for 39 choices
Capturing batches (bs=16 avail_mem=41.15 GB):  90%| | 47/52 [04:23<04:14, 51.00s/it]Capturing batches (bs=12 avail_mem=40.57 GB):  90%| | 47/52 [04:23<04:14, 51.00s/it][rank3]:W1026 11:36:41.560000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:41.571000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:41.588000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:41.598000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:41.609000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:41.620000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:41.628000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:41.632000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:41.640000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:41.643000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:41.657000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:41.667000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:41.677000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:41.689000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:41.701000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:41.701000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:41.712000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:41.713000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:41.731000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:41.740000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:41.751000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:41.773000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:41.774000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:41.785000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:42.213000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:42.220000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:42.231000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:42.240000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:42.262000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:42.270000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:42.281000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:42.291000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:42.297000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:42.305000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:42.313000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:42.322000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:42.348000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:42.356000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:42.361000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:42.367000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:42.372000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:42.375000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:42.384000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:42.392000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:42.418000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:42.426000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:42.431000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:42.436000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:42.442000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:42.444000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:42.453000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:42.461000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:42.498000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:42.500000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:42.506000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:42.510000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:43.112000 818 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank6]:W1026 11:36:43.118000 821 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank0]:W1026 11:36:43.118000 815 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank4]:W1026 11:36:43.125000 819 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank2]:W1026 11:36:43.158000 817 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank7]:W1026 11:36:43.161000 822 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank1]:W1026 11:36:43.167000 816 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank5]:W1026 11:36:43.168000 820 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_401 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_398 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_399 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8314 seconds and 0.2285 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_401 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_392 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_393 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_400 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0092 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_398 0.0092 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8098 seconds and 0.0709 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_396 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_402 0.0092 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_398 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_403 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8620 seconds and 0.1752 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_396 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_400 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_403 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_392 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_393 0.0092 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8538 seconds and 0.3009 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_401 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_393 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_392 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8855 seconds and 0.1343 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0090 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_396 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_399 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_403 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8838 seconds and 0.1003 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_396 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_402 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_403 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9248 seconds and 0.0990 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0088 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_400 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_392 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_393 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9264 seconds and 0.2734 seconds precompiling for 25 choices
[rank5]:W1026 11:36:53.444000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:53.563000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:53.656000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:53.733000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:36:53.943000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:53.973000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:54.071000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:54.109000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:36:54.161000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:54.175000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:36:54.237000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:54.387000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:36:54.472000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:36:54.613000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:36:54.669000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:36:54.882000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:36:55.600000 819 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank5]:W1026 11:36:55.626000 820 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank3]:W1026 11:36:55.759000 818 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank2]:W1026 11:36:55.941000 817 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank6]:W1026 11:36:56.102000 821 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank1]:W1026 11:36:56.141000 816 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank7]:W1026 11:36:56.162000 822 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank0]:W1026 11:36:56.198000 815 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0089 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0090 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0093 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0094 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0095 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0095 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_408 0.0096 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0099 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8971 seconds and 0.4122 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0094 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0094 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0095 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0095 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0099 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8814 seconds and 0.4223 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_408 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0099 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0101 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0101 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8959 seconds and 0.4318 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0090 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0090 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0093 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0094 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_408 0.0096 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9340 seconds and 0.4082 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0093 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0095 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0098 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0099 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0099 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9195 seconds and 0.4246 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0095 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0099 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0099 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0100 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9082 seconds and 0.4135 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0088 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0088 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0093 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0093 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0095 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8761 seconds and 0.4215 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_423 0.0096 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0097 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0097 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0098 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_408 0.0100 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0101 ms 84.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9128 seconds and 0.4247 seconds precompiling for 25 choices
[rank4]:W1026 11:37:02.097000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:02.174000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:02.274000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:02.321000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:02.408000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:02.455000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:02.507000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:02.531000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:02.631000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:02.679000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:02.688000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:02.746000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:02.756000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:02.764000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:02.814000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:02.823000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:02.858000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:02.864000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:02.891000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:02.923000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:02.991000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:03.021000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:03.098000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:03.199000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:06.681000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:06.761000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:06.864000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:06.899000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:06.975000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:07.076000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:07.136000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:07.195000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:07.213000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:07.272000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:07.314000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:07.374000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:07.427000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:07.485000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:07.504000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:07.561000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:07.570000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:07.604000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:07.647000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:07.662000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:07.717000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:07.748000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:07.793000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:07.904000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:08.477000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:08.506000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:08.554000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:08.561000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:08.581000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:08.639000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:08.657000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:08.682000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:08.743000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:08.795000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:08.850000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:08.872000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:08.926000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:08.959000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:08.970000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:08.974000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:09.027000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:09.036000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:09.047000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:09.084000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:09.136000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:09.147000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:09.507000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:09.609000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:10.192000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:10.205000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:10.216000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:10.232000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:10.243000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:10.257000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:10.280000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:10.542000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:10.694000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:10.706000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:10.715000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:10.724000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:10.736000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:10.759000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:10.785000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:10.809000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:10.960000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:10.971000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:10.985000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:10.997000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:11.007000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:11.021000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:11.043000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:11.067000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:12.611000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:12.621000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:12.654000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:12.672000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:12.684000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:12.687000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:12.693000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:12.699000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:12.702000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:12.712000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:12.731000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:12.736000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:12.749000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:12.749000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:12.760000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:12.770000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:12.780000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:12.780000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:12.789000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:12.798000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:12.809000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:12.819000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:12.829000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:12.839000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0279 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0454 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0494 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0583 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0597 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1874 seconds and 0.7990 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0112 ms 100.0% 
  triton_mm_431 0.0277 ms 40.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0417 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0452 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0580 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0595 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0609 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2672 seconds and 0.7538 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0279 ms 40.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0452 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0598 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2314 seconds and 0.7812 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_431 0.0280 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0455 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0533 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0755 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1909 seconds and 0.4931 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0277 ms 40.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2402 seconds and 0.7279 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0110 ms 100.0% 
  triton_mm_431 0.0277 ms 39.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0755 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2776 seconds and 0.4229 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_431 0.0280 ms 39.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0419 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0597 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0755 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.3192 seconds and 0.7834 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_431 0.0278 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0452 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0598 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2912 seconds and 0.4807 seconds precompiling for 39 choices
[rank0]:W1026 11:37:23.556000 815 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e670>
[rank0]:W1026 11:37:23.579000 815 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e820>
[rank7]:W1026 11:37:23.603000 822 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6880>
[rank2]:W1026 11:37:23.621000 817 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f62539b6430>
[rank7]:W1026 11:37:23.626000 822 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6790>
[rank2]:W1026 11:37:23.644000 817 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6252c5abe0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:37:23 DP0 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1026 11:37:23.655000 819 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446df0>
[rank4]:W1026 11:37:23.679000 819 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446f40>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:37:23 DP7 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:37:23 DP2 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1026 11:37:23.731000 820 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81a730>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:37:23 DP4 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1026 11:37:23.754000 820 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81ae50>
[rank1]:W1026 11:37:23.780000 816 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba310>
[rank1]:W1026 11:37:23.803000 816 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba8e0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:37:23 DP5 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:37:23 DP1 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1026 11:37:23.894000 821 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b2fd0>
[rank6]:W1026 11:37:23.917000 821 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b3180>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:37:23 DP6 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1026 11:37:24.259000 818 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e80>
[rank3]:W1026 11:37:24.282000 818 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e20>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:37:24 DP3 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1026 11:37:25.517000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:25.527000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:25.536000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:25.545000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:25.553000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:25.565000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:25.575000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:25.629000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:25.785000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:25.796000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:25.805000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:25.816000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:25.825000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:25.838000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:25.849000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:25.889000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:26.053000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:26.063000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:26.073000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:26.083000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:26.093000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:26.106000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:26.117000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:26.149000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:26.315000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:26.324000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:26.337000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:26.349000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:26.361000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:26.372000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:26.385000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:26.403000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:26.583000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:26.591000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:26.603000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:26.614000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:26.626000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:26.639000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:26.650000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:26.663000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:26.864000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:26.874000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:26.882000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:26.894000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:26.906000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:26.919000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:26.930000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:26.941000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:27.129000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:27.143000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:27.151000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:27.163000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:27.175000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:27.188000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:27.199000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:27.210000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:27.391000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:27.413000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:27.424000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:27.433000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:27.443000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:27.454000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:27.466000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:27.479000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:27.654000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:27.681000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:27.691000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:27.702000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:27.711000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:27.722000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:27.734000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:27.747000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:27.918000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:27.947000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:27.956000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:27.969000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:27.981000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:27.994000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:28.006000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:28.016000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:28.184000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:28.215000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:28.223000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:28.235000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:28.247000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:28.260000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:28.271000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:28.282000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:28.446000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:28.489000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:28.499000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:28.509000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:28.519000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:28.530000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:28.542000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:28.556000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:28.714000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:28.757000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:28.767000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:28.778000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:28.787000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:28.798000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:28.810000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:28.823000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:28.981000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:29.023000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:29.031000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:29.045000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:29.056000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:29.069000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:29.086000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:29.098000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:29.253000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:29.306000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:29.315000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:29.326000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:29.338000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:29.351000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:29.363000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:29.376000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:29.537000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:29.582000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:29.593000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:29.604000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:29.615000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:29.628000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:29.639000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:29.650000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:29.808000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:29.846000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:29.857000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:29.871000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:29.882000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:29.895000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:29.907000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:29.916000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:30.072000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:30.110000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:30.121000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:30.139000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:30.150000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:30.163000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:30.175000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:30.184000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:30.341000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:30.374000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:30.388000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:30.407000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:30.418000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:30.431000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:30.443000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:30.453000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:30.613000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:30.638000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:30.652000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:30.675000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:30.686000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:30.698000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:30.711000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:30.720000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:30.884000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:30.902000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:30.916000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:30.943000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:30.954000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:30.967000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:30.977000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:30.988000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:31.149000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:31.166000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:31.180000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:31.216000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:31.228000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:31.239000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:31.256000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:31.265000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:31.421000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:31.430000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:31.444000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:31.476000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:31.495000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:31.506000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:31.523000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:31.533000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:31.693000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:31.701000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:31.712000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:31.740000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:31.763000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:31.775000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:31.787000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:31.798000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:32.183000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:32.192000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:32.202000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:32.213000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:32.224000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:32.235000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:32.247000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:32.293000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:32.454000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:32.465000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:32.477000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:32.487000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:32.497000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:32.508000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:32.519000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:32.556000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:32.968000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:32.978000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:32.987000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:32.999000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:33.009000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:33.020000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:33.032000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:33.082000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:33.530000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:33.541000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:33.552000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:33.561000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:33.570000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:33.583000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:33.643000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:33.653000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:34.025000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:34.034000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:34.044000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:34.055000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:34.066000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:34.076000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:34.133000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:34.147000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:34.535000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:34.545000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:34.557000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:34.568000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:34.578000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:34.588000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:34.599000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:34.647000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:34.811000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:34.821000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:34.834000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:34.845000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:34.857000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:34.866000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:34.878000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:34.911000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:35.083000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:35.093000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:35.104000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:35.114000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:35.129000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:35.142000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:35.152000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:35.177000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:35.353000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:35.362000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:35.375000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:35.385000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:35.397000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:35.410000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:35.423000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:35.445000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:35.619000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:35.632000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:35.649000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:35.661000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:35.672000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:35.682000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:35.695000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:35.707000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:35.891000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:35.901000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:35.921000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:35.933000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:35.944000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:35.955000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:35.967000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:35.979000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:36.165000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:36.173000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:36.191000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:36.201000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:36.217000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:36.229000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:36.239000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:36.253000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:36.437000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:36.445000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:36.463000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:36.472000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:36.489000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:36.502000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:36.514000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:36.526000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:36.703000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:36.724000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:36.745000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:36.758000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:36.767000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:36.778000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:36.791000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:36.802000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:36.975000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:36.992000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:37.013000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:37.025000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:37.036000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:37.045000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:37.061000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:37.072000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:37.249000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:37.258000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:37.282000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:37.294000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:37.307000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:37.319000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:37.329000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:37.343000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:37.517000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:37.534000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:37.559000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:37.569000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:37.580000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:37.593000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:37.604000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:37.616000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:37.784000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:37.802000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:37.831000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:37.841000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:37.852000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:37.865000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:37.876000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:37.888000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:38.057000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:38.070000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:38.103000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:38.113000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:38.124000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:38.137000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:38.148000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:38.160000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:38.327000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:38.340000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:38.377000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:38.389000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:38.400000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:38.409000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:38.423000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:38.433000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:38.597000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:38.606000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:38.643000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:38.653000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:38.673000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:38.684000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:38.696000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:38.707000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:38.869000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:38.877000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:38.915000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:38.924000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:38.941000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:38.953000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:38.966000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:38.978000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:39.137000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:39.146000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:39.192000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:39.202000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:39.214000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:39.227000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:39.240000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:39.250000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:39.409000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:39.422000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:39.467000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:39.477000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:39.488000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:39.501000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:39.513000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:39.524000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:39.683000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:39.694000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:39.741000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:39.752000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:39.762000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:39.773000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:39.783000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:39.796000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:39.955000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:39.965000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:40.021000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:40.033000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:40.044000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:40.053000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:40.063000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:40.076000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:40.234000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:40.245000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:40.289000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:40.301000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:40.314000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:40.324000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:40.334000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:40.347000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:40.506000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:40.517000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:40.557000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:40.569000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:40.583000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:40.593000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:40.603000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:40.616000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:40.779000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:40.789000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:40.829000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:40.841000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:40.852000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:40.861000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:40.872000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:40.884000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:41.051000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:41.061000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:41.097000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:41.109000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:41.123000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:41.132000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:41.143000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:41.155000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:41.323000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:41.333000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:41.365000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:41.377000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:41.392000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:41.401000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:41.411000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:41.424000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:41.595000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:41.612000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:41.645000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:41.657000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:41.668000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:41.677000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:41.687000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:41.700000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:41.866000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:41.892000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:41.925000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:41.938000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:41.948000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:41.958000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:41.969000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:41.982000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:42.684000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:42.693000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:42.701000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:42.713000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:42.723000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:42.734000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:42.745000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:42.755000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0892 ms 100.0% 
  triton_mm_483 0.1430 ms 62.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1475 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1623 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1634 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1674 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1751 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1752 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1787 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1824 ms 48.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.5969 seconds and 0.5540 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0924 ms 100.0% 
  triton_mm_483 0.1495 ms 61.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1543 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1671 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1692 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1734 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1777 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1779 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1834 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1845 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7136 seconds and 0.6896 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0912 ms 100.0% 
  triton_mm_483 0.1483 ms 61.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1524 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1644 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1655 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1714 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1767 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1769 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1832 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1840 ms 49.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7157 seconds and 0.6431 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0907 ms 100.0% 
  triton_mm_483 0.1478 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1530 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1659 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1676 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1717 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1768 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1769 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1831 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1843 ms 49.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7836 seconds and 0.6092 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0919 ms 100.0% 
  triton_mm_483 0.1501 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1541 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1680 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1686 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1738 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1777 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1778 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_481 0.1855 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_475 0.1858 ms 49.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7295 seconds and 0.4297 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0902 ms 100.0% 
  triton_mm_483 0.1437 ms 62.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1485 ms 60.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1634 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1653 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1687 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1759 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1763 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1808 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1834 ms 49.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6458 seconds and 0.4398 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0930 ms 100.0% 
  triton_mm_483 0.1495 ms 62.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1542 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1672 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1702 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1734 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1778 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1783 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1820 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1848 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7364 seconds and 0.6767 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0930 ms 100.0% 
  triton_mm_483 0.1499 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1557 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1654 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1678 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1720 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1773 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1775 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1839 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1844 ms 50.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7630 seconds and 0.3779 seconds precompiling for 39 choices
Capturing batches (bs=12 avail_mem=40.57 GB):  92%|| 48/52 [05:40<03:55, 58.87s/it]Capturing batches (bs=8 avail_mem=39.98 GB):  92%|| 48/52 [05:40<03:55, 58.87s/it] [rank3]:W1026 11:37:58.950000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:58.962000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:58.982000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:58.995000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:59.005000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:59.017000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:59.020000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:59.031000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:59.035000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:59.046000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:59.052000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:59.065000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:59.074000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:59.087000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:59.093000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:59.105000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:59.105000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:59.115000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:59.126000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:59.139000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:59.148000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:59.161000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:59.179000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:59.189000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:59.605000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:59.612000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:59.638000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:59.648000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:59.658000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:59.669000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:59.680000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:59.691000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:59.692000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:59.697000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:59.726000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:59.731000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:59.747000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:59.752000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:59.763000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:59.764000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:59.769000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:59.774000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:59.798000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:59.802000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:59.819000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:59.823000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:37:59.834000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:59.835000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:37:59.838000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:59.845000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:37:59.868000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:37:59.871000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:37:59.889000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:37:59.894000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:37:59.906000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:37:59.915000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:00.296000 818 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank0]:W1026 11:38:00.300000 815 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank4]:W1026 11:38:00.331000 819 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank2]:W1026 11:38:00.332000 817 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank1]:W1026 11:38:00.356000 816 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank7]:W1026 11:38:00.358000 822 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank5]:W1026 11:38:00.373000 820 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank6]:W1026 11:38:00.378000 821 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_517 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_521 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0090 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_523 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_513 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_512 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9398 seconds and 0.1868 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_521 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_517 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_525 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_524 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_522 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9348 seconds and 0.2010 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_517 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_525 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_522 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_524 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0090 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8911 seconds and 0.2184 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_526 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_522 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_523 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8502 seconds and 0.2673 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_527 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_522 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8779 seconds and 0.2057 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_520 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_517 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_524 0.0089 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_525 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9076 seconds and 0.1237 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_525 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0089 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_522 0.0089 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_524 0.0089 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_520 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9092 seconds and 0.1213 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_521 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_516 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_520 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_526 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_517 0.0090 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_523 0.0090 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_524 0.0090 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_522 0.0090 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8496 seconds and 0.1300 seconds precompiling for 25 choices
[rank6]:W1026 11:38:10.782000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:10.918000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:11.083000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:11.288000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:11.377000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:11.384000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:11.419000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:11.553000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:11.577000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:11.584000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:11.881000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:11.900000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:12.039000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:12.052000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:12.081000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:12.552000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:12.682000 821 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank5]:W1026 11:38:12.794000 820 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank0]:W1026 11:38:12.824000 815 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank3]:W1026 11:38:13.111000 818 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank4]:W1026 11:38:13.135000 819 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank7]:W1026 11:38:13.489000 822 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank1]:W1026 11:38:13.510000 816 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank2]:W1026 11:38:14.126000 817 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0096 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0097 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_533 0.0097 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0097 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0098 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0099 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9109 seconds and 0.4139 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_546 0.0093 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0093 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0093 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0094 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_532 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_548 0.0098 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8555 seconds and 0.4255 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0088 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0093 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0093 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_547 0.0094 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0098 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0098 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8519 seconds and 0.4309 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0093 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0093 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0096 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_546 0.0096 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0097 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8786 seconds and 0.4110 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_539 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_538 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0091 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0091 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_546 0.0093 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0093 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0097 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9271 seconds and 0.4159 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0090 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0097 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_533 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0097 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_547 0.0099 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0099 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9274 seconds and 0.3973 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0088 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0092 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0092 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_546 0.0093 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0093 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0098 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_548 0.0098 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9980 seconds and 0.3972 seconds precompiling for 25 choices
[rank6]:W1026 11:38:19.258000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:19.335000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:19.437000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0082 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0085 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0086 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_551 0.0091 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_550 0.0091 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_546 0.0092 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0092 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0092 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0095 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9105 seconds and 0.4122 seconds precompiling for 25 choices
[rank5]:W1026 11:38:19.594000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:19.636000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:19.646000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:19.671000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:19.709000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:19.713000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:19.723000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:19.771000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:19.787000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:19.813000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:19.825000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:19.888000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:20.058000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:20.102000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:20.137000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:20.180000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:20.239000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:20.281000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:21.159000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:21.236000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:21.336000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:23.665000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:23.751000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:23.853000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:23 DP3 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1026 11:38:24.011000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:24.090000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:24.116000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:24.192000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:24.193000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:24 DP6 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1026 11:38:24.294000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:24.336000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:24 DP7 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1026 11:38:24.414000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:24.440000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:24.458000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:24.517000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:24.519000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:24.535000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:24 DP5 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1026 11:38:24.621000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:24.637000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:24 DP4 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:24 DP0 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1026 11:38:24.777000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:24.855000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:24.959000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:25 DP1 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1026 11:38:25.208000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:25.285000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:25.387000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:25 DP2 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1026 11:38:25.965000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:25.975000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:25.984000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:26.016000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:26.026000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:26.042000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:26.052000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:26.061000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:26.093000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:26.104000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:26.143000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:26.154000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:26.164000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:26.165000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:26.197000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:38:26 DP3 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1026 11:38:26.206000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:38:26 DP7 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:38:26 DP4 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1026 11:38:26.243000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:38:26 DP5 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:38:26 DP0 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1026 11:38:26.346000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:38:26 DP1 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1026 11:38:26.453000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:26.534000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:26.643000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:38:26 DP6 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1026 11:38:27.051000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:27.140000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:27.240000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-26 11:38:27 DP2 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1026 11:38:27.824000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:27.838000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:27.847000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:27.859000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:27.870000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:27.881000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:27.892000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:27.909000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:28.097000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:28.110000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:28.119000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:28.132000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:28.142000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:28.153000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:28.164000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:28.177000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:28.364000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:28.382000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:28.391000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:28.403000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:28.414000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:28.424000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:28.435000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:28.448000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:28 DP0 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:28 DP1 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:28 DP7 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:28 DP3 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:28 DP6 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:28 DP4 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:28 DP5 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:38:28 DP2 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1026 11:38:30.037000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:30.045000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:30.070000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:30.083000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:30.092000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:30.101000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:30.111000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:30.116000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:30.122000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:30.123000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:30.149000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:30.159000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:30.170000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:30.178000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:30.188000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:30.194000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:30.200000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:30.201000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:30.228000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:30.237000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:30.248000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:38:30 DP0 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank4]:W1026 11:38:30.256000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:38:30 DP7 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank5]:W1026 11:38:30.266000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:30.279000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:38:30 DP1 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:38:30 DP3 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:38:30 DP6 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:38:30 DP5 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:38:30 DP2 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-26 11:38:30 DP4 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0112 ms 100.0% 
  triton_mm_555 0.0273 ms 40.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0405 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0454 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0497 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0534 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0582 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0594 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0611 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0742 ms 15.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.8447 seconds and 0.4936 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0107 ms 100.0% 
  triton_mm_555 0.0274 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0406 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0451 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0494 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0532 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0581 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0600 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0621 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0745 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.4801 seconds and 0.4145 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0107 ms 100.0% 
  triton_mm_555 0.0277 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0404 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0451 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0495 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0531 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0581 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0602 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0628 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0760 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.9072 seconds and 0.5013 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0109 ms 100.0% 
  triton_mm_555 0.0273 ms 39.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0407 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0495 ms 22.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0534 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0581 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0598 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0621 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0743 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.9788 seconds and 0.5455 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0109 ms 100.0% 
  triton_mm_555 0.0276 ms 39.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0426 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0505 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0535 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0582 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0616 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0637 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0750 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.9851 seconds and 0.5094 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0108 ms 100.0% 
  triton_mm_555 0.0272 ms 39.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0405 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0452 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0494 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0532 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0581 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0595 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0617 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0740 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5733 seconds and 0.4347 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0108 ms 100.0% 
  triton_mm_555 0.0272 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0404 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0452 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0494 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0532 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0580 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0597 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0613 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0742 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5614 seconds and 0.4301 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0107 ms 100.0% 
  triton_mm_555 0.0271 ms 39.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0406 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0451 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0493 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0532 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0581 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0593 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0610 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0740 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 7.0640 seconds and 0.5319 seconds precompiling for 31 choices
[rank0]:W1026 11:38:39.279000 815 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e670>
[rank0]:W1026 11:38:39.303000 815 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e820>
[rank1]:W1026 11:38:39.368000 816 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba310>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:38:39 DP0 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1026 11:38:39.375000 822 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6880>
[rank1]:W1026 11:38:39.392000 816 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba8e0>
[rank7]:W1026 11:38:39.398000 822 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6790>
[rank2]:W1026 11:38:39.403000 817 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f62539b6430>
[rank2]:W1026 11:38:39.431000 817 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6252c5abe0>
[rank6]:W1026 11:38:39.439000 821 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b2fd0>
[rank6]:W1026 11:38:39.463000 821 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b3180>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:38:39 DP7 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:38:39 DP1 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:38:39 DP2 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:38:39 DP6 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1026 11:38:39.534000 819 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446df0>
[rank5]:W1026 11:38:39.553000 820 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81a730>
[rank4]:W1026 11:38:39.558000 819 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446f40>
[rank3]:W1026 11:38:39.576000 818 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e80>
[rank5]:W1026 11:38:39.577000 820 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81ae50>
[rank3]:W1026 11:38:39.600000 818 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e20>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:38:39 DP4 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:38:39 DP5 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:38:39 DP3 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1026 11:38:41.247000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:41.256000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:41.267000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:41.278000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:41.291000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:41.302000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:41.315000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:41.359000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:41.519000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:41.531000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:41.542000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:41.555000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:41.568000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:41.581000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:41.593000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:41.627000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:41.793000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:41.805000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:41.818000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:41.840000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:41.849000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:41.862000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:41.874000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:41.897000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:42.069000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:42.080000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:42.090000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:42.115000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:42.125000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:42.137000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:42.149000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:42.162000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:42.337000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:42.349000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:42.362000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:42.391000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:42.401000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:42.413000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:42.425000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:42.439000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:42.609000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:42.620000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:42.634000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:42.667000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:42.677000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:42.689000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:42.701000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:42.714000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:42.885000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:42.896000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:42.906000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:42.943000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:42.953000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:42.965000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:42.977000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:42.991000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:43.155000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:43.164000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:43.180000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:43.221000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:43.233000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:43.247000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:43.258000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:43.269000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:43.427000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:43.436000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:43.452000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:43.493000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:43.506000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:43.519000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:43.531000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:43.542000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:43.707000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:43.716000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:43.728000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:43.778000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:43.790000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:43.804000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:43.816000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:43.825000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:43.987000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:43.996000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:44.007000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:44.050000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:44.062000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:44.075000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:44.087000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:44.098000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:44.259000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:44.268000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:44.279000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:44.322000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:44.334000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:44.347000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:44.360000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:44.370000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:44.531000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:44.540000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:44.560000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:44.594000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:44.606000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:44.619000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:44.631000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:44.642000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:44.803000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:44.812000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:44.828000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:44.865000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:44.878000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:44.891000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:44.903000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:44.914000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:45.075000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:45.087000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:45.100000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:45.137000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:45.150000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:45.163000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:45.175000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:45.186000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:45.351000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:45.360000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:45.371000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:45.409000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:45.422000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:45.435000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:45.449000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:45.458000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:45.623000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:45.632000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:45.644000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:45.681000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:45.694000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:45.707000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:45.719000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:45.730000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:45.897000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:45.909000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:45.918000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:45.963000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:45.975000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:45.987000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:46.000000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:46.013000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:46.177000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:46.188000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:46.197000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:46.240000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:46.249000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:46.262000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:46.274000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:46.287000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:46.453000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:46.465000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:46.478000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:46.515000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:46.525000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:46.536000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:46.549000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:46.563000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:46.725000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:46.737000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:46.750000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:46.791000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:46.801000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:46.813000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:46.825000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:46.838000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:46.997000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:47.009000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:47.022000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:47.067000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:47.077000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:47.089000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:47.101000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:47.114000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:47.271000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:47.281000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:47.296000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:47.345000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:47.358000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:47.372000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:47.382000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:47.393000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:47.555000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:47.564000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:47.576000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:47.617000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:47.630000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:47.645000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:47.658000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:47.667000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:47.831000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:47.843000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:47.854000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:47.889000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:47.902000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:47.921000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:47.933000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:47.944000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:48.111000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:48.120000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:48.131000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:48.161000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:48.174000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:48.193000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:48.205000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:48.216000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:48.391000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:48.400000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:48.412000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:48.445000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:48.458000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:48.474000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:48.485000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:48.496000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:48.667000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:48.676000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:48.687000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:48.722000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:48.734000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:48.748000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:48.759000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:48.770000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:48.945000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:48.957000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:48.966000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:48.995000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:49.006000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:49.019000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:49.033000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:49.047000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:49.221000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:49.232000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:49.241000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:49.272000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:49.281000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:49.296000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:49.309000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:49.322000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:49.493000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:49.505000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:49.514000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:49.548000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:49.558000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:49.571000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:49.586000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:49.598000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:49.769000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:49.780000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:49.790000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:49.823000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:49.834000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:49.848000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:49.861000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:49.874000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:50.045000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:50.056000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:50.074000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:50.111000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:50.122000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:50.136000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:50.147000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:50.159000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:50.320000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:50.331000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:50.348000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:50.389000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:50.402000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:50.414000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:50.428000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:50.438000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:50.599000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:50.608000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:50.620000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:50.661000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:50.674000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:50.687000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:50.701000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:50.713000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:50.875000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:50.884000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:50.895000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:50.945000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:50.958000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:50.970000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:50.983000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:50.995000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:51.159000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:51.168000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:51.179000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:51.217000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:51.230000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:51.243000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:51.255000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:51.269000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:51.437000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:51.449000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:51.458000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:51.487000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:51.499000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:51.521000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:51.534000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:51.546000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:51.713000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:51.725000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:51.734000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:51.763000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:51.774000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:51.793000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:51.806000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:51.823000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:51.989000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:52.000000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:52.010000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:52.039000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:52.050000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:52.065000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:52.078000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:52.099000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:52.265000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:52.276000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:52.286000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:52.315000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:52.326000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:52.339000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:52.352000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:52.375000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:52.541000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:52.552000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:52.562000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:52.591000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:52.602000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:52.615000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:52.628000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:52.651000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:52.817000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:52.829000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:52.846000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:52.879000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:52.891000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:52.903000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:52.916000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:52.939000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:53.105000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:53.116000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:53.126000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:53.160000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:53.170000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:53.183000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:53.196000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:53.215000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:53.381000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:53.393000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:53.402000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:53.448000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:53.460000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:53.470000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:53.483000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:53.503000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:53.669000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:53.681000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:53.690000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:53.723000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:53.736000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:53.747000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:53.760000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:53.779000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:53.945000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:53.957000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:53.966000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:54.000000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:54.012000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:54.024000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:54.036000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:54.055000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:54.221000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:54.232000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:54.242000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:54.279000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:54.291000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:54.301000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:54.315000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:54.331000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:54.890000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:54.901000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:54.912000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:54.921000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:54.932000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:54.944000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:54.955000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:55.002000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:55.515000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:55.527000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:55.539000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:55.550000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:55.560000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:55.573000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:55.583000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:55.628000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:56.160000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:56.168000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:56.181000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:56.190000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:56.200000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:56.213000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:56.225000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:56.273000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:56.446000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:56.457000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:56.468000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:56.480000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:56.493000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:56.504000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:56.516000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:56.544000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:57.134000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:57.145000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:57.157000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:57.167000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:57.176000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:57.188000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:57.243000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:57.253000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:57.723000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:57.733000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:57.743000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:57.754000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:57.766000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:57.777000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:57.790000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:57.836000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:58.432000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:58.442000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:58.453000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:58.465000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:58.477000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:58.489000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:58.542000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:58.553000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:58.719000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:58.730000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:58.741000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:58.754000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:58.767000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:58.780000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:58.818000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:58.831000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:59.008000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:59.017000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:59.029000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:59.042000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:59.055000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:59.068000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:59.094000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:59.111000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:38:59.828000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:38:59.838000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:38:59.850000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:38:59.862000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:38:59.872000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:38:59.884000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:38:59.893000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:38:59.902000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0760 ms 100.0% 
  triton_mm_598 0.1105 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1110 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1130 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1267 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_597 0.1297 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_585 0.1312 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1378 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1406 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1438 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7984 seconds and 0.6539 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0764 ms 100.0% 
  triton_mm_598 0.1106 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1109 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1131 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1261 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_597 0.1299 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_585 0.1318 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1377 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1404 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1428 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7882 seconds and 0.6369 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0756 ms 100.0% 
  triton_mm_598 0.1103 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1106 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1128 ms 67.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1274 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_597 0.1300 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_585 0.1314 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1378 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1416 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1439 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7774 seconds and 0.6830 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0749 ms 100.0% 
  triton_mm_598 0.1100 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1101 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1124 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1230 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1304 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1342 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1379 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1413 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1428 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 6.7876 seconds and 0.5835 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0754 ms 100.0% 
  triton_mm_598 0.1104 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1109 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1127 ms 66.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1239 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1310 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1339 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1389 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1415 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1417 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7826 seconds and 0.6025 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0769 ms 100.0% 
  triton_mm_598 0.1105 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1110 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1126 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1269 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1314 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1322 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1396 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1401 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1427 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7956 seconds and 0.6277 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0728 ms 100.0% 
  triton_mm_598 0.1096 ms 66.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1099 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1121 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1220 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_597 0.1287 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_585 0.1291 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1348 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1362 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1396 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.6736 seconds and 0.4490 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0751 ms 100.0% 
  triton_mm_598 0.1101 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1105 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1127 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1250 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1306 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1327 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1355 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1393 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1393 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 6.7877 seconds and 0.4185 seconds precompiling for 31 choices
Capturing batches (bs=8 avail_mem=39.98 GB):  94%|| 49/52 [06:56<03:11, 63.84s/it]Capturing batches (bs=4 avail_mem=39.38 GB):  94%|| 49/52 [06:56<03:11, 63.84s/it][rank4]:W1026 11:39:14.387000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:14.414000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:14.423000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:14.439000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:14.454000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:14.458000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:14.468000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:14.479000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:14.485000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:14.489000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:14.493000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:14.509000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:14.524000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:14.532000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:14.538000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:14.549000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:14.559000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:14.560000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:14.567000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:14.583000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:14.598000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:14.613000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:14.624000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:14.634000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:15.034000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:15.066000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:15.076000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:15.091000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:15.104000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:15.113000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:15.119000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:15.127000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:15.140000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:15.152000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:15.162000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:15.174000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:15.187000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:15.191000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:15.199000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:15.212000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:15.223000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:15.223000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:15.234000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:15.246000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:15.257000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:15.261000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:15.270000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:15.283000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:15.293000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:15.294000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:15.304000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:15.316000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:15.327000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:15.340000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:15.353000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:15.364000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:15.922000 822 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank4]:W1026 11:39:15.926000 819 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank1]:W1026 11:39:15.939000 816 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank6]:W1026 11:39:15.962000 821 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank2]:W1026 11:39:15.976000 817 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank0]:W1026 11:39:15.984000 815 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank3]:W1026 11:39:15.998000 818 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank5]:W1026 11:39:16.010000 820 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_627 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_624 0.0084 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0084 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_629 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_634 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_635 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_630 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_621 0.0087 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8854 seconds and 0.2192 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_624 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_629 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_635 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_620 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_631 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_630 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8390 seconds and 0.1452 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_629 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_634 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_632 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_620 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_621 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9223 seconds and 0.2634 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_627 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0086 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_631 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_634 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_635 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_630 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9297 seconds and 0.2313 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0086 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0087 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_634 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_630 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_635 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_631 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9306 seconds and 0.2035 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0082 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0083 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0083 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_629 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_634 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_635 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_620 0.0086 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_621 0.0086 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9140 seconds and 0.0977 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_625 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0086 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_628 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_629 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_630 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_631 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_634 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_621 0.0088 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9098 seconds and 0.1255 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_624 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_629 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_634 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_635 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_621 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_632 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9367 seconds and 0.1654 seconds precompiling for 25 choices
[rank0]:W1026 11:39:26.261000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:26.393000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:26.592000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:26.613000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:26.662000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:26.666000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:26.775000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:26.878000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:26.918000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:27.064000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:27.094000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:27.118000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:27.162000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:27.184000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:27.403000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:27.580000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:28.223000 815 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank4]:W1026 11:39:28.346000 819 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank5]:W1026 11:39:28.380000 820 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank7]:W1026 11:39:28.504000 822 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank6]:W1026 11:39:28.569000 821 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank1]:W1026 11:39:28.639000 816 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank3]:W1026 11:39:28.818000 818 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank2]:W1026 11:39:28.853000 817 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0084 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0090 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_658 0.0091 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_654 0.0091 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_656 0.0098 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_657 0.0098 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9009 seconds and 0.4273 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_647 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_646 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0093 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_658 0.0094 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_654 0.0096 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_641 0.0098 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_657 0.0099 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9450 seconds and 0.4236 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0086 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0097 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_641 0.0097 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0097 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_658 0.0098 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_654 0.0099 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0099 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9166 seconds and 0.4375 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0088 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_658 0.0093 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0093 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_654 0.0096 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0096 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0098 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0099 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8825 seconds and 0.4097 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_649 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_659 0.0093 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_658 0.0094 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_654 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0096 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_656 0.0100 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_657 0.0100 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9290 seconds and 0.4081 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_647 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_646 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0087 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_658 0.0093 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0094 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_654 0.0094 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0095 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0097 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0098 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9545 seconds and 0.4177 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_647 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_646 0.0084 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_658 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0094 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_654 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_657 0.0099 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_641 0.0099 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9188 seconds and 0.4044 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_658 0.0093 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0093 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_654 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0095 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_657 0.0101 ms 84.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_656 0.0101 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9578 seconds and 0.4221 seconds precompiling for 25 choices
[rank0]:W1026 11:39:34.815000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:34.893000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:34.945000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:35.165000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:35.229000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:35.242000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:35.257000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:35.292000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:35.299000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:35.307000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:35.335000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:35.357000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:35.378000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:35.386000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:35.430000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:35.442000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:35.484000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:35.519000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:35.562000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:35.569000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:35.613000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:35.721000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:35.799000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:35.849000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:39 DP6 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:39 DP5 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:39 DP1 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:39 DP0 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:39 DP4 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP3 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP7 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP2 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1026 11:39:40.499000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:40.505000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:40.521000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:40.528000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:40.538000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:40.551000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:40.560000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:40.570000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:40.575000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:40.592000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:40.599000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:40.609000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:40.621000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:40.631000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:40.641000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:40.646000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:40.663000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:40.669000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:40.680000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:40.692000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP5 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1026 11:39:40.702000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP7 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP1 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1026 11:39:40.721000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP3 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP0 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP6 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP4 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1026 11:39:40.791000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:40.872000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:40 DP2 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1026 11:39:41.427000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:41.436000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:41.453000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:41.466000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:41.478000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:41.490000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:41.500000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:41.514000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:41.711000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:41.720000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:41.733000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:41.757000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:41.770000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:41.781000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:41.792000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:41.805000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:41.991000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:42.000000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:42.013000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:42.033000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:42.045000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:42.057000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:42.068000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:42.082000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP6 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP5 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP3 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP1 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP0 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP7 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP4 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP2 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1026 11:39:42.677000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:42.689000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:42.700000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:42.713000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:42.721000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:42.733000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:42.745000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:42.748000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:42.760000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:42.763000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:42.772000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:42.785000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:42.792000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:42.804000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:42.816000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:42.819000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:42.830000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:42.833000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:42.842000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:42.856000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:42.863000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:42.875000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP6 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1026 11:39:42.886000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP5 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP3 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1026 11:39:42.904000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP1 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP0 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP7 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP4 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:39:42 DP2 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1026 11:39:43.362000 821 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b2fd0>
[rank5]:W1026 11:39:43.372000 820 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81a730>
[rank3]:W1026 11:39:43.384000 818 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e80>
[rank6]:W1026 11:39:43.386000 821 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b3180>
[rank5]:W1026 11:39:43.395000 820 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81ae50>
[rank1]:W1026 11:39:43.396000 816 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba310>
[rank3]:W1026 11:39:43.407000 818 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e20>
[rank0]:W1026 11:39:43.408000 815 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e670>
[rank1]:W1026 11:39:43.419000 816 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba8e0>
[rank7]:W1026 11:39:43.422000 822 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6880>
[rank0]:W1026 11:39:43.431000 815 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e820>
[rank4]:W1026 11:39:43.434000 819 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446df0>
[rank2]:W1026 11:39:43.445000 817 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f62539b6430>
[rank7]:W1026 11:39:43.445000 822 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6790>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:39:43 DP6 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1026 11:39:43.457000 819 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446f40>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:39:43 DP5 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1026 11:39:43.469000 817 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6252c5abe0>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:39:43 DP3 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:39:43 DP1 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:39:43 DP0 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:39:43 DP7 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:39:43 DP4 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:39:43 DP2 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1026 11:39:44.702000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:44.714000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:44.739000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:44.750000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:44.760000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:44.774000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:44.787000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:44.800000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:44.982000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:44.994000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:45.019000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:45.031000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:45.040000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:45.054000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:45.068000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:45.080000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:45.259000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:45.271000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:45.301000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:45.315000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:45.328000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:45.339000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:45.350000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:45.364000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:45.539000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:45.551000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:45.588000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:45.603000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:45.614000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:45.627000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:45.641000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:45.653000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:45.822000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:45.834000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:45.879000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:45.896000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:45.909000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:45.918000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:45.932000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:45.944000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:46.113000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:46.126000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:46.159000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:46.176000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:46.191000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:46.199000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:46.214000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:46.226000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:46.394000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:46.406000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:46.439000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:46.459000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:46.474000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:46.482000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:46.496000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:46.508000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:46.676000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:46.686000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:46.721000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:46.741000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:46.753000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:46.764000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:46.777000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:46.790000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:46.959000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:46.969000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:47.002000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:47.021000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:47.033000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:47.044000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:47.057000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:47.070000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:47.246000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:47.258000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:47.291000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:47.311000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:47.324000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:47.335000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:47.347000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:47.359000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:47.530000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:47.542000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:47.572000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:47.596000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:47.609000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:47.619000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:47.640000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:48.176000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:48.366000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:48.379000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:48.390000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:48.400000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:48.411000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:48.453000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:48.472000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:48.483000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:49.016000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:49.027000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:49.040000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:49.051000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:49.063000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:49.072000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:49.083000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:49.127000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:49.686000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:49.698000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:49.707000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:49.717000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:49.726000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:49.740000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:49.798000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:49.808000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:49.977000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:49.989000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:49.999000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:50.011000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:50.020000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:50.039000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:50.078000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:50.091000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:50.679000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:50.689000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:50.699000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:50.709000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:50.721000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:50.734000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:50.747000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:50.792000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:50.967000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:50.976000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:50.986000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:50.997000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:51.010000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:51.024000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:51.038000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:51.068000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:51.255000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:51.264000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:51.275000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:51.285000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:51.298000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:51.312000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:51.326000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:51.344000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:51.539000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:51.551000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:51.561000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:51.572000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:51.586000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:51.599000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:51.613000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:51.626000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:51.826000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:51.838000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:51.849000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:51.862000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:51.874000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:51.884000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:51.895000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:51.906000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:52.110000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:52.121000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:52.134000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:52.147000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:52.163000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:52.173000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:52.185000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:52.195000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:52.391000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:52.401000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:52.411000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:52.427000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:52.454000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:52.468000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:52.481000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:52.494000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:52.675000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:52.684000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:52.694000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:52.711000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:52.740000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:52.752000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:52.764000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:52.775000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:52.961000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:52.973000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:52.985000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:52.998000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:53.027000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:53.038000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:53.049000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:53.060000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:53.246000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:53.257000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:53.269000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:53.282000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:53.315000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:53.326000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:53.337000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:53.348000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:53.530000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:53.541000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:53.554000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:53.566000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:53.604000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:53.615000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:53.626000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:53.636000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:53.811000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:53.821000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:53.831000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:53.843000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:53.894000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:53.908000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:53.921000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:53.934000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:54.099000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:54.108000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:54.118000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:54.129000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:54.178000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:54.192000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:54.205000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:54.218000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:54.386000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:54.397000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:54.410000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:54.423000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:54.459000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:54.472000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:54.482000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:54.494000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:54.673000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:54.685000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:54.697000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:54.710000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:54.759000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:54.771000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:54.781000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:54.791000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:54.961000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:54.973000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:54.985000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:54.998000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:55.043000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:55.055000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:55.065000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:55.075000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:55.246000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:55.257000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:55.269000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:55.282000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:55.327000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:55.339000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:55.350000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:55.360000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:55.528000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:55.537000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:55.548000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:55.563000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:55.614000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:55.629000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:55.641000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:55.654000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:55.822000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:55.834000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:55.845000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:55.859000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:55.895000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:55.907000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:55.919000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:55.930000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:56.106000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:56.118000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:56.130000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:56.143000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:56.191000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:56.202000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:56.214000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:56.224000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:56.402000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:56.413000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:56.425000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:56.438000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:56.488000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:56.498000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:56.510000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:56.520000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:56.690000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:56.702000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:56.713000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:56.726000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:56.775000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:56.786000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:56.797000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:56.808000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:56.976000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:56.985000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:56.995000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:57.007000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:57.066000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:57.078000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:57.092000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:57.105000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:57.271000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:57.280000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:57.289000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:57.300000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:57.349000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:57.363000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:57.377000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:57.390000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:57.558000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:57.570000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:57.582000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:57.595000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:57.631000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:57.643000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:57.655000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:57.666000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:57.846000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:57.859000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:57.871000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:57.883000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:57.919000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:57.930000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:57.942000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:57.951000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:58.130000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:58.142000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:58.154000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:58.167000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:58.216000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:58.226000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:58.238000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:58.247000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:58.418000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:58.430000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:58.441000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:58.454000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:58.503000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:58.514000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:58.527000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:58.536000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:58.710000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:58.722000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:58.733000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:58.746000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:58.791000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:58.802000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:58.813000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:58.822000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:58.991000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:59.004000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:59.013000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:59.027000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:59.078000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:59.093000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:59.106000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:59.119000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:59.287000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:59.296000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:59.306000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:59.317000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:59.362000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:59.375000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:59.390000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:59.403000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:59.578000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:59.589000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:59.602000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:59.615000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:59.643000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:59.663000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:59.679000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:59.690000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:39:59.866000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:39:59.877000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:39:59.889000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:39:59.903000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:39:59.931000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:39:59.950000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:39:59.962000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:39:59.972000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:00.150000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:00.161000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:00.174000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:00.187000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:00.228000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:00.238000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:00.249000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:00.260000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:00.438000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:00.449000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:00.462000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:00.475000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:00.519000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:00.532000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:00.546000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:00.559000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:00.727000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:00.736000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:00.746000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:00.759000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:00.809000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:00.823000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:00.838000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:00.850000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:01.022000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:01.034000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:01.046000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:01.059000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:01.091000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:01.103000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:01.115000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:01.126000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:01.310000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:01.321000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:01.334000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:01.347000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:01.379000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:01.390000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:01.402000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:01.412000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:01.595000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:01.605000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:01.615000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:01.631000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:01.670000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:01.685000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:01.697000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:01.710000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:01.883000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:01.892000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:01.903000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:01.919000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:01.956000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:01.968000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:01.980000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:01.991000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:02.173000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:02.185000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:02.197000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:02.210000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:02.243000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:02.254000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:02.265000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:02.276000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:02.462000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:02.474000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:02.486000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:02.499000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:02.531000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:02.542000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:02.554000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:02.564000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:03.287000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:03.299000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:03.311000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:03.323000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:03.333000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:03.345000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:03.354000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:03.362000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0628 ms 100.0% 
  triton_mm_663 0.1049 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1056 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1144 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_683 0.1145 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_673 0.1162 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1175 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1199 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1565 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1566 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.7477 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0636 ms 100.0% 
  triton_mm_675 0.1053 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.1060 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1144 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1146 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1162 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1189 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1219 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1567 ms 40.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_681 0.1568 ms 40.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.7926 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0633 ms 100.0% 
  triton_mm_663 0.1041 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1057 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1146 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1147 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1164 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1176 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1207 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1565 ms 40.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1567 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.7990 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0622 ms 100.0% 
  triton_mm_663 0.1044 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1054 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1139 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1142 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1158 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1161 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1180 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1551 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_681 0.1565 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.7995 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0628 ms 100.0% 
  triton_mm_663 0.1050 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1058 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1148 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1149 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1164 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1188 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1207 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1569 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1571 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8050 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0621 ms 100.0% 
  triton_mm_663 0.1050 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1056 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1136 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1141 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1156 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1179 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1191 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1565 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_681 0.1578 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8050 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_663 0.1042 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1055 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1134 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1137 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_662 0.1155 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1156 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_674 0.1179 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1554 ms 39.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_681 0.1565 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8088 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0627 ms 100.0% 
  triton_mm_663 0.1053 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1055 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1148 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1152 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1164 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1177 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1208 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1567 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1569 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8251 seconds and 0.0001 seconds precompiling for 27 choices
Capturing batches (bs=4 avail_mem=39.38 GB):  96%|| 50/52 [07:57<02:06, 63.15s/it]Capturing batches (bs=2 avail_mem=38.91 GB):  96%|| 50/52 [07:57<02:06, 63.15s/it][rank4]:W1026 11:40:15.932000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:15.950000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:15.968000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:15.987000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:16.000000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:16.003000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:16.011000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:16.021000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:16.023000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:16.035000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:16.039000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:16.059000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:16.071000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:16.078000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:16.082000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:16.094000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:16.096000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:16.106000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:16.114000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:16.135000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:16.147000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:16.158000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:16.180000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:16.191000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:16.585000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:16.602000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:16.627000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:16.648000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:16.663000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:16.672000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:16.672000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:16.683000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:16.690000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:16.696000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:16.713000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:16.736000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:16.746000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:16.751000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:16.760000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:16.763000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:16.769000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:16.780000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:16.786000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:16.809000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:16.817000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:16.824000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:16.833000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:16.835000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:16.841000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:16.853000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:16.857000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:16.881000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:16.896000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:16.904000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:16.913000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:16.924000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:17.488000 822 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank4]:W1026 11:40:17.494000 819 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank1]:W1026 11:40:17.497000 816 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank3]:W1026 11:40:17.506000 818 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank5]:W1026 11:40:17.510000 820 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank0]:W1026 11:40:17.539000 815 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank2]:W1026 11:40:17.556000 817 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank6]:W1026 11:40:17.575000 821 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_702 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_708 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_694 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_709 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_703 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_706 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8588 seconds and 0.2121 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_700 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_698 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_701 0.0083 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_702 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_708 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_703 0.0085 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_694 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_705 0.0086 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8982 seconds and 0.2232 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0083 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_708 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_706 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_707 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_709 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_695 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_694 0.0085 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9072 seconds and 0.2010 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_701 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_698 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_702 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_708 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_703 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_706 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_707 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9145 seconds and 0.2585 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_701 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0082 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_698 0.0083 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_702 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_708 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_706 0.0085 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_707 0.0085 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9160 seconds and 0.1836 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0084 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0084 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_702 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_708 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_695 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_709 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_705 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9198 seconds and 0.2718 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_700 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0082 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_698 0.0082 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0082 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_702 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_708 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_706 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_694 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9316 seconds and 0.1183 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_700 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_699 0.0082 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_694 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_695 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_702 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_708 0.0085 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0085 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9282 seconds and 0.1500 seconds precompiling for 25 choices
[rank0]:W1026 11:40:27.767000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:27.778000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:27.790000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:27.820000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:27.954000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:28.052000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:28.063000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:28.270000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:28.293000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:28.305000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:28.334000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:28.461000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:28.568000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:28.577000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:28.908000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:29.430000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:29.795000 815 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank1]:W1026 11:40:29.797000 816 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank3]:W1026 11:40:29.858000 818 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank4]:W1026 11:40:29.956000 819 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank7]:W1026 11:40:29.985000 822 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank2]:W1026 11:40:30.053000 817 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank5]:W1026 11:40:30.394000 820 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank6]:W1026 11:40:30.697000 821 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_728 0.0095 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0095 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0096 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0098 ms 84.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8875 seconds and 0.4149 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0081 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0083 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0083 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0093 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_728 0.0093 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_732 0.0093 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_729 0.0094 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0094 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0095 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9377 seconds and 0.4150 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0085 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0092 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0093 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_714 0.0094 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_728 0.0094 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0095 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_715 0.0097 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9339 seconds and 0.4069 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0092 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_733 0.0093 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0093 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_729 0.0093 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_728 0.0094 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0097 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8775 seconds and 0.4115 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_721 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_720 0.0082 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0083 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0083 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_732 0.0092 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_733 0.0092 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_728 0.0094 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0094 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0095 ms 84.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0096 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9356 seconds and 0.4218 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0080 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0080 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0083 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0083 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0090 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0092 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_714 0.0092 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0093 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_728 0.0093 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0095 ms 84.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9357 seconds and 0.4119 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_723 0.0084 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_733 0.0092 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0094 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_729 0.0095 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_728 0.0097 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_714 0.0099 ms 83.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0099 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9012 seconds and 0.4035 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0083 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_733 0.0092 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0093 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_729 0.0093 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_728 0.0094 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0096 ms 84.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0096 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9212 seconds and 0.4178 seconds precompiling for 25 choices
[rank3]:W1026 11:40:36.458000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:36.537000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:36.588000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:36.619000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:36.671000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:36.698000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:36.750000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:36.751000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:36.802000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:36.807000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:36.885000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:36.936000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:36.984000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:37.056000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:37.064000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:37.115000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:37.133000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:37.183000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:37.249000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:37.272000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:37.329000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:37.352000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:37.380000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:37.403000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:40 DP3 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:40 DP1 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:40 DP4 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:41 DP0 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:41 DP7 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:41 DP2 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:41 DP5 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:41 DP6 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1026 11:40:42.035000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:42.052000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:42.063000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:42.078000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:42.084000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:42.098000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:42.106000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:42.122000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:42.134000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:42.149000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:42.149000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:42.155000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:42.165000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:42.168000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:42.177000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:42.187000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:42.192000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:42.197000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:42 DP0 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1026 11:40:42.211000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:42 DP3 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:42 DP4 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:42 DP1 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:42 DP2 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1026 11:40:42.258000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:42.262000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:42 DP7 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1026 11:40:42.302000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:42.333000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:42 DP5 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1026 11:40:42.376000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:42 DP6 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1026 11:40:42.931000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:42.941000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:42.954000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:42.967000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:42.981000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:42.993000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:43.006000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:43.020000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:43.220000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:43.229000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:43.241000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:43.264000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:43.277000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:43.289000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:43.303000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:43.313000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:43.508000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:43.518000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:43.538000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:43.554000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:43.568000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:43.580000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:43.593000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:43.605000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:43 DP4 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:43 DP7 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:43 DP3 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:43 DP1 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:43 DP2 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:43 DP0 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:43 DP5 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:44 DP6 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1026 11:40:44.190000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:44.202000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:44.225000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:44.248000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:44.261000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:44.264000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:44.272000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:44.272000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:44.290000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:44.298000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:44.304000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:44.320000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:44.331000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:44.336000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:44.343000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:44.344000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:44.361000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:44.370000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:44.375000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:44 DP4 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1026 11:40:44.392000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:44 DP7 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1026 11:40:44.407000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:44.415000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:44 DP3 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1026 11:40:44.433000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:44.446000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:44 DP1 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:44 DP2 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:44 DP0 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:44 DP5 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-26 11:40:44 DP6 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1026 11:40:44.870000 819 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446df0>
[rank7]:W1026 11:40:44.882000 822 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6880>
[rank4]:W1026 11:40:44.894000 819 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446f40>
[rank7]:W1026 11:40:44.906000 822 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6790>
[rank3]:W1026 11:40:44.911000 818 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e80>
[rank3]:W1026 11:40:44.935000 818 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e20>
[rank1]:W1026 11:40:44.936000 816 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba310>
[rank2]:W1026 11:40:44.950000 817 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f62539b6430>
[rank1]:W1026 11:40:44.959000 816 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba8e0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:40:44 DP4 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1026 11:40:44.962000 815 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e670>
[rank2]:W1026 11:40:44.973000 817 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6252c5abe0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:40:44 DP7 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1026 11:40:44.976000 820 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81a730>
[rank0]:W1026 11:40:44.985000 815 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e820>
[rank6]:W1026 11:40:44.988000 821 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b2fd0>
[rank5]:W1026 11:40:45.000000 820 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81ae50>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:40:45 DP3 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1026 11:40:45.012000 821 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b3180>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:40:45 DP1 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:40:45 DP2 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:40:45 DP0 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:40:45 DP5 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:40:45 DP6 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1026 11:40:46.214000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:46.226000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:46.247000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:46.287000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:46.298000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:46.309000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:46.323000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:46.338000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:46.510000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:46.522000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:46.535000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:46.575000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:46.587000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:46.597000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:46.611000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:46.626000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:46.794000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:46.806000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:46.823000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:46.863000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:46.875000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:46.885000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:46.899000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:46.913000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:47.080000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:47.091000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:47.114000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:47.154000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:47.169000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:47.180000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:47.193000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:47.206000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:47.376000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:47.386000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:47.402000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:47.442000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:47.458000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:47.470000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:47.483000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:47.495000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:47.670000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:47.683000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:47.693000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:47.727000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:47.744000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:47.754000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:47.770000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:47.785000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:47.958000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:47.974000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:47.984000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:48.019000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:48.031000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:48.042000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:48.057000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:48.071000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:48.246000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:48.262000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:48.272000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:48.307000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:48.320000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:48.329000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:48.344000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:48.359000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:48.534000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:48.550000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:48.559000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:48.607000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:48.619000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:48.631000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:48.644000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:48.658000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:48.830000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:48.842000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:48.852000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:48.895000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:48.908000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:48.918000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:48.933000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:48.947000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:49.116000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:49.128000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:49.142000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:49.186000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:49.200000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:49.212000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:49.225000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:49.238000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:49.412000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:49.421000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:49.442000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:49.486000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:49.500000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:49.513000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:49.526000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:49.538000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:49.714000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:49.726000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:49.737000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:49.771000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:49.784000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:49.794000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:49.814000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:49.828000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:50.002000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:50.014000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:50.027000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:50.063000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:50.076000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:50.085000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:50.102000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:50.116000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:50.290000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:50.302000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:50.315000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:50.355000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:50.368000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:50.377000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:50.392000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:50.406000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:50.586000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:50.598000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:50.609000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:50.647000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:50.660000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:50.669000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:50.684000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:50.699000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:50.890000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:50.902000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:50.912000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:50.939000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:50.952000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:50.961000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:50.978000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:50.992000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:51.180000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:51.191000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:51.205000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:51.230000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:51.246000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:51.257000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:51.270000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:51.282000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:51.467000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:51.480000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:51.494000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:51.518000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:51.535000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:51.546000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:51.559000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:51.571000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:51.758000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:51.770000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:51.787000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:51.812000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:51.832000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:51.843000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:51.856000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:51.871000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:52.046000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:52.058000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:52.075000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:52.103000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:52.124000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:52.134000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:52.149000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:52.163000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:52.969000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:52.980000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:52.992000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:53.005000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:53.018000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:53.029000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:53.040000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:53.080000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:53.850000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:53.862000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:53.873000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:53.885000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:53.898000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:53.909000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:53.940000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:53.964000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:54.722000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:54.732000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:54.745000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:54.755000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:54.768000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:54.780000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:54.832000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:54.842000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:55.023000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:55.034000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:55.047000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:55.061000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:55.075000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:55.137000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:55.149000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:55.751000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:55.926000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:55.936000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:55.947000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:55.959000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:55.971000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:55.984000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:55.993000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:56.036000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:56.218000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:56.236000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:56.247000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:56.259000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:56.273000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:56.287000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:56.297000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:56.324000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:56.510000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:56.532000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:56.544000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:56.556000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:56.569000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:56.583000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:56.594000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:56.611000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:56.802000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:56.827000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:56.840000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:56.853000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:56.865000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:56.879000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:56.889000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:56.902000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:57.094000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:57.124000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:57.136000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:57.149000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:57.165000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:57.180000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:57.191000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:57.204000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:57.384000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:57.435000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:57.445000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:57.458000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:57.470000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:57.481000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:57.494000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:57.509000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:57.684000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:57.735000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:57.745000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:57.758000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:57.769000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:57.781000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:57.793000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:57.808000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:57.983000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:58.028000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:58.042000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:58.053000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:58.066000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:58.080000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:58.091000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:58.104000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:58.280000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:58.330000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:58.341000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:58.355000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:58.365000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:58.377000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:58.390000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:58.404000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:58.576000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:58.630000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:58.640000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:58.654000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:58.665000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:58.676000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:58.689000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:58.704000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:58.879000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:58.926000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:58.937000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:58.950000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:58.961000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:58.974000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:58.985000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:59.000000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:59.171000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:59.222000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:59.233000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:59.246000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:59.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:59.270000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:59.282000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:59.297000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:59.471000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:59.518000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:59.529000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:59.542000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:59.553000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:59.565000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:59.578000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:59.593000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:40:59.767000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:40:59.814000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:40:59.825000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:40:59.838000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:40:59.850000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:40:59.861000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:40:59.875000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:40:59.890000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:00.063000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:00.115000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:00.128000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:00.142000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:00.152000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:00.164000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:00.176000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:00.191000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:00.364000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:00.407000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:00.427000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:00.440000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:00.451000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:00.462000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:00.475000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:00.491000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:00.664000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:00.699000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:00.734000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:00.749000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:00.759000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:00.771000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:00.783000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:00.799000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:00.971000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:00.991000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:01.030000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:01.044000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:01.055000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:01.066000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:01.079000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:01.094000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:01.267000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:01.283000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:01.326000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:01.340000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:01.351000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:01.364000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:01.375000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:01.391000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:01.563000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:01.575000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:01.618000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:01.632000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:01.643000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:01.654000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:01.667000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:01.682000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:01.855000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:01.867000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:01.914000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:01.928000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:01.940000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:01.952000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:01.964000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:01.979000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:02.152000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:02.161000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:02.210000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:02.224000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:02.235000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:02.247000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:02.259000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:02.275000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:02.447000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:02.457000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:02.506000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:02.521000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:02.531000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:02.543000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:02.556000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:02.571000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:02.744000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:02.754000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:02.798000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:02.814000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:02.825000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:02.836000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:02.849000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:02.864000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:03.039000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:03.049000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:03.090000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:03.106000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:03.117000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:03.130000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:03.141000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:03.156000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:03.332000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:03.343000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:03.386000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:03.400000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:03.411000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:03.423000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:03.435000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:03.451000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:03.624000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:03.635000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:03.682000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:03.696000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:03.707000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:03.718000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:03.731000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:03.746000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:03.920000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:03.930000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:03.978000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:03.992000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:04.003000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:04.014000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:04.027000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:04.042000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:04.216000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:04.226000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:04.270000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:04.287000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:04.297000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:04.308000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:04.321000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:04.336000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:04.515000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:04.525000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:04.566000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:04.579000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:04.591000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:04.603000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:04.614000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:04.629000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:04.811000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:04.821000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:04.863000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:04.876000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:04.887000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:04.898000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:04.911000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:04.926000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:05.103000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:05.115000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:05.166000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:05.179000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:05.191000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:05.202000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:05.215000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:05.230000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:05.968000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:05.980000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:05.993000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:06.006000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:06.017000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:06.026000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:06.037000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:06.049000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0621 ms 100.0% 
  triton_mm_737 0.1036 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_736 0.1038 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_747 0.1044 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1045 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1094 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1095 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1133 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1134 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_741 0.1484 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1936 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_736 0.1037 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1038 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_746 0.1046 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1047 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1094 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_745 0.1126 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1126 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_741 0.1484 ms 41.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2096 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_736 0.1039 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1039 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_746 0.1045 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1045 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1092 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1093 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1128 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1130 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_741 0.1478 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2299 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0621 ms 100.0% 
  triton_mm_736 0.1036 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1037 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_746 0.1048 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1049 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_755 0.1095 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1096 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1132 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1133 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_741 0.1483 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2310 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0618 ms 100.0% 
  triton_mm_737 0.1037 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_736 0.1037 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_746 0.1045 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1045 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_755 0.1090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1126 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1128 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_740 0.1477 ms 41.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2066 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0621 ms 100.0% 
  triton_mm_736 0.1037 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1037 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_746 0.1045 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1046 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_755 0.1092 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1092 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1129 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1130 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_741 0.1480 ms 42.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2483 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0622 ms 100.0% 
  triton_mm_736 0.1037 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1038 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_746 0.1044 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1045 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_755 0.1094 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_744 0.1132 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1133 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_740 0.1484 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2262 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_737 0.1035 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_736 0.1036 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_747 0.1046 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1046 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_754 0.1091 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1091 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1127 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1128 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_741 0.1478 ms 41.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1974 seconds and 0.0000 seconds precompiling for 25 choices
Capturing batches (bs=2 avail_mem=38.91 GB):  98%|| 51/52 [08:59<01:02, 62.82s/it]Capturing batches (bs=1 avail_mem=38.46 GB):  98%|| 51/52 [08:59<01:02, 62.82s/it][rank6]:W1026 11:41:17.976000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:17.990000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:18.023000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:18.036000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:18.048000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:18.049000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:18.062000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:18.069000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:18.095000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:18.109000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:18.121000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:18.124000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:18.137000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:18.141000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:18.171000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:18.185000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:18.196000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:18.207000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:18.216000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:18.278000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:18.343000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:18.354000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:18.415000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:18.492000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:18.632000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:18.644000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:18.688000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:18.699000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:18.715000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:18.720000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:18.728000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:18.731000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:18.777000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:18.786000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:18.794000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:18.804000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:18.806000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:18.820000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:18.851000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:18.857000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:18.859000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:18.866000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:18.875000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:18.880000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:18.894000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:18.924000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:18.932000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:18.943000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:18.952000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:18.966000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:19.002000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:19.016000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:19.087000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:19.090000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:19.163000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:19.235000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:25.164000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:25.172000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:25.383000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:25.454000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:25.524000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:25.677000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:25.701000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:25.815000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:25.861000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:25.870000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:25.886000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:25.960000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:26.026000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:26.326000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:26.369000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:26.378000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:27.993000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:28.044000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:28.074000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:28.125000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:28.133000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:28.184000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:28.259000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:28.268000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:28.339000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:28.349000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:28.379000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:28.391000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:28.402000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:28.459000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:28.511000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:28.542000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:28.622000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:28.651000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:28.674000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:28.707000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:28.732000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:28.785000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:28.786000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:28.837000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:32.350000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:32.361000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:32.370000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:32.381000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:32.393000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:32.403000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:32.416000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:32.421000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:32.434000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:32.442000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:32.453000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:32.464000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:32.465000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:32.473000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:32.477000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:32.485000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:32.487000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:32.496000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:32.509000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:32.516000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:32.530000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:32.568000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:32.639000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:32.682000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:33.251000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:33.263000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:33.275000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:33.287000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:33.297000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:33.311000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:33.326000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:33.341000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:33.552000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:33.562000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:33.574000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:33.585000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:33.596000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:33.610000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:33.625000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:33.640000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:33.852000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:33.861000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:33.874000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:33.886000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:33.896000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:33.910000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:33.925000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:33.940000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:34.559000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:34.568000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:34.578000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:34.591000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:34.603000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:34.614000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:34.629000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:34.633000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:34.641000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:34.643000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:34.650000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:34.663000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:34.677000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:34.686000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:34.701000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:34.706000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:34.713000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:34.715000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:34.722000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:34.735000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:34.749000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:34.758000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:34.773000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:34.787000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:35.232000 821 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b2fd0>
[rank4]:W1026 11:41:35.242000 819 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446df0>
[rank0]:W1026 11:41:35.253000 815 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e670>
[rank6]:W1026 11:41:35.255000 821 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3a134b3180>
[rank7]:W1026 11:41:35.265000 822 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6880>
[rank4]:W1026 11:41:35.265000 819 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f65cb446f40>
[rank0]:W1026 11:41:35.276000 815 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9707d2e820>
[rank5]:W1026 11:41:35.276000 820 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81a730>
[rank7]:W1026 11:41:35.288000 822 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f08464f6790>
[rank1]:W1026 11:41:35.288000 816 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba310>
[rank5]:W1026 11:41:35.299000 820 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9c2b81ae50>
[rank2]:W1026 11:41:35.303000 817 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f62539b6430>
[rank1]:W1026 11:41:35.312000 816 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb13f4ba8e0>
[rank3]:W1026 11:41:35.318000 818 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e80>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:41:35 DP6 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1026 11:41:35.327000 817 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6252c5abe0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:41:35 DP4 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1026 11:41:35.341000 818 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec1a7186e20>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:41:35 DP0 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:41:35 DP7 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:41:35 DP5 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:41:35 DP1 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:41:35 DP2 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:41:35 DP3 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1026 11:41:36.966000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:36.979000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:36.992000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:37.003000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:37.014000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:37.024000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:37.035000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:37.084000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:37.267000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:37.280000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:37.293000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:37.305000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:37.317000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:37.330000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:37.340000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:37.376000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:37.579000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:37.592000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:37.605000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:37.618000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:37.630000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:37.642000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:37.653000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:37.668000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:37.879000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:37.892000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:37.905000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:37.917000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:37.930000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:37.944000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:37.954000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:37.966000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:38.180000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:38.193000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:38.206000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:38.217000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:38.230000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:38.242000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:38.253000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:38.264000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:38.487000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:38.500000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:38.513000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:38.525000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:38.537000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:38.550000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:38.561000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:38.572000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:38.787000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:38.800000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:38.813000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:38.825000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:38.837000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:38.849000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:38.860000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:38.871000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:39.087000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:39.100000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:39.113000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:39.125000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:39.137000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:39.150000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:39.160000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:39.171000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:39.384000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:39.396000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:39.407000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:39.416000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:39.435000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:39.450000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:39.463000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:39.477000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:39.688000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:39.698000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:39.709000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:39.719000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:39.733000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:39.747000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:39.762000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:39.776000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:39.992000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:40.002000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:40.013000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:40.022000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:40.037000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:40.051000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:40.065000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:40.079000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:40.311000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:40.324000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:40.338000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:40.350000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:40.362000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:40.373000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:40.385000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:40.397000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:40.611000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:40.624000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:40.637000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:40.649000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:40.661000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:40.674000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:40.684000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:40.696000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:40.908000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:40.920000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:40.932000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:40.942000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:40.959000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:40.974000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:40.987000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:41.002000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:41.212000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:41.222000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:41.233000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:41.243000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:41.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:41.273000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:41.286000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:41.300000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:41.516000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:41.526000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:41.537000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:41.547000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:41.562000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:41.576000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:41.591000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:41.605000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:41.820000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:41.830000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:41.842000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:41.851000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:41.865000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:41.880000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:41.894000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:41.909000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:42.124000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:42.134000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:42.145000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:42.155000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:42.170000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:42.184000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:42.198000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:42.213000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:42.428000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:42.438000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:42.449000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:42.459000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:42.474000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:42.488000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:42.503000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:42.517000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:42.732000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:42.742000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:42.753000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:42.763000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:42.777000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:42.792000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:42.805000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:42.820000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:43.039000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:43.052000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:43.066000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:43.077000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:43.089000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:43.101000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:43.113000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:43.125000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:43.339000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:43.354000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:43.368000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:43.380000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:43.392000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:43.405000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:43.415000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:43.427000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:43.643000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:43.656000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:43.669000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:43.681000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:43.694000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:43.706000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:43.716000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:43.728000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:43.940000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:43.952000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:43.964000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:43.975000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:43.995000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:44.009000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:44.023000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:44.038000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:44.244000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:44.256000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:44.267000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:44.276000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:44.291000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:44.307000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:44.321000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:44.336000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:44.551000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:44.564000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:44.577000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:44.589000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:44.602000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:44.613000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:44.625000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:44.637000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:44.855000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:44.868000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:44.881000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:44.893000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:44.905000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:44.917000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:44.928000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:44.939000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:45.157000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:45.167000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:45.178000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:45.188000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:45.207000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:45.221000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:45.236000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:45.250000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:45.463000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:45.476000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:45.489000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:45.501000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:45.514000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:45.525000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:45.537000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:45.549000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:45.767000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:45.780000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:45.793000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:45.805000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:45.817000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:45.829000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:45.841000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:45.852000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:46.068000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:46.079000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:46.090000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:46.100000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:46.119000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:46.133000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:46.147000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:46.162000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:46.376000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:46.386000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:46.397000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:46.406000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:46.421000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:46.435000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:46.449000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:46.463000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:46.684000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:46.696000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:46.707000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:46.716000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:46.731000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:46.745000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:46.759000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:46.774000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:46.996000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:47.006000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:47.016000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:47.027000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:47.040000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:47.055000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:47.069000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:47.083000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:47.300000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:47.312000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:47.322000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:47.332000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:47.346000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:47.360000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:47.375000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:47.389000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:47.611000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:47.624000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:47.638000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:47.650000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:47.662000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:47.674000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:47.686000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:47.698000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:47.915000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:47.930000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:47.943000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:47.955000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:47.968000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:47.980000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:47.991000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:48.002000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:48.217000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:48.228000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:48.240000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:48.250000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:48.270000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:48.285000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:48.299000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:48.313000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:48.527000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:48.540000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:48.553000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:48.565000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:48.578000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:48.590000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:48.602000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:48.614000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:48.843000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:48.856000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:48.869000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:48.882000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:48.894000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:48.906000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:48.917000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:48.928000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:49.828000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:49.837000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:49.848000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:49.861000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:49.874000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:49.886000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:49.940000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:49.952000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:50.820000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:50.830000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:50.841000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:50.851000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:50.865000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:50.877000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:50.935000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:50.950000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:51.851000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:51.864000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:51.876000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:51.889000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:51.900000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:51.920000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:51.956000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:51.969000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:52.152000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:52.164000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:52.175000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:52.192000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:52.206000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:52.222000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:52.258000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:52.273000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:52.456000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:52.472000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:52.482000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:52.500000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:52.513000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:52.527000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:52.557000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:52.573000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:52.763000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:52.782000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:52.794000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:52.808000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:52.820000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:52.832000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:52.856000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:52.872000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:53.067000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:53.086000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:53.099000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:53.114000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:53.126000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:53.137000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:53.156000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:53.172000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:53.371000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:53.390000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:53.403000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:53.418000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:53.430000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:53.442000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:53.456000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:53.472000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:53.675000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:53.694000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:53.707000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:53.722000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:53.734000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:53.745000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:53.760000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:53.776000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:53.981000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:53.999000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:54.010000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:54.026000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:54.041000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:54.051000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:54.064000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:54.076000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:54.295000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:54.308000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:54.320000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:54.334000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:54.346000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:54.358000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:54.370000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:54.382000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:54.603000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:54.616000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:54.629000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:54.642000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:54.654000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:54.666000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:54.678000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:54.690000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:54.911000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:54.924000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:54.936000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:54.950000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:54.962000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:54.973000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:54.985000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:54.997000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:55.215000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:55.231000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:55.243000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:55.259000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:55.270000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:55.281000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:55.293000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:55.305000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:55.522000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:55.539000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:55.550000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:55.566000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:55.578000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:55.590000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:55.602000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:55.614000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:55.839000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:55.855000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:55.867000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:55.882000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:55.893000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:55.905000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:55.917000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:55.929000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:56.159000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:56.175000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:56.187000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:56.199000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:56.212000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:56.224000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:56.236000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:56.248000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1026 11:41:56.996000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1026 11:41:57.009000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1026 11:41:57.020000 822 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1026 11:41:57.031000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1026 11:41:57.045000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1026 11:41:57.056000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1026 11:41:57.069000 820 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1026 11:41:57.078000 821 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_761 0.1043 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_770 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1083 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1084 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1118 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1120 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_765 0.1397 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1808 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_761 0.1043 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1043 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_770 0.1044 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1045 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_779 0.1086 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1087 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1124 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1125 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_765 0.1404 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1719 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0616 ms 100.0% 
  triton_mm_760 0.1040 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1041 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_770 0.1042 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1043 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_779 0.1086 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1087 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_768 0.1127 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1127 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_765 0.1407 ms 43.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1942 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0618 ms 100.0% 
  triton_mm_760 0.1039 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1039 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_770 0.1043 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1043 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1086 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1087 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_769 0.1125 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1126 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_765 0.1407 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2484 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0618 ms 100.0% 
  triton_mm_761 0.1043 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1043 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_770 0.1045 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1047 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1088 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1088 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1127 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1127 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_764 0.1408 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1965 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_761 0.1042 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1043 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_770 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1084 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1084 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_769 0.1119 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1119 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_764 0.1399 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1685 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_760 0.1041 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1041 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_771 0.1042 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_770 0.1043 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1085 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1085 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_768 0.1122 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1122 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_764 0.1400 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2061 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0617 ms 100.0% 
  triton_mm_761 0.1039 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1040 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_770 0.1043 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1043 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1086 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1086 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_769 0.1122 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1124 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_764 0.1404 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2497 seconds and 0.0001 seconds precompiling for 25 choices
Capturing batches (bs=1 avail_mem=38.46 GB): 100%|| 52/52 [09:51<00:00, 59.38s/it]Capturing batches (bs=1 avail_mem=38.46 GB): 100%|| 52/52 [09:51<00:00, 11.37s/it]
[2025-10-26 11:42:07 DP0 TP0] Registering 22 cuda graph addresses
[2025-10-26 11:42:09 DP7 TP7] Capture cuda graph end. Time elapsed: 592.77 s. mem usage=13.53 GB. avail mem=37.51 GB.
[2025-10-26 11:42:09 DP3 TP3] Capture cuda graph end. Time elapsed: 592.70 s. mem usage=13.62 GB. avail mem=37.29 GB.
[2025-10-26 11:42:09 DP4 TP4] Capture cuda graph end. Time elapsed: 591.82 s. mem usage=13.60 GB. avail mem=37.36 GB.
[2025-10-26 11:42:09 DP0 TP0] Capture cuda graph end. Time elapsed: 592.69 s. mem usage=13.21 GB. avail mem=38.11 GB.
[2025-10-26 11:42:09 DP5 TP5] Capture cuda graph end. Time elapsed: 592.76 s. mem usage=13.54 GB. avail mem=37.50 GB.
[2025-10-26 11:42:09 DP6 TP6] Capture cuda graph end. Time elapsed: 592.62 s. mem usage=13.54 GB. avail mem=37.49 GB.
[2025-10-26 11:42:09 DP1 TP1] Capture cuda graph end. Time elapsed: 592.78 s. mem usage=13.62 GB. avail mem=37.28 GB.
[2025-10-26 11:42:09 DP2 TP2] Capture cuda graph end. Time elapsed: 592.87 s. mem usage=13.70 GB. avail mem=37.20 GB.
[2025-10-26 11:42:09 DP0 TP0] max_total_num_tokens=676049, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=2112, context_len=163840, available_gpu_mem=38.11 GB
[2025-10-26 11:42:09] INFO:     Started server process [42]
[2025-10-26 11:42:09] INFO:     Waiting for application startup.
[2025-10-26 11:42:09] INFO:     Application startup complete.
[2025-10-26 11:42:09] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-26 11:42:10] INFO:     127.0.0.1:45730 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-26 11:42:10] INFO:     127.0.0.1:45732 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-26 11:42:10 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:10 DP3 TP3] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:10 DP2 TP2] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:10 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:11 DP7 TP7] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:11 DP4 TP4] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:11 DP6 TP6] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:11 DP5 TP5] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:11 DP1 TP1] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:11 DP2 TP2] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:11 DP0 TP0] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:11 DP3 TP3] [fused_moe] using default for (28, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:11 DP4 TP4] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:11 DP5 TP5] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:11 DP6 TP6] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:11 DP7 TP7] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:13] INFO:     127.0.0.1:45742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:13] The server is fired up and ready to roll!
[2025-10-26 11:42:18] INFO:     127.0.0.1:47270 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-26 11:42:18 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP7 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP4 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP5 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP6 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP3 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP2 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP1 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP0 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:18 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 733, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP5 TP5] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP4 TP4] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP0 TP0] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP3 TP3] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP7 TP7] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP6 TP6] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP2 TP2] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP1 TP1] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:42:18 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 1435, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:18 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 1504, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:18 DP2 TP2] Prefill batch, #new-seq: 3, #new-token: 2143, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:18 DP0 TP0] Prefill batch, #new-seq: 2, #new-token: 162, #cached-token: 1334, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:18 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 1460, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:18 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 1428, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:42:18 DP1 TP1] Prefill batch, #new-seq: 4, #new-token: 2908, #cached-token: 4, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-26 11:42:18 DP3 TP3] Prefill batch, #new-seq: 3, #new-token: 2144, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[2025-10-26 11:42:18 DP4 TP4] Prefill batch, #new-seq: 22, #new-token: 16162, #cached-token: 22, token usage: 0.00, #running-req: 2, #queue-req: 2, 
[2025-10-26 11:42:18 DP2 TP2] Prefill batch, #new-seq: 22, #new-token: 16000, #cached-token: 22, token usage: 0.00, #running-req: 3, #queue-req: 1, 
[2025-10-26 11:42:18 DP6 TP6] Prefill batch, #new-seq: 22, #new-token: 16012, #cached-token: 22, token usage: 0.00, #running-req: 2, #queue-req: 1, 
[2025-10-26 11:42:18 DP3 TP3] Prefill batch, #new-seq: 22, #new-token: 16010, #cached-token: 22, token usage: 0.00, #running-req: 3, #queue-req: 1, 
[2025-10-26 11:42:18 DP7 TP7] Prefill batch, #new-seq: 22, #new-token: 16064, #cached-token: 22, token usage: 0.00, #running-req: 2, #queue-req: 1, 
[2025-10-26 11:42:18 DP5 TP5] Prefill batch, #new-seq: 22, #new-token: 15897, #cached-token: 22, token usage: 0.00, #running-req: 2, #queue-req: 2, 
[2025-10-26 11:42:18 DP1 TP1] Prefill batch, #new-seq: 21, #new-token: 1229, #cached-token: 14049, token usage: 0.01, #running-req: 5, #queue-req: 0, 
[2025-10-26 11:42:18 DP0 TP0] Prefill batch, #new-seq: 23, #new-token: 1571, #cached-token: 15341, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:42:27 DP6 TP6] Prefill batch, #new-seq: 141, #new-token: 8196, #cached-token: 94330, token usage: 0.02, #running-req: 24, #queue-req: 0, 
[2025-10-26 11:42:27 DP5 TP5] Prefill batch, #new-seq: 141, #new-token: 8667, #cached-token: 94331, token usage: 0.02, #running-req: 24, #queue-req: 0, 
[2025-10-26 11:42:27 DP2 TP2] Prefill batch, #new-seq: 140, #new-token: 8239, #cached-token: 93670, token usage: 0.02, #running-req: 25, #queue-req: 0, 
[2025-10-26 11:42:27 DP4 TP4] Prefill batch, #new-seq: 141, #new-token: 8394, #cached-token: 94339, token usage: 0.03, #running-req: 24, #queue-req: 0, 
[2025-10-26 11:42:27 DP0 TP0] Prefill batch, #new-seq: 139, #new-token: 8796, #cached-token: 93008, token usage: 0.00, #running-req: 25, #queue-req: 0, 
[2025-10-26 11:42:27 DP7 TP7] Prefill batch, #new-seq: 141, #new-token: 8786, #cached-token: 94337, token usage: 0.02, #running-req: 24, #queue-req: 0, 
[2025-10-26 11:42:27 DP3 TP3] Prefill batch, #new-seq: 140, #new-token: 8503, #cached-token: 93674, token usage: 0.02, #running-req: 25, #queue-req: 0, 
[2025-10-26 11:42:27 DP1 TP1] Prefill batch, #new-seq: 139, #new-token: 8454, #cached-token: 92999, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:42:34] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:35] INFO:     127.0.0.1:51926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:35 DP2 TP2] Decode batch, #running-req: 165, #token: 15622, token usage: 0.02, cuda graph: True, gen throughput (token/s): 204.16, #queue-req: 0, 
[2025-10-26 11:42:35 DP0 TP0] Decode batch, #running-req: 163, #token: 16405, token usage: 0.02, cuda graph: True, gen throughput (token/s): 202.73, #queue-req: 0, 
[2025-10-26 11:42:35 DP5 TP5] Decode batch, #running-req: 165, #token: 16044, token usage: 0.02, cuda graph: True, gen throughput (token/s): 204.16, #queue-req: 0, 
[2025-10-26 11:42:35 DP4 TP4] Decode batch, #running-req: 165, #token: 16016, token usage: 0.02, cuda graph: True, gen throughput (token/s): 204.16, #queue-req: 0, 
[2025-10-26 11:42:35 DP7 TP7] Decode batch, #running-req: 165, #token: 16305, token usage: 0.02, cuda graph: True, gen throughput (token/s): 204.16, #queue-req: 0, 
[2025-10-26 11:42:35 DP1 TP1] Decode batch, #running-req: 165, #token: 16051, token usage: 0.02, cuda graph: True, gen throughput (token/s): 204.16, #queue-req: 0, 
[2025-10-26 11:42:35 DP6 TP6] Decode batch, #running-req: 165, #token: 15732, token usage: 0.02, cuda graph: True, gen throughput (token/s): 204.16, #queue-req: 0, 
[2025-10-26 11:42:35 DP3 TP3] Decode batch, #running-req: 165, #token: 16031, token usage: 0.02, cuda graph: True, gen throughput (token/s): 204.16, #queue-req: 0, 
[2025-10-26 11:42:35] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:35] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:52200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:47998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:58294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:50544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:52538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:36] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:53330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:50670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:51902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:54200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:49690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:49678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:47838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:50952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:37] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:51702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:52056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:51856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:56750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:47620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:52314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:38] INFO:     127.0.0.1:59164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:47906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:47772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39 DP5 TP5] Decode batch, #running-req: 117, #token: 16284, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1323.40, #queue-req: 0, 
[2025-10-26 11:42:39 DP1 TP1] Decode batch, #running-req: 112, #token: 15899, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1310.95, #queue-req: 0, 
[2025-10-26 11:42:39 DP2 TP2] Decode batch, #running-req: 122, #token: 16755, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1347.63, #queue-req: 0, 
[2025-10-26 11:42:39 DP4 TP4] Decode batch, #running-req: 121, #token: 16601, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1306.23, #queue-req: 0, 
[2025-10-26 11:42:39 DP6 TP6] Decode batch, #running-req: 122, #token: 16798, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1335.46, #queue-req: 0, 
[2025-10-26 11:42:39 DP3 TP3] Decode batch, #running-req: 127, #token: 17876, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1329.03, #queue-req: 0, 
[2025-10-26 11:42:39] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39 DP0 TP0] Decode batch, #running-req: 126, #token: 18378, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1322.77, #queue-req: 0, 
[2025-10-26 11:42:39 DP7 TP7] Decode batch, #running-req: 116, #token: 16154, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1342.04, #queue-req: 0, 
[2025-10-26 11:42:39] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:39] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:56480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:50576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:56166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:55638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:51276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:48226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:50660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:59116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:40] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:49574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:47766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:41] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:48002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:47494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:47666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:47408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:49920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:53814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:54048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:42] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43 DP1 TP1] Decode batch, #running-req: 48, #token: 9648, token usage: 0.01, cuda graph: True, gen throughput (token/s): 862.41, #queue-req: 0, 
[2025-10-26 11:42:43 DP5 TP5] Decode batch, #running-req: 49, #token: 9476, token usage: 0.01, cuda graph: True, gen throughput (token/s): 954.08, #queue-req: 0, 
[2025-10-26 11:42:43 DP7 TP7] Decode batch, #running-req: 47, #token: 9002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 867.13, #queue-req: 0, 
[2025-10-26 11:42:43 DP2 TP2] Decode batch, #running-req: 54, #token: 10135, token usage: 0.01, cuda graph: True, gen throughput (token/s): 968.32, #queue-req: 0, 
[2025-10-26 11:42:43 DP4 TP4] Decode batch, #running-req: 60, #token: 11403, token usage: 0.02, cuda graph: True, gen throughput (token/s): 924.09, #queue-req: 0, 
[2025-10-26 11:42:43 DP0 TP0] Decode batch, #running-req: 62, #token: 11994, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1033.37, #queue-req: 0, 
[2025-10-26 11:42:43 DP3 TP3] Decode batch, #running-req: 70, #token: 12864, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1071.79, #queue-req: 0, 
[2025-10-26 11:42:43] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43 DP6 TP6] Decode batch, #running-req: 53, #token: 9863, token usage: 0.01, cuda graph: True, gen throughput (token/s): 933.01, #queue-req: 0, 
[2025-10-26 11:42:43] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:43] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:49344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:52006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:48702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:52718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:52152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:50506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:51398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:44] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:52866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:48442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:53920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:49232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:48272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:55426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:48460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:51586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:55788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:49326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:51468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:45] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46 DP3 TP3] Decode batch, #running-req: 24, #token: 6071, token usage: 0.01, cuda graph: True, gen throughput (token/s): 664.07, #queue-req: 0, 
[2025-10-26 11:42:46 DP1 TP1] Decode batch, #running-req: 20, #token: 5410, token usage: 0.01, cuda graph: True, gen throughput (token/s): 470.73, #queue-req: 0, 
[2025-10-26 11:42:46 DP4 TP4] Decode batch, #running-req: 27, #token: 6936, token usage: 0.01, cuda graph: True, gen throughput (token/s): 607.20, #queue-req: 0, 
[2025-10-26 11:42:46 DP0 TP0] Decode batch, #running-req: 23, #token: 5613, token usage: 0.01, cuda graph: True, gen throughput (token/s): 639.92, #queue-req: 0, 
[2025-10-26 11:42:46 DP7 TP7] Decode batch, #running-req: 24, #token: 6239, token usage: 0.01, cuda graph: True, gen throughput (token/s): 511.32, #queue-req: 0, 
[2025-10-26 11:42:46 DP6 TP6] Decode batch, #running-req: 26, #token: 6067, token usage: 0.01, cuda graph: True, gen throughput (token/s): 586.37, #queue-req: 0, 
[2025-10-26 11:42:46] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46 DP2 TP2] Decode batch, #running-req: 15, #token: 3855, token usage: 0.01, cuda graph: True, gen throughput (token/s): 483.71, #queue-req: 0, 
[2025-10-26 11:42:46 DP5 TP5] Decode batch, #running-req: 16, #token: 4331, token usage: 0.01, cuda graph: True, gen throughput (token/s): 451.35, #queue-req: 0, 
[2025-10-26 11:42:46] INFO:     127.0.0.1:48114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:53602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:51650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:53144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:48490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:46] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:47372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:49246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:47] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48 DP5 TP5] Decode batch, #running-req: 7, #token: 2508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 225.40, #queue-req: 0, 
[2025-10-26 11:42:48 DP6 TP6] Decode batch, #running-req: 9, #token: 3131, token usage: 0.00, cuda graph: True, gen throughput (token/s): 307.75, #queue-req: 0, 
[2025-10-26 11:42:48 DP2 TP2] Decode batch, #running-req: 7, #token: 2549, token usage: 0.00, cuda graph: True, gen throughput (token/s): 181.61, #queue-req: 0, 
[2025-10-26 11:42:48 DP1 TP1] Decode batch, #running-req: 10, #token: 3424, token usage: 0.01, cuda graph: True, gen throughput (token/s): 305.18, #queue-req: 0, 
[2025-10-26 11:42:48 DP7 TP7] Decode batch, #running-req: 12, #token: 3960, token usage: 0.01, cuda graph: True, gen throughput (token/s): 342.33, #queue-req: 0, 
[2025-10-26 11:42:48 DP4 TP4] Decode batch, #running-req: 13, #token: 4021, token usage: 0.01, cuda graph: True, gen throughput (token/s): 365.17, #queue-req: 0, 
[2025-10-26 11:42:48 DP3 TP3] Decode batch, #running-req: 6, #token: 2077, token usage: 0.00, cuda graph: True, gen throughput (token/s): 244.61, #queue-req: 0, 
[2025-10-26 11:42:48 DP0 TP0] Decode batch, #running-req: 8, #token: 2642, token usage: 0.00, cuda graph: True, gen throughput (token/s): 269.55, #queue-req: 0, 
[2025-10-26 11:42:48] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:58776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:54766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:48] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:47914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49 DP1 TP1] Decode batch, #running-req: 1, #token: 1007, token usage: 0.00, cuda graph: True, gen throughput (token/s): 88.70, #queue-req: 0, 
[2025-10-26 11:42:49 DP0 TP0] Decode batch, #running-req: 3, #token: 1631, token usage: 0.00, cuda graph: True, gen throughput (token/s): 98.11, #queue-req: 0, 
[2025-10-26 11:42:49 DP3 TP3] Decode batch, #running-req: 1, #token: 1000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 52.28, #queue-req: 0, 
[2025-10-26 11:42:49 DP5 TP5] Decode batch, #running-req: 2, #token: 1324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 95.72, #queue-req: 0, 
[2025-10-26 11:42:49 DP6 TP6] Decode batch, #running-req: 5, #token: 2219, token usage: 0.00, cuda graph: True, gen throughput (token/s): 149.18, #queue-req: 0, 
[2025-10-26 11:42:49 DP7 TP7] Decode batch, #running-req: 3, #token: 1540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 184.43, #queue-req: 0, 
[2025-10-26 11:42:49 DP2 TP2] Decode batch, #running-req: 5, #token: 2266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.51, #queue-req: 0, 
[2025-10-26 11:42:49 DP4 TP4] Decode batch, #running-req: 7, #token: 2649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 230.83, #queue-req: 0, 
[2025-10-26 11:42:49] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:48998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:49] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:54910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:50] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:51 DP4 TP4] Decode batch, #running-req: 1, #token: 1082, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.42, #queue-req: 0, 
[2025-10-26 11:42:51 DP0 TP0] Decode batch, #running-req: 1, #token: 1030, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.39, #queue-req: 0, 
[2025-10-26 11:42:51 DP3 TP3] Decode batch, #running-req: 1, #token: 1040, token usage: 0.00, cuda graph: True, gen throughput (token/s): 30.07, #queue-req: 0, 
[2025-10-26 11:42:51 DP6 TP6] Decode batch, #running-req: 2, #token: 1374, token usage: 0.00, cuda graph: True, gen throughput (token/s): 95.49, #queue-req: 0, 
[2025-10-26 11:42:51 DP2 TP2] Decode batch, #running-req: 1, #token: 1030, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.69, #queue-req: 0, 
[2025-10-26 11:42:51] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:51] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:51] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:51] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:52 DP6 TP6] Decode batch, #running-req: 1, #token: 1064, token usage: 0.00, cuda graph: True, gen throughput (token/s): 39.74, #queue-req: 0, 
[2025-10-26 11:42:52 DP3 TP3] Decode batch, #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 37.84, #queue-req: 0, 
[2025-10-26 11:42:52] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:42:53 DP6 TP6] Decode batch, #running-req: 1, #token: 1104, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.65, #queue-req: 0, 
[2025-10-26 11:42:54 DP6 TP6] Decode batch, #running-req: 1, #token: 1144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.63, #queue-req: 0, 
[2025-10-26 11:42:55] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:07] INFO:     127.0.0.1:37232 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-26 11:43:07 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:07 DP7 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:07 DP4 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:07 DP5 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:07 DP6 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:07 DP1 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:07 DP2 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:07 DP3 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:07 DP0 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08] INFO:     127.0.0.1:37240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:08 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:08 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1435, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:08 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 56, #cached-token: 1397, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:08 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1421, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:08 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 734, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:08 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 110, #cached-token: 1405, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:08 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1428, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:08 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1447, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:08 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 132, #cached-token: 3570, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08 DP0 TP0] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08 DP1 TP1] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08 DP7 TP7] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08 DP3 TP3] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08 DP5 TP5] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08 DP4 TP4] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08 DP2 TP2] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08 DP6 TP6] [fused_moe] using default for (307, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:08 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 368, #cached-token: 6159, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:08 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 404, #cached-token: 6087, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:08 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 462, #cached-token: 6811, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:08 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 388, #cached-token: 6829, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:08 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 489, #cached-token: 7669, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-26 11:43:08 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 185, #cached-token: 4121, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-26 11:43:08 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 450, #cached-token: 6120, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:08 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 313, #cached-token: 7021, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:08 DP4 TP4] Prefill batch, #new-seq: 15, #new-token: 615, #cached-token: 10344, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:08 DP2 TP2] Prefill batch, #new-seq: 15, #new-token: 1160, #cached-token: 10108, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:08 DP6 TP6] Prefill batch, #new-seq: 16, #new-token: 636, #cached-token: 10942, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-26 11:43:08 DP1 TP1] Prefill batch, #new-seq: 15, #new-token: 753, #cached-token: 10119, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:08 DP3 TP3] Prefill batch, #new-seq: 15, #new-token: 708, #cached-token: 10232, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:08 DP0 TP0] Prefill batch, #new-seq: 14, #new-token: 664, #cached-token: 9635, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:08 DP5 TP5] Prefill batch, #new-seq: 16, #new-token: 791, #cached-token: 10946, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-26 11:43:08 DP7 TP7] Prefill batch, #new-seq: 16, #new-token: 593, #cached-token: 10952, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-26 11:43:08 DP1 TP1] Prefill batch, #new-seq: 13, #new-token: 524, #cached-token: 8898, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-26 11:43:08 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 636, #cached-token: 8807, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-26 11:43:08 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 697, #cached-token: 8807, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:43:08 DP7 TP7] Prefill batch, #new-seq: 12, #new-token: 583, #cached-token: 8132, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-26 11:43:08 DP3 TP3] Prefill batch, #new-seq: 13, #new-token: 482, #cached-token: 8791, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-26 11:43:08 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 580, #cached-token: 8847, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-26 11:43:08 DP2 TP2] Prefill batch, #new-seq: 13, #new-token: 807, #cached-token: 8732, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-26 11:43:08 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 748, #cached-token: 8750, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-26 11:43:09 DP4 TP4] Prefill batch, #new-seq: 25, #new-token: 926, #cached-token: 17314, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-26 11:43:09 DP0 TP0] Prefill batch, #new-seq: 25, #new-token: 827, #cached-token: 17440, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-26 11:43:09 DP5 TP5] Prefill batch, #new-seq: 25, #new-token: 756, #cached-token: 17476, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-26 11:43:09 DP1 TP1] Prefill batch, #new-seq: 25, #new-token: 552, #cached-token: 17548, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-26 11:43:09 DP3 TP3] Prefill batch, #new-seq: 25, #new-token: 807, #cached-token: 17506, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-26 11:43:09 DP7 TP7] Prefill batch, #new-seq: 25, #new-token: 843, #cached-token: 17362, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-26 11:43:09 DP2 TP2] Prefill batch, #new-seq: 25, #new-token: 510, #cached-token: 17687, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-26 11:43:09 DP6 TP6] Prefill batch, #new-seq: 25, #new-token: 792, #cached-token: 17490, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-26 11:43:09 DP0 TP0] Prefill batch, #new-seq: 20, #new-token: 619, #cached-token: 13937, token usage: 0.01, #running-req: 64, #queue-req: 0, 
[2025-10-26 11:43:09 DP5 TP5] Prefill batch, #new-seq: 20, #new-token: 612, #cached-token: 14012, token usage: 0.01, #running-req: 65, #queue-req: 0, 
[2025-10-26 11:43:09 DP1 TP1] Prefill batch, #new-seq: 20, #new-token: 565, #cached-token: 14050, token usage: 0.01, #running-req: 65, #queue-req: 0, 
[2025-10-26 11:43:09 DP4 TP4] Prefill batch, #new-seq: 20, #new-token: 566, #cached-token: 13877, token usage: 0.01, #running-req: 65, #queue-req: 0, 
[2025-10-26 11:43:09 DP7 TP7] Prefill batch, #new-seq: 21, #new-token: 769, #cached-token: 14521, token usage: 0.01, #running-req: 64, #queue-req: 0, 
[2025-10-26 11:43:09 DP3 TP3] Prefill batch, #new-seq: 20, #new-token: 621, #cached-token: 13892, token usage: 0.01, #running-req: 65, #queue-req: 0, 
[2025-10-26 11:43:09 DP2 TP2] Prefill batch, #new-seq: 20, #new-token: 500, #cached-token: 14001, token usage: 0.01, #running-req: 65, #queue-req: 0, 
[2025-10-26 11:43:09 DP6 TP6] Prefill batch, #new-seq: 20, #new-token: 769, #cached-token: 13859, token usage: 0.01, #running-req: 65, #queue-req: 0, 
[2025-10-26 11:43:09 DP4 TP4] Prefill batch, #new-seq: 23, #new-token: 567, #cached-token: 16133, token usage: 0.01, #running-req: 85, #queue-req: 0, 
[2025-10-26 11:43:09 DP1 TP1] Prefill batch, #new-seq: 24, #new-token: 853, #cached-token: 16664, token usage: 0.01, #running-req: 85, #queue-req: 0, 
[2025-10-26 11:43:09 DP5 TP5] Prefill batch, #new-seq: 23, #new-token: 544, #cached-token: 16285, token usage: 0.01, #running-req: 85, #queue-req: 0, 
[2025-10-26 11:43:09 DP0 TP0] Prefill batch, #new-seq: 24, #new-token: 906, #cached-token: 16619, token usage: 0.01, #running-req: 84, #queue-req: 0, 
[2025-10-26 11:43:09 DP3 TP3] Prefill batch, #new-seq: 23, #new-token: 605, #cached-token: 16257, token usage: 0.01, #running-req: 85, #queue-req: 0, 
[2025-10-26 11:43:09 DP7 TP7] Prefill batch, #new-seq: 23, #new-token: 483, #cached-token: 16275, token usage: 0.01, #running-req: 85, #queue-req: 0, 
[2025-10-26 11:43:09 DP6 TP6] Prefill batch, #new-seq: 23, #new-token: 516, #cached-token: 16230, token usage: 0.01, #running-req: 85, #queue-req: 0, 
[2025-10-26 11:43:09 DP2 TP2] Prefill batch, #new-seq: 23, #new-token: 709, #cached-token: 16106, token usage: 0.01, #running-req: 85, #queue-req: 0, 
[2025-10-26 11:43:10 DP4 TP4] Prefill batch, #new-seq: 20, #new-token: 658, #cached-token: 13949, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:10 DP5 TP5] Prefill batch, #new-seq: 20, #new-token: 638, #cached-token: 13911, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:10 DP0 TP0] Prefill batch, #new-seq: 20, #new-token: 552, #cached-token: 14080, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:10 DP1 TP1] Prefill batch, #new-seq: 20, #new-token: 997, #cached-token: 13856, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:43:10 DP7 TP7] Prefill batch, #new-seq: 20, #new-token: 504, #cached-token: 14243, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:10 DP3 TP3] Prefill batch, #new-seq: 21, #new-token: 704, #cached-token: 14572, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:10 DP2 TP2] Prefill batch, #new-seq: 21, #new-token: 791, #cached-token: 14437, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:10 DP6 TP6] Prefill batch, #new-seq: 20, #new-token: 652, #cached-token: 14039, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:10 DP6 TP6] Prefill batch, #new-seq: 21, #new-token: 951, #cached-token: 14477, token usage: 0.01, #running-req: 128, #queue-req: 0, 
[2025-10-26 11:43:10 DP4 TP4] Prefill batch, #new-seq: 21, #new-token: 611, #cached-token: 14750, token usage: 0.01, #running-req: 128, #queue-req: 0, 
[2025-10-26 11:43:10 DP7 TP7] Prefill batch, #new-seq: 21, #new-token: 914, #cached-token: 14541, token usage: 0.01, #running-req: 128, #queue-req: 0, 
[2025-10-26 11:43:10 DP5 TP5] Prefill batch, #new-seq: 21, #new-token: 664, #cached-token: 14458, token usage: 0.01, #running-req: 128, #queue-req: 0, 
[2025-10-26 11:43:10 DP2 TP2] Prefill batch, #new-seq: 21, #new-token: 700, #cached-token: 14493, token usage: 0.01, #running-req: 129, #queue-req: 0, 
[2025-10-26 11:43:10 DP0 TP0] Prefill batch, #new-seq: 21, #new-token: 914, #cached-token: 14607, token usage: 0.01, #running-req: 128, #queue-req: 0, 
[2025-10-26 11:43:10 DP1 TP1] Prefill batch, #new-seq: 21, #new-token: 775, #cached-token: 14417, token usage: 0.01, #running-req: 129, #queue-req: 0, 
[2025-10-26 11:43:10 DP3 TP3] Prefill batch, #new-seq: 21, #new-token: 674, #cached-token: 14571, token usage: 0.01, #running-req: 129, #queue-req: 0, 
[2025-10-26 11:43:10 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 242, #cached-token: 4834, token usage: 0.01, #running-req: 149, #queue-req: 0, 
[2025-10-26 11:43:10 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 92, #cached-token: 4978, token usage: 0.01, #running-req: 149, #queue-req: 0, 
[2025-10-26 11:43:10 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 196, #cached-token: 4911, token usage: 0.01, #running-req: 149, #queue-req: 0, 
[2025-10-26 11:43:10 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 408, #cached-token: 4837, token usage: 0.01, #running-req: 150, #queue-req: 0, 
[2025-10-26 11:43:10 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 207, #cached-token: 5011, token usage: 0.02, #running-req: 149, #queue-req: 0, 
[2025-10-26 11:43:10 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 249, #cached-token: 4928, token usage: 0.01, #running-req: 149, #queue-req: 0, 
[2025-10-26 11:43:10 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 211, #cached-token: 4873, token usage: 0.01, #running-req: 150, #queue-req: 0, 
[2025-10-26 11:43:10 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5112, token usage: 0.01, #running-req: 150, #queue-req: 0, 
[2025-10-26 11:43:11 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 85, #cached-token: 6472, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-26 11:43:11 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 319, #cached-token: 5587, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-26 11:43:11 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6468, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-26 11:43:11 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 220, #cached-token: 6311, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-26 11:43:11 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 165, #cached-token: 5729, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-26 11:43:11 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 246, #cached-token: 5671, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-26 11:43:11 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 93, #cached-token: 5720, token usage: 0.02, #running-req: 157, #queue-req: 0, 
[2025-10-26 11:43:11 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 283, #cached-token: 6355, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-26 11:43:11 DP6 TP6] Decode batch, #running-req: 165, #token: 11128, token usage: 0.02, cuda graph: True, gen throughput (token/s): 21.25, #queue-req: 0, 
[2025-10-26 11:43:12 DP5 TP5] Decode batch, #running-req: 165, #token: 11748, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.20, #queue-req: 0, 
[2025-10-26 11:43:12 DP7 TP7] Decode batch, #running-req: 165, #token: 13230, token usage: 0.02, cuda graph: True, gen throughput (token/s): 102.08, #queue-req: 0, 
[2025-10-26 11:43:13 DP0 TP0] Decode batch, #running-req: 164, #token: 14652, token usage: 0.02, cuda graph: True, gen throughput (token/s): 146.15, #queue-req: 0, 
[2025-10-26 11:43:14 DP4 TP4] Decode batch, #running-req: 165, #token: 14178, token usage: 0.02, cuda graph: True, gen throughput (token/s): 166.21, #queue-req: 0, 
[2025-10-26 11:43:14] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:14] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:15 DP1 TP1] Decode batch, #running-req: 165, #token: 16021, token usage: 0.02, cuda graph: True, gen throughput (token/s): 208.69, #queue-req: 0, 
[2025-10-26 11:43:15 DP2 TP2] Decode batch, #running-req: 165, #token: 16687, token usage: 0.02, cuda graph: True, gen throughput (token/s): 237.21, #queue-req: 0, 
[2025-10-26 11:43:15] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:15] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:15 DP3 TP3] Decode batch, #running-req: 165, #token: 16923, token usage: 0.03, cuda graph: True, gen throughput (token/s): 270.54, #queue-req: 0, 
[2025-10-26 11:43:15] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:41850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:38260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:39106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:39240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16 DP6 TP6] Decode batch, #running-req: 165, #token: 17569, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1386.01, #queue-req: 0, 
[2025-10-26 11:43:16] INFO:     127.0.0.1:38636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:42850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:41610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:37850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:42050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:44924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:46100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:41484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:37664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:37774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:41660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:46528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:42162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:38582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:46020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:40328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:42362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:48898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:48682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:43094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:40440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16 DP5 TP5] Decode batch, #running-req: 157, #token: 17520, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1371.10, #queue-req: 0, 
[2025-10-26 11:43:16] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:40586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:37910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:45290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:37584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:43696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:45938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:38100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:16] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:44746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:39880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:48028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:37262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:37808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:41872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:47522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:41116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:41766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:41672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:42090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:45726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:39682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:42252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:42614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:41812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:46250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:42364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:41294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:42876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:40708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:39046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:48656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:47216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17 DP7 TP7] Decode batch, #running-req: 157, #token: 18924, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1362.25, #queue-req: 0, 
[2025-10-26 11:43:17] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:41888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:40620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:46076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:40512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:17] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:40098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:41518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:42000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:42678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:41940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18 DP0 TP0] Decode batch, #running-req: 146, #token: 18948, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1316.02, #queue-req: 0, 
[2025-10-26 11:43:18] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:37562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:37394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:42748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:42146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:42682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:37864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:37996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18 DP4 TP4] Decode batch, #running-req: 140, #token: 17513, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1305.55, #queue-req: 0, 
[2025-10-26 11:43:18] INFO:     127.0.0.1:38130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:42558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:43138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:44586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:41214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:40900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:41336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:38716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:18] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:37540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:37718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:37646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:37454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19 DP1 TP1] Decode batch, #running-req: 115, #token: 15706, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1260.19, #queue-req: 0, 
[2025-10-26 11:43:19] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:37896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:39438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:37700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:46276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:44262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:37472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:42952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:38748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:40368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:41722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:43732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:19] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20 DP2 TP2] Decode batch, #running-req: 114, #token: 16668, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1274.31, #queue-req: 0, 
[2025-10-26 11:43:20] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:46052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:40108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:45702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:40276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:45034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20 DP3 TP3] Decode batch, #running-req: 112, #token: 16212, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1249.80, #queue-req: 0, 
[2025-10-26 11:43:20] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:45998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20 DP6 TP6] Decode batch, #running-req: 102, #token: 15282, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1255.69, #queue-req: 0, 
[2025-10-26 11:43:20] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:45406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:40528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:43486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:40770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:45128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:39798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:40166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:37288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:40428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:20] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:44774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21 DP5 TP5] Decode batch, #running-req: 94, #token: 14381, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1160.14, #queue-req: 0, 
[2025-10-26 11:43:21] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:39738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:37524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:44896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:44952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:44500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:37272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:39960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:37784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:37494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:44912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21 DP7 TP7] Decode batch, #running-req: 82, #token: 14081, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1224.71, #queue-req: 0, 
[2025-10-26 11:43:21] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:37286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:44552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:40666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:44496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:37958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:42800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:21] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:39596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:39290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:39820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:42284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:37456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22 DP0 TP0] Decode batch, #running-req: 80, #token: 14661, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1172.58, #queue-req: 0, 
[2025-10-26 11:43:22] INFO:     127.0.0.1:47558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:39896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:39116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:39778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:42804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:37770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:42812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22 DP4 TP4] Decode batch, #running-req: 62, #token: 10171, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1001.94, #queue-req: 0, 
[2025-10-26 11:43:22] INFO:     127.0.0.1:38062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:37798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:47696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:42208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:37446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:38210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:39814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:42114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:44442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:41980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:22] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:45682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23 DP1 TP1] Decode batch, #running-req: 52, #token: 10057, token usage: 0.01, cuda graph: True, gen throughput (token/s): 892.93, #queue-req: 0, 
[2025-10-26 11:43:23] INFO:     127.0.0.1:44154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:44290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23 DP2 TP2] Decode batch, #running-req: 60, #token: 12015, token usage: 0.02, cuda graph: True, gen throughput (token/s): 987.28, #queue-req: 0, 
[2025-10-26 11:43:23] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:44678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:44744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:45520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23 DP3 TP3] Decode batch, #running-req: 53, #token: 10452, token usage: 0.02, cuda graph: True, gen throughput (token/s): 964.37, #queue-req: 0, 
[2025-10-26 11:43:23] INFO:     127.0.0.1:45414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:45964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:37710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:40178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:39754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:44824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:23] INFO:     127.0.0.1:42130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24 DP6 TP6] Decode batch, #running-req: 35, #token: 6920, token usage: 0.01, cuda graph: True, gen throughput (token/s): 777.39, #queue-req: 0, 
[2025-10-26 11:43:24] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:41626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:45856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:48358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:38886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:41380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:45898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:37662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:38614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:37316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24 DP5 TP5] Decode batch, #running-req: 34, #token: 7423, token usage: 0.01, cuda graph: True, gen throughput (token/s): 743.73, #queue-req: 0, 
[2025-10-26 11:43:24] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:46086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:42106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:37254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:38974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:37302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:41832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:42588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:46826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:38828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:37574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:38084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24 DP7 TP7] Decode batch, #running-req: 33, #token: 7333, token usage: 0.01, cuda graph: True, gen throughput (token/s): 688.09, #queue-req: 0, 
[2025-10-26 11:43:24] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:42016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:48806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:42668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:39126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:44332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:24] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:38954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:44388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:43294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:37430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:37952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25 DP0 TP0] Decode batch, #running-req: 34, #token: 8085, token usage: 0.01, cuda graph: True, gen throughput (token/s): 723.41, #queue-req: 0, 
[2025-10-26 11:43:25] INFO:     127.0.0.1:42412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:44562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:47206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:37818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:41310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25 DP4 TP4] Decode batch, #running-req: 18, #token: 3918, token usage: 0.01, cuda graph: True, gen throughput (token/s): 483.36, #queue-req: 0, 
[2025-10-26 11:43:25] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:44132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:44384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:40606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:41674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:37558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:38708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:45606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:45822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:42040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:42414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:40218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:47986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:38684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:42022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25 DP1 TP1] Decode batch, #running-req: 19, #token: 5076, token usage: 0.01, cuda graph: True, gen throughput (token/s): 505.40, #queue-req: 0, 
[2025-10-26 11:43:25] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:45574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:46496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:42328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:43362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:25] INFO:     127.0.0.1:39050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26 DP2 TP2] Decode batch, #running-req: 16, #token: 4440, token usage: 0.01, cuda graph: True, gen throughput (token/s): 524.02, #queue-req: 0, 
[2025-10-26 11:43:26] INFO:     127.0.0.1:47184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:47438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:44406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:40692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:38422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:37922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26 DP3 TP3] Decode batch, #running-req: 20, #token: 4925, token usage: 0.01, cuda graph: True, gen throughput (token/s): 591.14, #queue-req: 0, 
[2025-10-26 11:43:26] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:48810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26 DP6 TP6] Decode batch, #running-req: 11, #token: 3444, token usage: 0.01, cuda graph: True, gen throughput (token/s): 352.44, #queue-req: 0, 
[2025-10-26 11:43:26] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:40386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:43652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:46646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:40598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:45246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:42262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26 DP5 TP5] Decode batch, #running-req: 12, #token: 3345, token usage: 0.00, cuda graph: True, gen throughput (token/s): 357.73, #queue-req: 0, 
[2025-10-26 11:43:26] INFO:     127.0.0.1:47246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:38224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:37304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:43656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:43628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:37728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:39582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:37554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:41102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:26] INFO:     127.0.0.1:37354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27 DP7 TP7] Decode batch, #running-req: 11, #token: 3421, token usage: 0.01, cuda graph: True, gen throughput (token/s): 399.34, #queue-req: 0, 
[2025-10-26 11:43:27] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:38700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:48858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:45002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27 DP0 TP0] Decode batch, #running-req: 17, #token: 5105, token usage: 0.01, cuda graph: True, gen throughput (token/s): 450.93, #queue-req: 0, 
[2025-10-26 11:43:27] INFO:     127.0.0.1:38816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27 DP4 TP4] Decode batch, #running-req: 9, #token: 3024, token usage: 0.00, cuda graph: True, gen throughput (token/s): 222.99, #queue-req: 0, 
[2025-10-26 11:43:27] INFO:     127.0.0.1:40086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:46180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:37960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:37878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:38648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:38272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:47602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:42242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27 DP1 TP1] Decode batch, #running-req: 9, #token: 2789, token usage: 0.00, cuda graph: True, gen throughput (token/s): 244.73, #queue-req: 0, 
[2025-10-26 11:43:27] INFO:     127.0.0.1:42416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:40118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:38598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:47762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:41222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:27] INFO:     127.0.0.1:48736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28 DP2 TP2] Decode batch, #running-req: 9, #token: 3214, token usage: 0.00, cuda graph: True, gen throughput (token/s): 223.37, #queue-req: 0, 
[2025-10-26 11:43:28] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:47686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:43942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:45596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28 DP3 TP3] Decode batch, #running-req: 11, #token: 3764, token usage: 0.01, cuda graph: True, gen throughput (token/s): 306.79, #queue-req: 0, 
[2025-10-26 11:43:28] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:38040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28 DP6 TP6] Decode batch, #running-req: 6, #token: 2452, token usage: 0.00, cuda graph: True, gen throughput (token/s): 192.24, #queue-req: 0, 
[2025-10-26 11:43:28] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28 DP5 TP5] Decode batch, #running-req: 3, #token: 1558, token usage: 0.00, cuda graph: True, gen throughput (token/s): 117.59, #queue-req: 0, 
[2025-10-26 11:43:28] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:39962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:38662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:37370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28 DP7 TP7] Decode batch, #running-req: 5, #token: 1870, token usage: 0.00, cuda graph: True, gen throughput (token/s): 182.40, #queue-req: 0, 
[2025-10-26 11:43:28] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:45476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:38030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:28] INFO:     127.0.0.1:43492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29 DP0 TP0] Decode batch, #running-req: 5, #token: 2312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 232.75, #queue-req: 0, 
[2025-10-26 11:43:29] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:42398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:46088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:41614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29 DP4 TP4] Decode batch, #running-req: 3, #token: 1589, token usage: 0.00, cuda graph: True, gen throughput (token/s): 136.92, #queue-req: 0, 
[2025-10-26 11:43:29] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:47936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:45546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:44542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29 DP2 TP2] Decode batch, #running-req: 5, #token: 1897, token usage: 0.00, cuda graph: True, gen throughput (token/s): 144.34, #queue-req: 0, 
[2025-10-26 11:43:29] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29] INFO:     127.0.0.1:41814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:29 DP3 TP3] Decode batch, #running-req: 3, #token: 1314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 158.49, #queue-req: 0, 
[2025-10-26 11:43:29 DP6 TP6] Decode batch, #running-req: 3, #token: 1611, token usage: 0.00, cuda graph: True, gen throughput (token/s): 107.80, #queue-req: 0, 
[2025-10-26 11:43:30] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30] INFO:     127.0.0.1:40852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30] INFO:     127.0.0.1:39904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30 DP7 TP7] Decode batch, #running-req: 1, #token: 1010, token usage: 0.00, cuda graph: True, gen throughput (token/s): 57.25, #queue-req: 0, 
[2025-10-26 11:43:30] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30] INFO:     127.0.0.1:46306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30 DP0 TP0] Decode batch, #running-req: 1, #token: 1070, token usage: 0.00, cuda graph: True, gen throughput (token/s): 84.17, #queue-req: 0, 
[2025-10-26 11:43:30 DP4 TP4] Decode batch, #running-req: 2, #token: 1340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 59.78, #queue-req: 0, 
[2025-10-26 11:43:30] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30] INFO:     127.0.0.1:41176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:30 DP2 TP2] Decode batch, #running-req: 1, #token: 1033, token usage: 0.00, cuda graph: True, gen throughput (token/s): 82.33, #queue-req: 0, 
[2025-10-26 11:43:30] INFO:     127.0.0.1:44106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:31 DP3 TP3] Decode batch, #running-req: 1, #token: 1037, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.70, #queue-req: 0, 
[2025-10-26 11:43:31 DP6 TP6] Decode batch, #running-req: 1, #token: 1034, token usage: 0.00, cuda graph: True, gen throughput (token/s): 55.51, #queue-req: 0, 
[2025-10-26 11:43:31] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:31] INFO:     127.0.0.1:46312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:31 DP4 TP4] Decode batch, #running-req: 1, #token: 1036, token usage: 0.00, cuda graph: True, gen throughput (token/s): 46.50, #queue-req: 0, 
[2025-10-26 11:43:32 DP6 TP6] Decode batch, #running-req: 1, #token: 1074, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.48, #queue-req: 0, 
[2025-10-26 11:43:32 DP4 TP4] Decode batch, #running-req: 1, #token: 1076, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.60, #queue-req: 0, 
[2025-10-26 11:43:33 DP6 TP6] Decode batch, #running-req: 1, #token: 1114, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.61, #queue-req: 0, 
[2025-10-26 11:43:33 DP4 TP4] Decode batch, #running-req: 1, #token: 1116, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.62, #queue-req: 0, 
[2025-10-26 11:43:34 DP6 TP6] Decode batch, #running-req: 1, #token: 1154, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.62, #queue-req: 0, 
[2025-10-26 11:43:34 DP4 TP4] Decode batch, #running-req: 1, #token: 1156, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.60, #queue-req: 0, 
[2025-10-26 11:43:34] INFO:     127.0.0.1:42840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:34] INFO:     127.0.0.1:38998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:47] INFO:     127.0.0.1:49516 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-26 11:43:47 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:47] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:47 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:47 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1435, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:47 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 69, #cached-token: 1393, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:47 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1421, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:47 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 734, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:47 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1428, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:47 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 60, #cached-token: 1446, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:47 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1447, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:43:47 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 152, #cached-token: 3530, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:47 DP0 TP0] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:47 DP7 TP7] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:47 DP1 TP1] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:47 DP4 TP4] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:47 DP6 TP6] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:47 DP2 TP2] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:47 DP3 TP3] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:47 DP5 TP5] [fused_moe] using default for (290, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:47 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 204, #cached-token: 7155, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:47 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 214, #cached-token: 7070, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-26 11:43:47 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 336, #cached-token: 6183, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:47 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 542, #cached-token: 6808, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:47 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 435, #cached-token: 6870, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:47 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 291, #cached-token: 4076, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-26 11:43:47 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 265, #cached-token: 6898, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:47 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 398, #cached-token: 6161, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:43:48 DP2 TP2] Prefill batch, #new-seq: 14, #new-token: 608, #cached-token: 9711, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:48 DP5 TP5] Prefill batch, #new-seq: 14, #new-token: 512, #cached-token: 9685, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:48 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 408, #cached-token: 9731, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:48 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 783, #cached-token: 10214, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-26 11:43:48 DP6 TP6] Prefill batch, #new-seq: 15, #new-token: 539, #cached-token: 10491, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-26 11:43:48 DP1 TP1] Prefill batch, #new-seq: 14, #new-token: 566, #cached-token: 9632, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:48 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 659, #cached-token: 9550, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:43:48 DP7 TP7] Prefill batch, #new-seq: 15, #new-token: 780, #cached-token: 10222, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-26 11:43:48 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 430, #cached-token: 9095, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:43:48 DP2 TP2] Prefill batch, #new-seq: 14, #new-token: 613, #cached-token: 9594, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:43:48 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 422, #cached-token: 9079, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:43:48 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 243, #cached-token: 9147, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:43:48 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 455, #cached-token: 9083, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:43:48 DP1 TP1] Prefill batch, #new-seq: 14, #new-token: 476, #cached-token: 9593, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:43:48 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 527, #cached-token: 8853, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:43:48 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 562, #cached-token: 9576, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-26 11:43:48 DP2 TP2] Prefill batch, #new-seq: 20, #new-token: 355, #cached-token: 14189, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-26 11:43:48 DP6 TP6] Prefill batch, #new-seq: 20, #new-token: 309, #cached-token: 14233, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-26 11:43:48 DP1 TP1] Prefill batch, #new-seq: 20, #new-token: 485, #cached-token: 14082, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-26 11:43:48 DP4 TP4] Prefill batch, #new-seq: 21, #new-token: 187, #cached-token: 15219, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-26 11:43:48 DP5 TP5] Prefill batch, #new-seq: 20, #new-token: 337, #cached-token: 14208, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-26 11:43:48 DP0 TP0] Prefill batch, #new-seq: 20, #new-token: 322, #cached-token: 14340, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-26 11:43:48 DP3 TP3] Prefill batch, #new-seq: 20, #new-token: 296, #cached-token: 14371, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-26 11:43:48 DP7 TP7] Prefill batch, #new-seq: 20, #new-token: 545, #cached-token: 14081, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-26 11:43:48 DP4 TP4] Prefill batch, #new-seq: 15, #new-token: 408, #cached-token: 10428, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-26 11:43:48 DP5 TP5] Prefill batch, #new-seq: 16, #new-token: 555, #cached-token: 10960, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:43:48 DP6 TP6] Prefill batch, #new-seq: 16, #new-token: 802, #cached-token: 10792, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:43:48 DP0 TP0] Prefill batch, #new-seq: 16, #new-token: 657, #cached-token: 11032, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:43:48 DP2 TP2] Prefill batch, #new-seq: 16, #new-token: 338, #cached-token: 11343, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-26 11:43:48 DP3 TP3] Prefill batch, #new-seq: 16, #new-token: 555, #cached-token: 11030, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-26 11:43:48 DP1 TP1] Prefill batch, #new-seq: 16, #new-token: 587, #cached-token: 11080, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-26 11:43:48 DP7 TP7] Prefill batch, #new-seq: 16, #new-token: 440, #cached-token: 11276, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:43:49 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 154, #cached-token: 10020, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-26 11:43:49 DP0 TP0] Prefill batch, #new-seq: 14, #new-token: 69, #cached-token: 10061, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-26 11:43:49 DP2 TP2] Prefill batch, #new-seq: 13, #new-token: 221, #cached-token: 9212, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-26 11:43:49 DP6 TP6] Prefill batch, #new-seq: 14, #new-token: 169, #cached-token: 10047, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-26 11:43:49 DP5 TP5] Prefill batch, #new-seq: 14, #new-token: 218, #cached-token: 10040, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-26 11:43:49 DP1 TP1] Prefill batch, #new-seq: 14, #new-token: 148, #cached-token: 10049, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-26 11:43:49 DP7 TP7] Prefill batch, #new-seq: 14, #new-token: 150, #cached-token: 10045, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-26 11:43:49 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 221, #cached-token: 10011, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-26 11:43:49 DP4 TP4] Prefill batch, #new-seq: 20, #new-token: 260, #cached-token: 14245, token usage: 0.01, #running-req: 89, #queue-req: 0, 
[2025-10-26 11:43:49 DP0 TP0] Prefill batch, #new-seq: 19, #new-token: 404, #cached-token: 13430, token usage: 0.01, #running-req: 89, #queue-req: 0, 
[2025-10-26 11:43:49 DP6 TP6] Prefill batch, #new-seq: 19, #new-token: 105, #cached-token: 13735, token usage: 0.01, #running-req: 89, #queue-req: 0, 
[2025-10-26 11:43:49 DP2 TP2] Prefill batch, #new-seq: 20, #new-token: 466, #cached-token: 14157, token usage: 0.01, #running-req: 89, #queue-req: 0, 
[2025-10-26 11:43:49 DP5 TP5] Prefill batch, #new-seq: 20, #new-token: 302, #cached-token: 14435, token usage: 0.01, #running-req: 89, #queue-req: 0, 
[2025-10-26 11:43:49 DP1 TP1] Prefill batch, #new-seq: 19, #new-token: 651, #cached-token: 13202, token usage: 0.01, #running-req: 90, #queue-req: 0, 
[2025-10-26 11:43:49 DP3 TP3] Prefill batch, #new-seq: 19, #new-token: 261, #cached-token: 13653, token usage: 0.01, #running-req: 90, #queue-req: 0, 
[2025-10-26 11:43:49 DP7 TP7] Prefill batch, #new-seq: 19, #new-token: 391, #cached-token: 13489, token usage: 0.01, #running-req: 89, #queue-req: 0, 
[2025-10-26 11:43:49 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 106, #cached-token: 5692, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:43:49 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 70, #cached-token: 5653, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:43:49 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 183, #cached-token: 6397, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:49 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 174, #cached-token: 5641, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:49 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 138, #cached-token: 5716, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:43:49 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 163, #cached-token: 5715, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:43:49 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 135, #cached-token: 5660, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:43:49 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 152, #cached-token: 6432, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:43:49 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 291, #cached-token: 9195, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:43:49 DP2 TP2] Prefill batch, #new-seq: 14, #new-token: 530, #cached-token: 9713, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:43:49 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 346, #cached-token: 9916, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:43:49 DP0 TP0] Prefill batch, #new-seq: 14, #new-token: 613, #cached-token: 9694, token usage: 0.01, #running-req: 116, #queue-req: 0, 
[2025-10-26 11:43:49 DP5 TP5] Prefill batch, #new-seq: 14, #new-token: 603, #cached-token: 9792, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:43:49 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 476, #cached-token: 9146, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:43:49 DP1 TP1] Prefill batch, #new-seq: 14, #new-token: 434, #cached-token: 9981, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:43:49 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 409, #cached-token: 9785, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:43:49 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 99, #cached-token: 5667, token usage: 0.01, #running-req: 131, #queue-req: 0, 
[2025-10-26 11:43:49 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 224, #cached-token: 6284, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-26 11:43:49 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 40, #cached-token: 5687, token usage: 0.01, #running-req: 131, #queue-req: 0, 
[2025-10-26 11:43:49 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 84, #cached-token: 6585, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-26 11:43:49 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 44, #cached-token: 5719, token usage: 0.01, #running-req: 131, #queue-req: 0, 
[2025-10-26 11:43:49 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 239, #cached-token: 6321, token usage: 0.01, #running-req: 131, #queue-req: 0, 
[2025-10-26 11:43:49 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 64, #cached-token: 6404, token usage: 0.01, #running-req: 131, #queue-req: 0, 
[2025-10-26 11:43:49 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 167, #cached-token: 6576, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP4 TP4] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP6 TP6] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP1 TP1] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP0 TP0] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP5 TP5] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP3 TP3] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP7 TP7] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP2 TP2] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP4 TP4] Prefill batch, #new-seq: 16, #new-token: 114, #cached-token: 11456, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-10-26 11:43:50 DP5 TP5] Prefill batch, #new-seq: 16, #new-token: 166, #cached-token: 11404, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-10-26 11:43:50 DP1 TP1] Prefill batch, #new-seq: 15, #new-token: 300, #cached-token: 10666, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-10-26 11:43:50 DP2 TP2] Prefill batch, #new-seq: 16, #new-token: 402, #cached-token: 11379, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-10-26 11:43:50 DP6 TP6] Prefill batch, #new-seq: 15, #new-token: 359, #cached-token: 10574, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-10-26 11:43:50 DP7 TP7] Prefill batch, #new-seq: 16, #new-token: 333, #cached-token: 11459, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-10-26 11:43:50 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 331, #cached-token: 10643, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-10-26 11:43:50 DP3 TP3] Prefill batch, #new-seq: 15, #new-token: 349, #cached-token: 10571, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-10-26 11:43:50 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 51, #cached-token: 4962, token usage: 0.01, #running-req: 155, #queue-req: 0, 
[2025-10-26 11:43:50 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 63, #cached-token: 5054, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[2025-10-26 11:43:50 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 238, #cached-token: 4992, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[2025-10-26 11:43:50 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 70, #cached-token: 5121, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[2025-10-26 11:43:50 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 43, #cached-token: 5081, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[2025-10-26 11:43:50 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 126, #cached-token: 4219, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[2025-10-26 11:43:50 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 118, #cached-token: 4917, token usage: 0.01, #running-req: 155, #queue-req: 0, 
[2025-10-26 11:43:50 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5276, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP7 TP7] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP5 TP5] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP6 TP6] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP4 TP4] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP0 TP0] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP2 TP2] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP1 TP1] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP3 TP3] [fused_moe] using default for (716, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP4 TP4] Prefill batch, #new-seq: 3, #new-token: 43, #cached-token: 2122, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-26 11:43:50 DP5 TP5] Prefill batch, #new-seq: 3, #new-token: 35, #cached-token: 2128, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-26 11:43:50 DP6 TP6] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2892, token usage: 0.02, #running-req: 161, #queue-req: 0, 
[2025-10-26 11:43:50 DP2 TP2] Prefill batch, #new-seq: 3, #new-token: 69, #cached-token: 2089, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-26 11:43:50 DP1 TP1] Prefill batch, #new-seq: 3, #new-token: 233, #cached-token: 2008, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-26 11:43:50 DP0 TP0] Prefill batch, #new-seq: 3, #new-token: 63, #cached-token: 2125, token usage: 0.02, #running-req: 161, #queue-req: 0, 
[2025-10-26 11:43:50 DP3 TP3] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2243, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-26 11:43:50 DP7 TP7] Prefill batch, #new-seq: 4, #new-token: 64, #cached-token: 2874, token usage: 0.02, #running-req: 161, #queue-req: 0, 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP4 TP4] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP6 TP6] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP7 TP7] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP5 TP5] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP2 TP2] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP1 TP1] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP3 TP3] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:50 DP0 TP0] [fused_moe] using default for (514, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:43:51 DP6 TP6] Decode batch, #running-req: 165, #token: 12265, token usage: 0.02, cuda graph: True, gen throughput (token/s): 85.82, #queue-req: 0, 
[2025-10-26 11:43:51 DP5 TP5] Decode batch, #running-req: 165, #token: 12489, token usage: 0.02, cuda graph: True, gen throughput (token/s): 72.52, #queue-req: 0, 
[2025-10-26 11:43:52 DP1 TP1] Decode batch, #running-req: 165, #token: 12790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-10-26 11:43:52 DP0 TP0] Decode batch, #running-req: 164, #token: 12588, token usage: 0.02, cuda graph: True, gen throughput (token/s): 84.84, #queue-req: 0, 
[2025-10-26 11:43:52] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:53 DP3 TP3] Decode batch, #running-req: 165, #token: 14746, token usage: 0.02, cuda graph: True, gen throughput (token/s): 188.67, #queue-req: 0, 
[2025-10-26 11:43:54 DP7 TP7] Decode batch, #running-req: 165, #token: 16029, token usage: 0.02, cuda graph: True, gen throughput (token/s): 206.47, #queue-req: 0, 
[2025-10-26 11:43:54] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:54] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:54] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:54 DP4 TP4] Decode batch, #running-req: 165, #token: 15789, token usage: 0.02, cuda graph: True, gen throughput (token/s): 273.90, #queue-req: 0, 
[2025-10-26 11:43:54 DP2 TP2] Decode batch, #running-req: 165, #token: 16734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 246.60, #queue-req: 0, 
[2025-10-26 11:43:54] INFO:     127.0.0.1:32890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:54] INFO:     127.0.0.1:52424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:60640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:33008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:50026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:55] INFO:     127.0.0.1:59242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:60210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:60058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:59838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:55272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:52362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:49556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:51298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:51862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56 DP6 TP6] Decode batch, #running-req: 159, #token: 18066, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1372.36, #queue-req: 0, 
[2025-10-26 11:43:56] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56 DP5 TP5] Decode batch, #running-req: 156, #token: 18200, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1361.81, #queue-req: 0, 
[2025-10-26 11:43:56] INFO:     127.0.0.1:55106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56 DP1 TP1] Decode batch, #running-req: 151, #token: 17551, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1362.28, #queue-req: 0, 
[2025-10-26 11:43:56 DP0 TP0] Decode batch, #running-req: 153, #token: 18028, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1363.09, #queue-req: 0, 
[2025-10-26 11:43:56] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:60682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:59336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:56] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:53086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:53822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:50506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:49542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:58598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:49690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:55112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:57] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:33142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58 DP3 TP3] Decode batch, #running-req: 129, #token: 17342, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1321.06, #queue-req: 0, 
[2025-10-26 11:43:58] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58 DP7 TP7] Decode batch, #running-req: 128, #token: 17621, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1353.60, #queue-req: 0, 
[2025-10-26 11:43:58] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:59686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:49996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:58] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:59000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59 DP4 TP4] Decode batch, #running-req: 115, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1286.76, #queue-req: 0, 
[2025-10-26 11:43:59] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:60632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:32874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59 DP2 TP2] Decode batch, #running-req: 116, #token: 16875, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1349.51, #queue-req: 0, 
[2025-10-26 11:43:59] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:33240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:49782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:51322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:43:59] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:51876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:60482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:56758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:60670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:51984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:49766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00 DP6 TP6] Decode batch, #running-req: 90, #token: 14269, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1205.93, #queue-req: 0, 
[2025-10-26 11:44:00] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00 DP5 TP5] Decode batch, #running-req: 98, #token: 15338, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1286.17, #queue-req: 0, 
[2025-10-26 11:44:00] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00 DP0 TP0] Decode batch, #running-req: 83, #token: 13465, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1217.45, #queue-req: 0, 
[2025-10-26 11:44:00 DP1 TP1] Decode batch, #running-req: 93, #token: 15013, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1180.84, #queue-req: 0, 
[2025-10-26 11:44:00] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:32806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:33104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:00] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:32824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:59012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:49638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:49790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:33038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:50006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:49916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:52136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:01] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02 DP3 TP3] Decode batch, #running-req: 72, #token: 12855, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1066.97, #queue-req: 0, 
[2025-10-26 11:44:02] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:32910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02 DP7 TP7] Decode batch, #running-req: 58, #token: 10604, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1006.04, #queue-req: 0, 
[2025-10-26 11:44:02] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02 DP4 TP4] Decode batch, #running-req: 50, #token: 9848, token usage: 0.01, cuda graph: True, gen throughput (token/s): 884.67, #queue-req: 0, 
[2025-10-26 11:44:02] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02 DP2 TP2] Decode batch, #running-req: 55, #token: 10344, token usage: 0.02, cuda graph: True, gen throughput (token/s): 943.99, #queue-req: 0, 
[2025-10-26 11:44:02] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:52152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:02] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:32782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:60852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:32994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:60752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:50554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:54848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03 DP6 TP6] Decode batch, #running-req: 38, #token: 8184, token usage: 0.01, cuda graph: True, gen throughput (token/s): 767.26, #queue-req: 0, 
[2025-10-26 11:44:03] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03 DP5 TP5] Decode batch, #running-req: 30, #token: 5798, token usage: 0.01, cuda graph: True, gen throughput (token/s): 747.72, #queue-req: 0, 
[2025-10-26 11:44:03] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03 DP0 TP0] Decode batch, #running-req: 31, #token: 7185, token usage: 0.01, cuda graph: True, gen throughput (token/s): 713.90, #queue-req: 0, 
[2025-10-26 11:44:03 DP1 TP1] Decode batch, #running-req: 34, #token: 7940, token usage: 0.01, cuda graph: True, gen throughput (token/s): 769.19, #queue-req: 0, 
[2025-10-26 11:44:03] INFO:     127.0.0.1:60626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:03] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:54406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:59234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:33052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:59164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:59322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:60366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:53648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:59350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:58930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04 DP3 TP3] Decode batch, #running-req: 23, #token: 5405, token usage: 0.01, cuda graph: True, gen throughput (token/s): 658.77, #queue-req: 0, 
[2025-10-26 11:44:04] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:04] INFO:     127.0.0.1:51790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05 DP7 TP7] Decode batch, #running-req: 30, #token: 7752, token usage: 0.01, cuda graph: True, gen throughput (token/s): 602.25, #queue-req: 0, 
[2025-10-26 11:44:05] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05 DP4 TP4] Decode batch, #running-req: 16, #token: 4264, token usage: 0.01, cuda graph: True, gen throughput (token/s): 459.03, #queue-req: 0, 
[2025-10-26 11:44:05] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:51054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:49610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05 DP2 TP2] Decode batch, #running-req: 14, #token: 3601, token usage: 0.01, cuda graph: True, gen throughput (token/s): 475.77, #queue-req: 0, 
[2025-10-26 11:44:05] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:51612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:60194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:05] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06 DP6 TP6] Decode batch, #running-req: 15, #token: 4461, token usage: 0.01, cuda graph: True, gen throughput (token/s): 450.55, #queue-req: 0, 
[2025-10-26 11:44:06] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:51874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06 DP5 TP5] Decode batch, #running-req: 14, #token: 3893, token usage: 0.01, cuda graph: True, gen throughput (token/s): 331.15, #queue-req: 0, 
[2025-10-26 11:44:06] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06 DP1 TP1] Decode batch, #running-req: 16, #token: 4445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 385.89, #queue-req: 0, 
[2025-10-26 11:44:06 DP0 TP0] Decode batch, #running-req: 5, #token: 1922, token usage: 0.00, cuda graph: True, gen throughput (token/s): 284.14, #queue-req: 0, 
[2025-10-26 11:44:06] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:51442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06 DP3 TP3] Decode batch, #running-req: 8, #token: 2795, token usage: 0.00, cuda graph: True, gen throughput (token/s): 265.81, #queue-req: 0, 
[2025-10-26 11:44:06] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:06] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07 DP7 TP7] Decode batch, #running-req: 17, #token: 5490, token usage: 0.01, cuda graph: True, gen throughput (token/s): 470.54, #queue-req: 0, 
[2025-10-26 11:44:07] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:51976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07 DP4 TP4] Decode batch, #running-req: 5, #token: 2081, token usage: 0.00, cuda graph: True, gen throughput (token/s): 177.37, #queue-req: 0, 
[2025-10-26 11:44:07] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07 DP2 TP2] Decode batch, #running-req: 7, #token: 2738, token usage: 0.00, cuda graph: True, gen throughput (token/s): 195.60, #queue-req: 0, 
[2025-10-26 11:44:07] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:50676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:59630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:32772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:07 DP6 TP6] Decode batch, #running-req: 3, #token: 1648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 155.16, #queue-req: 0, 
[2025-10-26 11:44:08 DP5 TP5] Decode batch, #running-req: 2, #token: 1001, token usage: 0.00, cuda graph: True, gen throughput (token/s): 152.28, #queue-req: 0, 
[2025-10-26 11:44:08] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08 DP0 TP0] Decode batch, #running-req: 2, #token: 1232, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.45, #queue-req: 0, 
[2025-10-26 11:44:08 DP1 TP1] Decode batch, #running-req: 2, #token: 960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 205.97, #queue-req: 0, 
[2025-10-26 11:44:08] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:50282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:50352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08 DP3 TP3] Decode batch, #running-req: 3, #token: 1524, token usage: 0.00, cuda graph: True, gen throughput (token/s): 109.42, #queue-req: 0, 
[2025-10-26 11:44:08] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08 DP7 TP7] Decode batch, #running-req: 7, #token: 2650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 287.92, #queue-req: 0, 
[2025-10-26 11:44:08] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08 DP4 TP4] Decode batch, #running-req: 1, #token: 987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 85.53, #queue-req: 0, 
[2025-10-26 11:44:08] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:08 DP2 TP2] Decode batch, #running-req: 4, #token: 1968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.21, #queue-req: 0, 
[2025-10-26 11:44:09] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09 DP6 TP6] Decode batch, #running-req: 1, #token: 1009, token usage: 0.00, cuda graph: True, gen throughput (token/s): 47.31, #queue-req: 0, 
[2025-10-26 11:44:09] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09 DP1 TP1] Decode batch, #running-req: 1, #token: 999, token usage: 0.00, cuda graph: True, gen throughput (token/s): 28.59, #queue-req: 0, 
[2025-10-26 11:44:09] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:09 DP7 TP7] Decode batch, #running-req: 1, #token: 1080, token usage: 0.00, cuda graph: True, gen throughput (token/s): 91.48, #queue-req: 0, 
[2025-10-26 11:44:09] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:10] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:23] INFO:     127.0.0.1:51786 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-26 11:44:23 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:23] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:23 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:23 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1435, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:23 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1504, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:23 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1421, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:23 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 102, #cached-token: 670, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:23 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1428, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:23 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1460, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:23 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1447, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:23 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 115, #cached-token: 3484, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:23 DP0 TP0] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:23 DP1 TP1] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:23 DP7 TP7] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:23 DP5 TP5] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:23 DP6 TP6] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:23 DP4 TP4] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:23 DP3 TP3] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:23 DP2 TP2] [fused_moe] using default for (229, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:23 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 674, #cached-token: 6746, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:23 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 529, #cached-token: 6694, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:23 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 324, #cached-token: 6947, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:23 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 300, #cached-token: 6863, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:23 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 361, #cached-token: 7647, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-26 11:44:23 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 621, #cached-token: 6774, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:23 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 227, #cached-token: 4830, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-26 11:44:23 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 402, #cached-token: 6842, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:23 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 348, #cached-token: 5458, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:23 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 326, #cached-token: 5493, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:23 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 325, #cached-token: 5496, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:23 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 324, #cached-token: 4820, token usage: 0.00, #running-req: 13, #queue-req: 0, 
[2025-10-26 11:44:23 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 438, #cached-token: 4689, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:23 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 376, #cached-token: 5442, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:23 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 341, #cached-token: 5574, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:23 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 468, #cached-token: 5427, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:23 DP2 TP2] Prefill batch, #new-seq: 16, #new-token: 545, #cached-token: 11113, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:23 DP4 TP4] Prefill batch, #new-seq: 16, #new-token: 545, #cached-token: 11183, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:23 DP5 TP5] Prefill batch, #new-seq: 15, #new-token: 521, #cached-token: 10529, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:23 DP0 TP0] Prefill batch, #new-seq: 17, #new-token: 517, #cached-token: 11816, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-26 11:44:23 DP7 TP7] Prefill batch, #new-seq: 15, #new-token: 365, #cached-token: 10524, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:23 DP6 TP6] Prefill batch, #new-seq: 15, #new-token: 407, #cached-token: 10438, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:23 DP1 TP1] Prefill batch, #new-seq: 17, #new-token: 604, #cached-token: 11720, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:23 DP3 TP3] Prefill batch, #new-seq: 16, #new-token: 591, #cached-token: 11096, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:24 DP1 TP1] Prefill batch, #new-seq: 13, #new-token: 198, #cached-token: 9146, token usage: 0.00, #running-req: 37, #queue-req: 0, 
[2025-10-26 11:44:24 DP5 TP5] Prefill batch, #new-seq: 14, #new-token: 151, #cached-token: 10071, token usage: 0.01, #running-req: 35, #queue-req: 0, 
[2025-10-26 11:44:24 DP2 TP2] Prefill batch, #new-seq: 14, #new-token: 75, #cached-token: 10124, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[2025-10-26 11:44:24 DP6 TP6] Prefill batch, #new-seq: 14, #new-token: 196, #cached-token: 10056, token usage: 0.00, #running-req: 35, #queue-req: 0, 
[2025-10-26 11:44:24 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 239, #cached-token: 9276, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[2025-10-26 11:44:24 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 116, #cached-token: 10085, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[2025-10-26 11:44:24 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 131, #cached-token: 10111, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[2025-10-26 11:44:24 DP7 TP7] Prefill batch, #new-seq: 14, #new-token: 212, #cached-token: 9975, token usage: 0.00, #running-req: 35, #queue-req: 0, 
[2025-10-26 11:44:25 DP1 TP1] Prefill batch, #new-seq: 99, #new-token: 1923, #cached-token: 70290, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-26 11:44:25 DP4 TP4] Prefill batch, #new-seq: 98, #new-token: 1886, #cached-token: 69663, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-26 11:44:25 DP0 TP0] Prefill batch, #new-seq: 99, #new-token: 1383, #cached-token: 70854, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-26 11:44:25 DP5 TP5] Prefill batch, #new-seq: 99, #new-token: 2021, #cached-token: 70376, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-26 11:44:25 DP3 TP3] Prefill batch, #new-seq: 98, #new-token: 1701, #cached-token: 69623, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-26 11:44:25 DP6 TP6] Prefill batch, #new-seq: 99, #new-token: 1837, #cached-token: 70280, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-26 11:44:25 DP2 TP2] Prefill batch, #new-seq: 98, #new-token: 1673, #cached-token: 69817, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-26 11:44:25 DP7 TP7] Prefill batch, #new-seq: 99, #new-token: 1783, #cached-token: 70536, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-26 11:44:25 DP6 TP6] Prefill batch, #new-seq: 11, #new-token: 286, #cached-token: 7707, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-26 11:44:25 DP2 TP2] Prefill batch, #new-seq: 12, #new-token: 94, #cached-token: 8721, token usage: 0.02, #running-req: 148, #queue-req: 0, 
[2025-10-26 11:44:25 DP4 TP4] Prefill batch, #new-seq: 12, #new-token: 234, #cached-token: 8511, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-26 11:44:25 DP1 TP1] Prefill batch, #new-seq: 11, #new-token: 261, #cached-token: 7926, token usage: 0.01, #running-req: 149, #queue-req: 0, 
[2025-10-26 11:44:25 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 156, #cached-token: 8012, token usage: 0.02, #running-req: 148, #queue-req: 0, 
[2025-10-26 11:44:25 DP5 TP5] Prefill batch, #new-seq: 12, #new-token: 163, #cached-token: 8540, token usage: 0.02, #running-req: 148, #queue-req: 0, 
[2025-10-26 11:44:25 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 223, #cached-token: 8591, token usage: 0.02, #running-req: 148, #queue-req: 0, 
[2025-10-26 11:44:25 DP7 TP7] Prefill batch, #new-seq: 11, #new-token: 66, #cached-token: 8091, token usage: 0.02, #running-req: 148, #queue-req: 0, 
[2025-10-26 11:44:26 DP2 TP2] Prefill batch, #new-seq: 5, #new-token: 108, #cached-token: 3531, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:44:26 DP5 TP5] Prefill batch, #new-seq: 5, #new-token: 84, #cached-token: 3506, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:44:26 DP6 TP6] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4357, token usage: 0.02, #running-req: 159, #queue-req: 0, 
[2025-10-26 11:44:26 DP4 TP4] Prefill batch, #new-seq: 5, #new-token: 204, #cached-token: 3436, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:44:26 DP0 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3672, token usage: 0.02, #running-req: 159, #queue-req: 0, 
[2025-10-26 11:44:26 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 127, #cached-token: 3518, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:44:26 DP3 TP3] Prefill batch, #new-seq: 5, #new-token: 61, #cached-token: 3621, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:44:26 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4376, token usage: 0.02, #running-req: 159, #queue-req: 0, 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:26 DP0 TP0] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:26 DP5 TP5] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:26 DP4 TP4] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:26 DP3 TP3] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:26 DP1 TP1] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:26 DP6 TP6] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:26 DP7 TP7] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:26 DP2 TP2] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:27 DP2 TP2] Decode batch, #running-req: 165, #token: 11508, token usage: 0.02, cuda graph: True, gen throughput (token/s): 40.12, #queue-req: 0, 
[2025-10-26 11:44:27 DP3 TP3] Decode batch, #running-req: 165, #token: 12693, token usage: 0.02, cuda graph: True, gen throughput (token/s): 96.51, #queue-req: 0, 
[2025-10-26 11:44:28 DP0 TP0] Decode batch, #running-req: 164, #token: 13668, token usage: 0.02, cuda graph: True, gen throughput (token/s): 136.94, #queue-req: 0, 
[2025-10-26 11:44:28] INFO:     127.0.0.1:52262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:29 DP7 TP7] Decode batch, #running-req: 164, #token: 14462, token usage: 0.02, cuda graph: True, gen throughput (token/s): 189.55, #queue-req: 0, 
[2025-10-26 11:44:29] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:29 DP4 TP4] Decode batch, #running-req: 165, #token: 15191, token usage: 0.02, cuda graph: True, gen throughput (token/s): 213.71, #queue-req: 0, 
[2025-10-26 11:44:29 DP6 TP6] Decode batch, #running-req: 165, #token: 15012, token usage: 0.02, cuda graph: True, gen throughput (token/s): 219.24, #queue-req: 0, 
[2025-10-26 11:44:30] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:30 DP5 TP5] Decode batch, #running-req: 165, #token: 15899, token usage: 0.02, cuda graph: True, gen throughput (token/s): 230.08, #queue-req: 0, 
[2025-10-26 11:44:30 DP1 TP1] Decode batch, #running-req: 165, #token: 15746, token usage: 0.02, cuda graph: True, gen throughput (token/s): 246.37, #queue-req: 0, 
[2025-10-26 11:44:30] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:30] INFO:     127.0.0.1:35120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:52280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:52166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:32816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31 DP2 TP2] Decode batch, #running-req: 161, #token: 17420, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1382.83, #queue-req: 0, 
[2025-10-26 11:44:31] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:56750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:34310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:31] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:35134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:57026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:59942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:53822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:34510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:59176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:59476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32 DP3 TP3] Decode batch, #running-req: 151, #token: 17678, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1363.98, #queue-req: 0, 
[2025-10-26 11:44:32] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:33952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:54054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:34742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:60804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:33568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:52760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:33644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:32] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:52840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:33680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33 DP0 TP0] Decode batch, #running-req: 147, #token: 18224, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1342.62, #queue-req: 0, 
[2025-10-26 11:44:33] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:60366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33 DP7 TP7] Decode batch, #running-req: 147, #token: 18889, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1354.24, #queue-req: 0, 
[2025-10-26 11:44:33] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:32994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:33] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34 DP6 TP6] Decode batch, #running-req: 124, #token: 16531, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1338.00, #queue-req: 0, 
[2025-10-26 11:44:34 DP4 TP4] Decode batch, #running-req: 131, #token: 17874, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1332.07, #queue-req: 0, 
[2025-10-26 11:44:34] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34 DP5 TP5] Decode batch, #running-req: 121, #token: 16598, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1340.92, #queue-req: 0, 
[2025-10-26 11:44:34 DP1 TP1] Decode batch, #running-req: 127, #token: 16973, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1351.75, #queue-req: 0, 
[2025-10-26 11:44:34] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:59094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:33768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:34] INFO:     127.0.0.1:35480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:33748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:33372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:53430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:33866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:55222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:55142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:33814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:35] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36 DP2 TP2] Decode batch, #running-req: 97, #token: 14909, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1232.88, #queue-req: 0, 
[2025-10-26 11:44:36] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:32898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36 DP3 TP3] Decode batch, #running-req: 92, #token: 14608, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1213.33, #queue-req: 0, 
[2025-10-26 11:44:36] INFO:     127.0.0.1:52552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:36] INFO:     127.0.0.1:34424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:35068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:33020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:35146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37 DP0 TP0] Decode batch, #running-req: 74, #token: 12825, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1146.77, #queue-req: 0, 
[2025-10-26 11:44:37] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:52022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:33260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:35208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:35292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37 DP7 TP7] Decode batch, #running-req: 73, #token: 12930, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1158.90, #queue-req: 0, 
[2025-10-26 11:44:37] INFO:     127.0.0.1:34700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:33180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:33254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:37] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38 DP6 TP6] Decode batch, #running-req: 54, #token: 9695, token usage: 0.01, cuda graph: True, gen throughput (token/s): 914.26, #queue-req: 0, 
[2025-10-26 11:44:38 DP4 TP4] Decode batch, #running-req: 64, #token: 11655, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1045.83, #queue-req: 0, 
[2025-10-26 11:44:38] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:59528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:51942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38 DP5 TP5] Decode batch, #running-req: 60, #token: 10464, token usage: 0.02, cuda graph: True, gen throughput (token/s): 962.80, #queue-req: 0, 
[2025-10-26 11:44:38 DP1 TP1] Decode batch, #running-req: 60, #token: 11039, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1025.72, #queue-req: 0, 
[2025-10-26 11:44:38] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:35180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:32782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:52362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:32844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:52246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:33876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:38] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:34556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:54068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:33166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39 DP2 TP2] Decode batch, #running-req: 43, #token: 9370, token usage: 0.01, cuda graph: True, gen throughput (token/s): 818.48, #queue-req: 0, 
[2025-10-26 11:44:39] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:33104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:33606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:54664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:52298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:32952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39 DP3 TP3] Decode batch, #running-req: 35, #token: 7853, token usage: 0.01, cuda graph: True, gen throughput (token/s): 814.69, #queue-req: 0, 
[2025-10-26 11:44:39] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:34668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:60694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:39] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40 DP0 TP0] Decode batch, #running-req: 31, #token: 6824, token usage: 0.01, cuda graph: True, gen throughput (token/s): 648.21, #queue-req: 0, 
[2025-10-26 11:44:40] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:33240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40 DP7 TP7] Decode batch, #running-req: 26, #token: 6484, token usage: 0.01, cuda graph: True, gen throughput (token/s): 649.60, #queue-req: 0, 
[2025-10-26 11:44:40] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:33820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:52700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40 DP4 TP4] Decode batch, #running-req: 25, #token: 6137, token usage: 0.01, cuda graph: True, gen throughput (token/s): 631.91, #queue-req: 0, 
[2025-10-26 11:44:40 DP6 TP6] Decode batch, #running-req: 23, #token: 5714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 523.64, #queue-req: 0, 
[2025-10-26 11:44:40] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40 DP5 TP5] Decode batch, #running-req: 18, #token: 4696, token usage: 0.01, cuda graph: True, gen throughput (token/s): 492.73, #queue-req: 0, 
[2025-10-26 11:44:40 DP1 TP1] Decode batch, #running-req: 22, #token: 5704, token usage: 0.01, cuda graph: True, gen throughput (token/s): 598.34, #queue-req: 0, 
[2025-10-26 11:44:40] INFO:     127.0.0.1:34990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:35234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:40] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:54332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:34212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:60550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:34982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:33798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:52606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:60966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:35028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41 DP2 TP2] Decode batch, #running-req: 12, #token: 3516, token usage: 0.01, cuda graph: True, gen throughput (token/s): 423.22, #queue-req: 0, 
[2025-10-26 11:44:41] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:53144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:33698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:32870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41 DP3 TP3] Decode batch, #running-req: 9, #token: 2974, token usage: 0.00, cuda graph: True, gen throughput (token/s): 366.00, #queue-req: 0, 
[2025-10-26 11:44:41] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:53870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:41] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:52682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:34450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42 DP0 TP0] Decode batch, #running-req: 10, #token: 3319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 356.16, #queue-req: 0, 
[2025-10-26 11:44:42] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:52384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42 DP7 TP7] Decode batch, #running-req: 14, #token: 4023, token usage: 0.01, cuda graph: True, gen throughput (token/s): 403.46, #queue-req: 0, 
[2025-10-26 11:44:42] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:54084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42 DP4 TP4] Decode batch, #running-req: 8, #token: 2961, token usage: 0.00, cuda graph: True, gen throughput (token/s): 319.65, #queue-req: 0, 
[2025-10-26 11:44:42 DP6 TP6] Decode batch, #running-req: 10, #token: 3302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 300.80, #queue-req: 0, 
[2025-10-26 11:44:42] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42 DP1 TP1] Decode batch, #running-req: 9, #token: 2910, token usage: 0.00, cuda graph: True, gen throughput (token/s): 291.43, #queue-req: 0, 
[2025-10-26 11:44:42 DP5 TP5] Decode batch, #running-req: 4, #token: 1537, token usage: 0.00, cuda graph: True, gen throughput (token/s): 191.13, #queue-req: 0, 
[2025-10-26 11:44:42] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:42] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43 DP2 TP2] Decode batch, #running-req: 6, #token: 2546, token usage: 0.00, cuda graph: True, gen throughput (token/s): 210.05, #queue-req: 0, 
[2025-10-26 11:44:43] INFO:     127.0.0.1:34994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:34238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:56374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:34150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43 DP3 TP3] Decode batch, #running-req: 1, #token: 965, token usage: 0.00, cuda graph: True, gen throughput (token/s): 114.24, #queue-req: 0, 
[2025-10-26 11:44:43] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:59272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:33342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:43 DP0 TP0] Decode batch, #running-req: 2, #token: 1280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 158.49, #queue-req: 0, 
[2025-10-26 11:44:43] INFO:     127.0.0.1:34126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:34404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44 DP7 TP7] Decode batch, #running-req: 3, #token: 1620, token usage: 0.00, cuda graph: True, gen throughput (token/s): 201.00, #queue-req: 0, 
[2025-10-26 11:44:44] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44 DP6 TP6] Decode batch, #running-req: 4, #token: 1864, token usage: 0.00, cuda graph: True, gen throughput (token/s): 158.62, #queue-req: 0, 
[2025-10-26 11:44:44 DP4 TP4] Decode batch, #running-req: 5, #token: 2039, token usage: 0.00, cuda graph: True, gen throughput (token/s): 150.64, #queue-req: 0, 
[2025-10-26 11:44:44] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:60188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44 DP5 TP5] Decode batch, #running-req: 2, #token: 1304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.12, #queue-req: 0, 
[2025-10-26 11:44:44 DP1 TP1] Decode batch, #running-req: 5, #token: 2204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 142.21, #queue-req: 0, 
[2025-10-26 11:44:44] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:53398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:55788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:44 DP2 TP2] Decode batch, #running-req: 1, #token: 991, token usage: 0.00, cuda graph: True, gen throughput (token/s): 59.71, #queue-req: 0, 
[2025-10-26 11:44:44] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:45] INFO:     127.0.0.1:60682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:45 DP3 TP3] Decode batch, #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.68, #queue-req: 0, 
[2025-10-26 11:44:45] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:45] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:45] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:45] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:45] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:45 DP4 TP4] Decode batch, #running-req: 1, #token: 1077, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.74, #queue-req: 0, 
[2025-10-26 11:44:45 DP6 TP6] Decode batch, #running-req: 1, #token: 1013, token usage: 0.00, cuda graph: True, gen throughput (token/s): 95.72, #queue-req: 0, 
[2025-10-26 11:44:45 DP5 TP5] Decode batch, #running-req: 1, #token: 1023, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.34, #queue-req: 0, 
[2025-10-26 11:44:45 DP1 TP1] Decode batch, #running-req: 1, #token: 1029, token usage: 0.00, cuda graph: True, gen throughput (token/s): 78.29, #queue-req: 0, 
[2025-10-26 11:44:45] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:46] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:46] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:46] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:59] INFO:     127.0.0.1:44714 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-26 11:44:59 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:59] INFO:     127.0.0.1:44730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:44:59 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 27, #cached-token: 670, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP4 TP4] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP5 TP5] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP7 TP7] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP0 TP0] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP3 TP3] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP6 TP6] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP2 TP2] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP1 TP1] [fused_moe] using default for (27, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1435, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:59 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1451, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:59 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 123, #cached-token: 1340, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:59 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1513, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:59 DP0 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1494, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:59 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1428, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:59 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 58, #cached-token: 1388, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-26 11:44:59 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 189, #cached-token: 3500, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP0 TP0] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP5 TP5] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP1 TP1] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP7 TP7] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP4 TP4] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP3 TP3] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP6 TP6] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP2 TP2] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:44:59 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 307, #cached-token: 6985, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:59 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 408, #cached-token: 4848, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-26 11:44:59 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 288, #cached-token: 6924, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:59 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 406, #cached-token: 6830, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:59 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 166, #cached-token: 7114, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:59 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 333, #cached-token: 7019, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:59 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 343, #cached-token: 6881, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:59 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 195, #cached-token: 6964, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-26 11:44:59 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 324, #cached-token: 5531, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:59 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 303, #cached-token: 4791, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:59 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 292, #cached-token: 4770, token usage: 0.00, #running-req: 13, #queue-req: 0, 
[2025-10-26 11:44:59 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 481, #cached-token: 5408, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:59 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 321, #cached-token: 5550, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:59 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 331, #cached-token: 5575, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:59 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 239, #cached-token: 5626, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:59 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 389, #cached-token: 5478, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-26 11:44:59 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 337, #cached-token: 9111, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:59 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 216, #cached-token: 9238, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:59 DP0 TP0] Prefill batch, #new-seq: 14, #new-token: 403, #cached-token: 9822, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-26 11:44:59 DP1 TP1] Prefill batch, #new-seq: 14, #new-token: 337, #cached-token: 9805, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:59 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 320, #cached-token: 9222, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:59 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 443, #cached-token: 9855, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:59 DP2 TP2] Prefill batch, #new-seq: 14, #new-token: 297, #cached-token: 9793, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:44:59 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 205, #cached-token: 9276, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-26 11:45:00 DP1 TP1] Prefill batch, #new-seq: 13, #new-token: 748, #cached-token: 8820, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-26 11:45:00 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 540, #cached-token: 8859, token usage: 0.00, #running-req: 33, #queue-req: 0, 
[2025-10-26 11:45:00 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 614, #cached-token: 8976, token usage: 0.00, #running-req: 33, #queue-req: 0, 
[2025-10-26 11:45:00 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 379, #cached-token: 8378, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-26 11:45:00 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 530, #cached-token: 8871, token usage: 0.00, #running-req: 33, #queue-req: 0, 
[2025-10-26 11:45:00 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 681, #cached-token: 8798, token usage: 0.00, #running-req: 33, #queue-req: 0, 
[2025-10-26 11:45:00 DP2 TP2] Prefill batch, #new-seq: 12, #new-token: 361, #cached-token: 8286, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-26 11:45:00 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 655, #cached-token: 8799, token usage: 0.00, #running-req: 33, #queue-req: 0, 
[2025-10-26 11:45:00 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 504, #cached-token: 8856, token usage: 0.01, #running-req: 46, #queue-req: 0, 
[2025-10-26 11:45:00 DP1 TP1] Prefill batch, #new-seq: 12, #new-token: 479, #cached-token: 8254, token usage: 0.01, #running-req: 47, #queue-req: 0, 
[2025-10-26 11:45:00 DP0 TP0] Prefill batch, #new-seq: 12, #new-token: 349, #cached-token: 8425, token usage: 0.01, #running-req: 46, #queue-req: 0, 
[2025-10-26 11:45:00 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 333, #cached-token: 9091, token usage: 0.01, #running-req: 46, #queue-req: 0, 
[2025-10-26 11:45:00 DP2 TP2] Prefill batch, #new-seq: 13, #new-token: 574, #cached-token: 8983, token usage: 0.01, #running-req: 46, #queue-req: 0, 
[2025-10-26 11:45:00 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 435, #cached-token: 8999, token usage: 0.01, #running-req: 46, #queue-req: 0, 
[2025-10-26 11:45:00 DP3 TP3] Prefill batch, #new-seq: 13, #new-token: 605, #cached-token: 8973, token usage: 0.01, #running-req: 46, #queue-req: 0, 
[2025-10-26 11:45:00 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 443, #cached-token: 9062, token usage: 0.01, #running-req: 46, #queue-req: 0, 
[2025-10-26 11:45:00 DP4 TP4] Prefill batch, #new-seq: 18, #new-token: 674, #cached-token: 12498, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:45:00 DP2 TP2] Prefill batch, #new-seq: 18, #new-token: 621, #cached-token: 12535, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:45:00 DP6 TP6] Prefill batch, #new-seq: 18, #new-token: 450, #cached-token: 12580, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:45:00 DP5 TP5] Prefill batch, #new-seq: 18, #new-token: 517, #cached-token: 12631, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:45:00 DP0 TP0] Prefill batch, #new-seq: 19, #new-token: 697, #cached-token: 13278, token usage: 0.01, #running-req: 58, #queue-req: 0, 
[2025-10-26 11:45:00 DP1 TP1] Prefill batch, #new-seq: 19, #new-token: 379, #cached-token: 13337, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:45:00 DP3 TP3] Prefill batch, #new-seq: 18, #new-token: 453, #cached-token: 12622, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:45:00 DP7 TP7] Prefill batch, #new-seq: 18, #new-token: 287, #cached-token: 12723, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-26 11:45:00 DP4 TP4] Prefill batch, #new-seq: 16, #new-token: 158, #cached-token: 11587, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-26 11:45:00 DP6 TP6] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11665, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-26 11:45:00 DP2 TP2] Prefill batch, #new-seq: 17, #new-token: 157, #cached-token: 12236, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-26 11:45:00 DP5 TP5] Prefill batch, #new-seq: 16, #new-token: 48, #cached-token: 11474, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-26 11:45:00 DP7 TP7] Prefill batch, #new-seq: 16, #new-token: 96, #cached-token: 11557, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-26 11:45:00 DP1 TP1] Prefill batch, #new-seq: 16, #new-token: 111, #cached-token: 11560, token usage: 0.01, #running-req: 78, #queue-req: 0, 
[2025-10-26 11:45:00 DP3 TP3] Prefill batch, #new-seq: 17, #new-token: 177, #cached-token: 12234, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-26 11:45:00 DP0 TP0] Prefill batch, #new-seq: 16, #new-token: 253, #cached-token: 11379, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP5 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP4 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP7 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP2 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP6 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP0 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP3 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP1 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP4 TP4] Prefill batch, #new-seq: 16, #new-token: 349, #cached-token: 11413, token usage: 0.01, #running-req: 93, #queue-req: 0, 
[2025-10-26 11:45:01 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 129, #cached-token: 10792, token usage: 0.01, #running-req: 93, #queue-req: 0, 
[2025-10-26 11:45:01 DP2 TP2] Prefill batch, #new-seq: 15, #new-token: 101, #cached-token: 10803, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-26 11:45:01 DP1 TP1] Prefill batch, #new-seq: 15, #new-token: 134, #cached-token: 10779, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-26 11:45:01 DP5 TP5] Prefill batch, #new-seq: 16, #new-token: 138, #cached-token: 11560, token usage: 0.01, #running-req: 93, #queue-req: 0, 
[2025-10-26 11:45:01 DP6 TP6] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11689, token usage: 0.01, #running-req: 93, #queue-req: 0, 
[2025-10-26 11:45:01 DP3 TP3] Prefill batch, #new-seq: 15, #new-token: 119, #cached-token: 10792, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-26 11:45:01 DP7 TP7] Prefill batch, #new-seq: 16, #new-token: 80, #cached-token: 11622, token usage: 0.01, #running-req: 93, #queue-req: 0, 
[2025-10-26 11:45:01 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 121, #cached-token: 4982, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:45:01 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 48, #cached-token: 5011, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:45:01 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 93, #cached-token: 5670, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:45:01 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 70, #cached-token: 5104, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:45:01 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 164, #cached-token: 5067, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:45:01 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 83, #cached-token: 5683, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-26 11:45:01 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 79, #cached-token: 5738, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-26 11:45:01 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 50, #cached-token: 5707, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP4 TP4] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP2 TP2] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP0 TP0] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP3 TP3] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP1 TP1] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP7 TP7] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP6 TP6] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP5 TP5] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 240, #cached-token: 6349, token usage: 0.01, #running-req: 116, #queue-req: 0, 
[2025-10-26 11:45:01 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 90, #cached-token: 6514, token usage: 0.01, #running-req: 116, #queue-req: 0, 
[2025-10-26 11:45:01 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 177, #cached-token: 5929, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:45:01 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 91, #cached-token: 5745, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:45:01 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 171, #cached-token: 6310, token usage: 0.01, #running-req: 116, #queue-req: 0, 
[2025-10-26 11:45:01 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 187, #cached-token: 6464, token usage: 0.01, #running-req: 116, #queue-req: 0, 
[2025-10-26 11:45:01 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 146, #cached-token: 5683, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-26 11:45:01 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 139, #cached-token: 6491, token usage: 0.01, #running-req: 116, #queue-req: 0, 
[2025-10-26 11:45:01 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 285, #cached-token: 5637, token usage: 0.01, #running-req: 125, #queue-req: 0, 
[2025-10-26 11:45:01 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 339, #cached-token: 5510, token usage: 0.01, #running-req: 125, #queue-req: 0, 
[2025-10-26 11:45:01 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 128, #cached-token: 5665, token usage: 0.01, #running-req: 125, #queue-req: 0, 
[2025-10-26 11:45:01 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 277, #cached-token: 5517, token usage: 0.01, #running-req: 125, #queue-req: 0, 
[2025-10-26 11:45:01 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 71, #cached-token: 5033, token usage: 0.01, #running-req: 125, #queue-req: 0, 
[2025-10-26 11:45:01 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 317, #cached-token: 5580, token usage: 0.01, #running-req: 125, #queue-req: 0, 
[2025-10-26 11:45:01 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 240, #cached-token: 5658, token usage: 0.01, #running-req: 125, #queue-req: 0, 
[2025-10-26 11:45:01 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 203, #cached-token: 5653, token usage: 0.01, #running-req: 125, #queue-req: 0, 
[2025-10-26 11:45:01 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 177, #cached-token: 5620, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-26 11:45:01 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 77, #cached-token: 5627, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-26 11:45:01 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 177, #cached-token: 5654, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-26 11:45:01 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 473, #cached-token: 5610, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-10-26 11:45:01 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 36, #cached-token: 5702, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-26 11:45:01 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 63, #cached-token: 5723, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-26 11:45:01 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 221, #cached-token: 5737, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-26 11:45:01 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 74, #cached-token: 5722, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-26 11:45:01 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 45, #cached-token: 7195, token usage: 0.01, #running-req: 141, #queue-req: 0, 
[2025-10-26 11:45:01 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 123, #cached-token: 7124, token usage: 0.01, #running-req: 141, #queue-req: 0, 
[2025-10-26 11:45:01 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 65, #cached-token: 7211, token usage: 0.01, #running-req: 141, #queue-req: 0, 
[2025-10-26 11:45:01 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7252, token usage: 0.01, #running-req: 141, #queue-req: 0, 
[2025-10-26 11:45:01 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 60, #cached-token: 7219, token usage: 0.01, #running-req: 141, #queue-req: 0, 
[2025-10-26 11:45:01 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 78, #cached-token: 7307, token usage: 0.02, #running-req: 141, #queue-req: 0, 
[2025-10-26 11:45:01 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 97, #cached-token: 8009, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-10-26 11:45:01 DP1 TP1] Prefill batch, #new-seq: 11, #new-token: 168, #cached-token: 7862, token usage: 0.01, #running-req: 141, #queue-req: 0, 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP5 TP5] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP6 TP6] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP0 TP0] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP2 TP2] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP3 TP3] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP7 TP7] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP4 TP4] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP1 TP1] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 137, #cached-token: 6403, token usage: 0.02, #running-req: 151, #queue-req: 0, 
[2025-10-26 11:45:01 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 108, #cached-token: 6553, token usage: 0.01, #running-req: 151, #queue-req: 0, 
[2025-10-26 11:45:01 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 165, #cached-token: 6341, token usage: 0.01, #running-req: 151, #queue-req: 0, 
[2025-10-26 11:45:01 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 170, #cached-token: 5848, token usage: 0.02, #running-req: 152, #queue-req: 0, 
[2025-10-26 11:45:01 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 90, #cached-token: 5863, token usage: 0.02, #running-req: 151, #queue-req: 0, 
[2025-10-26 11:45:01 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 233, #cached-token: 5652, token usage: 0.02, #running-req: 151, #queue-req: 0, 
[2025-10-26 11:45:01 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6552, token usage: 0.02, #running-req: 151, #queue-req: 0, 
[2025-10-26 11:45:01 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 103, #cached-token: 6440, token usage: 0.01, #running-req: 151, #queue-req: 0, 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP2 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP7 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP4 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP6 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP5 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP0 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP1 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:01 DP3 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP4 TP4] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3580, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:45:02 DP6 TP6] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3631, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:45:02 DP2 TP2] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3618, token usage: 0.01, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:45:02 DP0 TP0] Prefill batch, #new-seq: 5, #new-token: 41, #cached-token: 3581, token usage: 0.02, #running-req: 159, #queue-req: 0, 
[2025-10-26 11:45:02 DP5 TP5] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3640, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:45:02 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3656, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:45:02 DP3 TP3] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3677, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-26 11:45:02 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4414, token usage: 0.02, #running-req: 159, #queue-req: 0, 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP5 TP5] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP4 TP4] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP2 TP2] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP7 TP7] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP3 TP3] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP6 TP6] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP1 TP1] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP0 TP0] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-26 11:45:02 DP5 TP5] Decode batch, #running-req: 165, #token: 11539, token usage: 0.02, cuda graph: True, gen throughput (token/s): 59.72, #queue-req: 0, 
[2025-10-26 11:45:02 DP7 TP7] Decode batch, #running-req: 165, #token: 12393, token usage: 0.02, cuda graph: True, gen throughput (token/s): 55.51, #queue-req: 0, 
[2025-10-26 11:45:04 DP6 TP6] Decode batch, #running-req: 165, #token: 12954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 144.51, #queue-req: 0, 
[2025-10-26 11:45:04 DP4 TP4] Decode batch, #running-req: 165, #token: 13792, token usage: 0.02, cuda graph: True, gen throughput (token/s): 160.23, #queue-req: 0, 
[2025-10-26 11:45:04 DP0 TP0] Decode batch, #running-req: 164, #token: 14420, token usage: 0.02, cuda graph: True, gen throughput (token/s): 167.56, #queue-req: 0, 
[2025-10-26 11:45:05] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:05] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06 DP2 TP2] Decode batch, #running-req: 165, #token: 16054, token usage: 0.02, cuda graph: True, gen throughput (token/s): 270.24, #queue-req: 0, 
[2025-10-26 11:45:06] INFO:     127.0.0.1:55926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06 DP1 TP1] Decode batch, #running-req: 165, #token: 17368, token usage: 0.03, cuda graph: True, gen throughput (token/s): 299.29, #queue-req: 0, 
[2025-10-26 11:45:06 DP3 TP3] Decode batch, #running-req: 164, #token: 17110, token usage: 0.03, cuda graph: True, gen throughput (token/s): 296.78, #queue-req: 0, 
[2025-10-26 11:45:06] INFO:     127.0.0.1:49170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:49344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:45722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:06] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:48684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:47328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:49036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:49676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:47772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:46168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07 DP5 TP5] Decode batch, #running-req: 159, #token: 17603, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1377.67, #queue-req: 0, 
[2025-10-26 11:45:07 DP7 TP7] Decode batch, #running-req: 160, #token: 18520, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1377.05, #queue-req: 0, 
[2025-10-26 11:45:07] INFO:     127.0.0.1:55954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:54798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:51234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:44746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:46496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:45914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:07] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:45676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:49256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:45898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:49220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:52538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:45576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:45618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:45710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:53870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:53008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:47130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:49486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:51666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:52866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08 DP6 TP6] Decode batch, #running-req: 152, #token: 17768, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1341.66, #queue-req: 0, 
[2025-10-26 11:45:08] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:52390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:50628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:08] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09 DP4 TP4] Decode batch, #running-req: 145, #token: 18152, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1335.27, #queue-req: 0, 
[2025-10-26 11:45:09] INFO:     127.0.0.1:53846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:56204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09 DP0 TP0] Decode batch, #running-req: 142, #token: 18508, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1324.75, #queue-req: 0, 
[2025-10-26 11:45:09] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:44884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:44924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:47204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:50448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:09] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:44998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:45462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:44934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10 DP2 TP2] Decode batch, #running-req: 111, #token: 15493, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1288.57, #queue-req: 0, 
[2025-10-26 11:45:10] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:50488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:47284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:10] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11 DP1 TP1] Decode batch, #running-req: 112, #token: 16967, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1281.16, #queue-req: 0, 
[2025-10-26 11:45:11] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11 DP3 TP3] Decode batch, #running-req: 102, #token: 15348, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1231.94, #queue-req: 0, 
[2025-10-26 11:45:11] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:47880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:50738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:50834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:48192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:48122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:44826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11 DP5 TP5] Decode batch, #running-req: 91, #token: 13955, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1234.04, #queue-req: 0, 
[2025-10-26 11:45:11] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11 DP7 TP7] Decode batch, #running-req: 102, #token: 15944, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1282.49, #queue-req: 0, 
[2025-10-26 11:45:11] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:52250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:11] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:46250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:44768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:44782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12 DP6 TP6] Decode batch, #running-req: 73, #token: 11923, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1150.08, #queue-req: 0, 
[2025-10-26 11:45:12] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12 DP4 TP4] Decode batch, #running-req: 80, #token: 13606, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1117.18, #queue-req: 0, 
[2025-10-26 11:45:12] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:44926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:12] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:48204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13 DP0 TP0] Decode batch, #running-req: 77, #token: 12664, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1166.22, #queue-req: 0, 
[2025-10-26 11:45:13] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:48450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:46230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:48014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:55996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:48802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:53678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:13] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:44900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14 DP2 TP2] Decode batch, #running-req: 51, #token: 9762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 894.66, #queue-req: 0, 
[2025-10-26 11:45:14] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14 DP1 TP1] Decode batch, #running-req: 58, #token: 11539, token usage: 0.02, cuda graph: True, gen throughput (token/s): 944.86, #queue-req: 0, 
[2025-10-26 11:45:14] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14 DP3 TP3] Decode batch, #running-req: 51, #token: 9921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 879.72, #queue-req: 0, 
[2025-10-26 11:45:14] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:45414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:53926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:49766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:14] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:44834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:51984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:45644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:48418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15 DP5 TP5] Decode batch, #running-req: 27, #token: 5585, token usage: 0.01, cuda graph: True, gen throughput (token/s): 693.05, #queue-req: 0, 
[2025-10-26 11:45:15] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15 DP7 TP7] Decode batch, #running-req: 41, #token: 8318, token usage: 0.01, cuda graph: True, gen throughput (token/s): 866.19, #queue-req: 0, 
[2025-10-26 11:45:15] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:48744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:49286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:44800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:46216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:49906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15 DP6 TP6] Decode batch, #running-req: 28, #token: 6213, token usage: 0.01, cuda graph: True, gen throughput (token/s): 591.61, #queue-req: 0, 
[2025-10-26 11:45:15] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15 DP4 TP4] Decode batch, #running-req: 28, #token: 5809, token usage: 0.01, cuda graph: True, gen throughput (token/s): 688.25, #queue-req: 0, 
[2025-10-26 11:45:15] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:15] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16 DP0 TP0] Decode batch, #running-req: 22, #token: 5792, token usage: 0.01, cuda graph: True, gen throughput (token/s): 586.83, #queue-req: 0, 
[2025-10-26 11:45:16] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:49500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:55048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:44908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:48712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:46392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:46538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:49134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:53872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:49194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:49056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:48470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:46030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:46300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:46092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:47736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:53086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:46222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:46572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:50506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16 DP2 TP2] Decode batch, #running-req: 16, #token: 4465, token usage: 0.01, cuda graph: True, gen throughput (token/s): 438.85, #queue-req: 0, 
[2025-10-26 11:45:16] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:16] INFO:     127.0.0.1:48212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:48892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17 DP1 TP1] Decode batch, #running-req: 25, #token: 6628, token usage: 0.01, cuda graph: True, gen throughput (token/s): 633.11, #queue-req: 0, 
[2025-10-26 11:45:17] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:48178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:44992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:51902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17 DP3 TP3] Decode batch, #running-req: 17, #token: 4949, token usage: 0.01, cuda graph: True, gen throughput (token/s): 479.07, #queue-req: 0, 
[2025-10-26 11:45:17] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:51862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:45480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17 DP5 TP5] Decode batch, #running-req: 10, #token: 2333, token usage: 0.00, cuda graph: True, gen throughput (token/s): 270.93, #queue-req: 0, 
[2025-10-26 11:45:17 DP7 TP7] Decode batch, #running-req: 21, #token: 5848, token usage: 0.01, cuda graph: True, gen throughput (token/s): 491.82, #queue-req: 0, 
[2025-10-26 11:45:17] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:45346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:49252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:45376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:17] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18 DP6 TP6] Decode batch, #running-req: 10, #token: 3146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 310.77, #queue-req: 0, 
[2025-10-26 11:45:18] INFO:     127.0.0.1:52456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18 DP4 TP4] Decode batch, #running-req: 9, #token: 3084, token usage: 0.00, cuda graph: True, gen throughput (token/s): 294.81, #queue-req: 0, 
[2025-10-26 11:45:18] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18 DP0 TP0] Decode batch, #running-req: 10, #token: 3544, token usage: 0.01, cuda graph: True, gen throughput (token/s): 291.04, #queue-req: 0, 
[2025-10-26 11:45:18] INFO:     127.0.0.1:44814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:46010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:46584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:47194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:47008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:45538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18 DP2 TP2] Decode batch, #running-req: 7, #token: 2367, token usage: 0.00, cuda graph: True, gen throughput (token/s): 240.93, #queue-req: 0, 
[2025-10-26 11:45:18] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:53714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:18] INFO:     127.0.0.1:46306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19 DP1 TP1] Decode batch, #running-req: 7, #token: 2046, token usage: 0.00, cuda graph: True, gen throughput (token/s): 293.54, #queue-req: 0, 
[2025-10-26 11:45:19] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19 DP3 TP3] Decode batch, #running-req: 11, #token: 3946, token usage: 0.01, cuda graph: True, gen throughput (token/s): 309.65, #queue-req: 0, 
[2025-10-26 11:45:19] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:45974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19 DP5 TP5] Decode batch, #running-req: 2, #token: 1201, token usage: 0.00, cuda graph: True, gen throughput (token/s): 85.80, #queue-req: 0, 
[2025-10-26 11:45:19 DP7 TP7] Decode batch, #running-req: 9, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 306.06, #queue-req: 0, 
[2025-10-26 11:45:19] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:46066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:45730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19 DP6 TP6] Decode batch, #running-req: 2, #token: 1291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 128.78, #queue-req: 0, 
[2025-10-26 11:45:19] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19 DP4 TP4] Decode batch, #running-req: 4, #token: 1570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 140.23, #queue-req: 0, 
[2025-10-26 11:45:19] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:19 DP0 TP0] Decode batch, #running-req: 4, #token: 1691, token usage: 0.00, cuda graph: True, gen throughput (token/s): 172.19, #queue-req: 0, 
[2025-10-26 11:45:19] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:46052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20 DP2 TP2] Decode batch, #running-req: 2, #token: 1290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 90.33, #queue-req: 0, 
[2025-10-26 11:45:20] INFO:     127.0.0.1:45488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:45266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20 DP1 TP1] Decode batch, #running-req: 1, #token: 996, token usage: 0.00, cuda graph: True, gen throughput (token/s): 60.57, #queue-req: 0, 
[2025-10-26 11:45:20] INFO:     127.0.0.1:46260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20 DP3 TP3] Decode batch, #running-req: 6, #token: 2383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 184.41, #queue-req: 0, 
[2025-10-26 11:45:20] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20] INFO:     127.0.0.1:47630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:20 DP7 TP7] Decode batch, #running-req: 2, #token: 1340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 129.38, #queue-req: 0, 
[2025-10-26 11:45:21] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:21] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:21] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:21] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:21 DP4 TP4] Decode batch, #running-req: 1, #token: 1048, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.15, #queue-req: 0, 
[2025-10-26 11:45:21] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:21] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:21 DP0 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 47.90, #queue-req: 0, 
[2025-10-26 11:45:21] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:21] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:21 DP3 TP3] Decode batch, #running-req: 2, #token: 1467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 87.95, #queue-req: 0, 
[2025-10-26 11:45:22] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:22] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-26 11:45:27] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-26 11:45:30] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
