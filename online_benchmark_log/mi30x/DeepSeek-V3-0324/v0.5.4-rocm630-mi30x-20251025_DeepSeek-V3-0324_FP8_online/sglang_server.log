INFO 10-25 15:30:41 __init__.py:179] Automatically detected platform rocm.
WARNING 10-25 15:30:41 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-25 15:30:42] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-25 15:30:43] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.9, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', elastic_ep_backend=None, mooncake_ib_device=None, tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=60458020, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, rl_on_policy_target=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-25 15:30:43] Using default HuggingFace chat template with detected content format: string
INFO 10-25 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-25 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-25 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-25 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-25 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-25 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-25 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-25 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-25 15:30:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-25 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-25 15:30:53 TP6] Process 293 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-25 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-25 15:30:53 TP4] Process 291 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-10-25 15:30:53 TP6] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-25 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-25 15:30:53 TP0] Process 287 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-25 15:30:54 TP4] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-25 15:30:54] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-25 15:30:54 TP0] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-25 15:30:54] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-25 15:30:54 TP7] Process 294 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-25 15:30:54] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-25 15:30:54] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-25 15:30:54] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-25 15:30:54 TP3] Process 290 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-10-25 15:30:54 TP5] Process 292 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-25 15:30:54] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-25 15:30:54 TP1] Process 288 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-25 15:30:54 TP2] Process 289 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-25 15:30:54 TP7] Init torch distributed begin.
[2025-10-25 15:30:55 TP5] Init torch distributed begin.
[2025-10-25 15:30:55 TP3] Init torch distributed begin.
[2025-10-25 15:30:55 TP1] Init torch distributed begin.
[2025-10-25 15:30:55 TP2] Init torch distributed begin.
[2025-10-25 15:30:55 TP0] sglang is using nccl==2.21.5
[2025-10-25 15:30:57 TP7] Init torch distributed ends. mem usage=3.92 GB
[2025-10-25 15:30:57 TP5] Init torch distributed ends. mem usage=3.91 GB
[2025-10-25 15:30:57 TP6] Init torch distributed ends. mem usage=3.93 GB
[2025-10-25 15:30:57 TP0] Init torch distributed ends. mem usage=3.63 GB
[2025-10-25 15:30:57 TP4] Init torch distributed ends. mem usage=3.99 GB
[2025-10-25 15:30:57 TP2] Init torch distributed ends. mem usage=4.05 GB
[2025-10-25 15:30:57 TP3] Init torch distributed ends. mem usage=4.04 GB
[2025-10-25 15:30:57 TP1] Init torch distributed ends. mem usage=4.05 GB
[2025-10-25 15:30:58 TP7] Load weight begin. avail mem=187.34 GB
[2025-10-25 15:30:58 TP5] Load weight begin. avail mem=187.35 GB
[2025-10-25 15:30:58 TP2] Load weight begin. avail mem=187.21 GB
[2025-10-25 15:30:58 TP6] Load weight begin. avail mem=187.33 GB
[2025-10-25 15:30:58 TP4] Load weight begin. avail mem=187.27 GB
[2025-10-25 15:30:58 TP0] Load weight begin. avail mem=187.63 GB
[2025-10-25 15:30:58 TP0] Detected fp8 checkpoint.
[2025-10-25 15:30:58 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-10-25 15:30:58 TP3] Load weight begin. avail mem=187.22 GB
[2025-10-25 15:30:58 TP1] Load weight begin. avail mem=187.21 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:41,  3.86it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:17,  9.37it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:12, 12.39it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:00<00:09, 16.54it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:21,  7.03it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:01<00:17,  8.57it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:01<00:14, 10.16it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:01<00:11, 12.49it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:01<00:10, 13.01it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:02<00:11, 11.93it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:02<00:12, 11.05it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:02<00:12, 11.09it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:02<00:09, 14.01it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:02<00:07, 17.93it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:02<00:09, 13.97it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:03<00:17,  7.04it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:03<00:14,  8.44it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:03<00:12,  9.94it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:04<00:11, 10.08it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:04<00:10, 11.10it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:04<00:08, 14.09it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:04<00:09, 11.62it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:04<00:10, 10.73it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:04<00:08, 13.26it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:05<00:06, 15.15it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:05<00:07, 14.03it/s]
Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:05<00:05, 16.95it/s]
Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:05<00:05, 17.66it/s]
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:06<00:09, 10.07it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:06<00:11,  7.96it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:06<00:10,  8.87it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:06<00:08, 10.16it/s]
Loading safetensors checkpoint shards:  47% Completed | 77/163 [00:06<00:08, 10.48it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:07<00:07, 11.67it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:07<00:05, 13.93it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:07<00:05, 15.04it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:07<00:04, 16.04it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:07<00:04, 15.00it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:07<00:04, 16.58it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:08<00:04, 14.27it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:08<00:04, 14.90it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:08<00:04, 15.40it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:08<00:05, 10.88it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:08<00:04, 13.45it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:08<00:04, 13.70it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:09<00:08,  6.47it/s]
Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:09<00:05,  8.76it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:10<00:04, 10.05it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:10<00:03, 12.32it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:10<00:03, 13.07it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:10<00:03, 12.68it/s]
Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:10<00:02, 15.18it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:10<00:02, 16.13it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:10<00:02, 14.92it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:11<00:02, 13.80it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:11<00:02, 13.17it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:11<00:02, 11.13it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:11<00:01, 13.89it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:11<00:01, 15.18it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:11<00:01, 16.14it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:12<00:01, 15.05it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:12<00:00, 16.14it/s]
Loading safetensors checkpoint shards:  92% Completed | 150/163 [00:12<00:00, 17.42it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:12<00:00, 18.92it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:12<00:00, 18.61it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:12<00:00, 13.77it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:13<00:00,  6.06it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:13<00:00,  7.40it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00, 11.78it/s]

[2025-10-25 15:31:59 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.70 GB, mem usage=79.56 GB.
[2025-10-25 15:31:59 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-10-25 15:31:59 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.79 GB, mem usage=79.56 GB.
[2025-10-25 15:31:59 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-10-25 15:32:02 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.06 GB, mem usage=79.56 GB.
[2025-10-25 15:32:02 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-10-25 15:32:02 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-25 15:32:04 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-25 15:32:04 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-25 15:32:05 TP7] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-25 15:32:05 TP6] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-25 15:32:05 TP7] Memory pool end. avail mem=43.53 GB
[2025-10-25 15:32:05 TP6] Memory pool end. avail mem=43.52 GB
[2025-10-25 15:32:05 TP5] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-25 15:32:05 TP5] Memory pool end. avail mem=43.54 GB
[2025-10-25 15:32:05 TP4] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-25 15:32:05 TP4] Memory pool end. avail mem=43.45 GB
[2025-10-25 15:32:05 TP1] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-25 15:32:05 TP1] Memory pool end. avail mem=43.40 GB
[2025-10-25 15:32:05 TP0] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-25 15:32:05 TP0] Memory pool end. avail mem=43.81 GB
[2025-10-25 15:32:05 TP2] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-25 15:32:05 TP2] Memory pool end. avail mem=43.39 GB
[2025-10-25 15:32:05 TP3] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-25 15:32:05 TP3] Memory pool end. avail mem=43.40 GB
[2025-10-25 15:32:06 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
[2025-10-25 15:32:06 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
[2025-10-25 15:32:06 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[2025-10-25 15:32:06 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.20 GB
[2025-10-25 15:32:07 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.25 GB
[2025-10-25 15:32:07 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[2025-10-25 15:32:07 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.32 GB
[2025-10-25 15:32:07 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.61 GB
[2025-10-25 15:32:07 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.97 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-25 15:32:09 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-25 15:32:09 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-25 15:32:09 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-25 15:32:09 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-25 15:32:09 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-25 15:32:09 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-25 15:32:09 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-25 15:32:09 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:10 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:10 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:10 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:10 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:10 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:10 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:10 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:10 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-25 15:32:11 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-25 15:32:11 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-25 15:32:11 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:11 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-25 15:32:11 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:11 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:11 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:12 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-25 15:32:12 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-25 15:32:12 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-25 15:32:12 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:12 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-25 15:32:12 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:12 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:12 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:12 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:12 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:12 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.97 GB):   2%|         | 1/52 [00:04<04:06,  4.84s/it]Capturing batches (bs=496 avail_mem=42.31 GB):   2%|         | 1/52 [00:04<04:06,  4.84s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:13 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:13 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:13 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:13 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:13 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:13 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:13 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:13 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.31 GB):   4%|         | 2/52 [00:05<02:06,  2.54s/it]Capturing batches (bs=480 avail_mem=42.30 GB):   4%|         | 2/52 [00:05<02:06,  2.54s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.30 GB):   6%|         | 3/52 [00:06<01:16,  1.56s/it]Capturing batches (bs=464 avail_mem=42.30 GB):   6%|         | 3/52 [00:06<01:16,  1.56s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.30 GB):   8%|         | 4/52 [00:06<00:53,  1.11s/it]Capturing batches (bs=448 avail_mem=42.29 GB):   8%|         | 4/52 [00:06<00:53,  1.11s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:14 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.29 GB):  10%|         | 5/52 [00:06<00:40,  1.17it/s]Capturing batches (bs=432 avail_mem=42.29 GB):  10%|         | 5/52 [00:06<00:40,  1.17it/s][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.29 GB):  12%|        | 6/52 [00:07<00:32,  1.43it/s]Capturing batches (bs=416 avail_mem=42.28 GB):  12%|        | 6/52 [00:07<00:32,  1.43it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:15 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.28 GB):  13%|        | 7/52 [00:07<00:27,  1.66it/s]Capturing batches (bs=400 avail_mem=42.28 GB):  13%|        | 7/52 [00:07<00:27,  1.66it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.28 GB):  15%|        | 8/52 [00:08<00:23,  1.86it/s]Capturing batches (bs=384 avail_mem=42.27 GB):  15%|        | 8/52 [00:08<00:23,  1.86it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.27 GB):  17%|        | 9/52 [00:08<00:19,  2.20it/s]Capturing batches (bs=368 avail_mem=42.27 GB):  17%|        | 9/52 [00:08<00:19,  2.20it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:16 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.27 GB):  19%|        | 10/52 [00:08<00:18,  2.27it/s]Capturing batches (bs=352 avail_mem=42.27 GB):  19%|        | 10/52 [00:08<00:18,  2.27it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.27 GB):  21%|        | 11/52 [00:09<00:17,  2.33it/s]Capturing batches (bs=336 avail_mem=42.26 GB):  21%|        | 11/52 [00:09<00:17,  2.33it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.26 GB):  23%|       | 12/52 [00:09<00:16,  2.37it/s]Capturing batches (bs=320 avail_mem=42.26 GB):  23%|       | 12/52 [00:09<00:16,  2.37it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:17 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.26 GB):  25%|       | 13/52 [00:09<00:14,  2.67it/s]Capturing batches (bs=304 avail_mem=42.25 GB):  25%|       | 13/52 [00:09<00:14,  2.67it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.25 GB):  27%|       | 14/52 [00:10<00:14,  2.58it/s]Capturing batches (bs=288 avail_mem=42.25 GB):  27%|       | 14/52 [00:10<00:14,  2.58it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.25 GB):  29%|       | 15/52 [00:10<00:12,  2.86it/s]Capturing batches (bs=272 avail_mem=42.24 GB):  29%|       | 15/52 [00:10<00:12,  2.86it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:18 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.24 GB):  31%|       | 16/52 [00:11<00:13,  2.71it/s]Capturing batches (bs=256 avail_mem=42.24 GB):  31%|       | 16/52 [00:11<00:13,  2.71it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:19 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:19 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:19 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:19 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:19 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:19 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:19 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:19 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:19 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.24 GB):  33%|      | 17/52 [00:11<00:13,  2.62it/s]Capturing batches (bs=248 avail_mem=42.23 GB):  33%|      | 17/52 [00:11<00:13,  2.62it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:19 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.23 GB):  35%|      | 18/52 [00:11<00:13,  2.57it/s]Capturing batches (bs=240 avail_mem=42.23 GB):  35%|      | 18/52 [00:11<00:13,  2.57it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.23 GB):  37%|      | 19/52 [00:12<00:13,  2.52it/s]Capturing batches (bs=232 avail_mem=42.22 GB):  37%|      | 19/52 [00:12<00:13,  2.52it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:20 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.22 GB):  38%|      | 20/52 [00:12<00:12,  2.48it/s]Capturing batches (bs=224 avail_mem=42.22 GB):  38%|      | 20/52 [00:12<00:12,  2.48it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.22 GB):  40%|      | 21/52 [00:13<00:12,  2.45it/s]Capturing batches (bs=216 avail_mem=42.21 GB):  40%|      | 21/52 [00:13<00:12,  2.45it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.21 GB):  42%|     | 22/52 [00:13<00:12,  2.44it/s]Capturing batches (bs=208 avail_mem=42.21 GB):  42%|     | 22/52 [00:13<00:12,  2.44it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:21 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.21 GB):  44%|     | 23/52 [00:13<00:10,  2.73it/s]Capturing batches (bs=200 avail_mem=42.20 GB):  44%|     | 23/52 [00:13<00:10,  2.73it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.20 GB):  46%|     | 24/52 [00:14<00:10,  2.64it/s]Capturing batches (bs=192 avail_mem=42.20 GB):  46%|     | 24/52 [00:14<00:10,  2.64it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.20 GB):  48%|     | 25/52 [00:14<00:09,  2.90it/s]Capturing batches (bs=184 avail_mem=42.20 GB):  48%|     | 25/52 [00:14<00:09,  2.90it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.20 GB):  50%|     | 26/52 [00:14<00:08,  3.12it/s]Capturing batches (bs=176 avail_mem=42.19 GB):  50%|     | 26/52 [00:14<00:08,  3.12it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:22 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.19 GB):  52%|    | 27/52 [00:14<00:07,  3.30it/s]Capturing batches (bs=168 avail_mem=42.19 GB):  52%|    | 27/52 [00:14<00:07,  3.30it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.19 GB):  54%|    | 28/52 [00:15<00:08,  2.99it/s]Capturing batches (bs=160 avail_mem=42.19 GB):  54%|    | 28/52 [00:15<00:08,  2.99it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:23 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.19 GB):  56%|    | 29/52 [00:15<00:07,  3.20it/s]Capturing batches (bs=152 avail_mem=42.18 GB):  56%|    | 29/52 [00:15<00:07,  3.20it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.18 GB):  58%|    | 30/52 [00:16<00:07,  2.94it/s]Capturing batches (bs=144 avail_mem=42.18 GB):  58%|    | 30/52 [00:16<00:07,  2.94it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.18 GB):  60%|    | 31/52 [00:16<00:06,  3.12it/s]Capturing batches (bs=136 avail_mem=42.17 GB):  60%|    | 31/52 [00:16<00:06,  3.12it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.17 GB):  62%|   | 32/52 [00:16<00:06,  3.28it/s]Capturing batches (bs=128 avail_mem=42.17 GB):  62%|   | 32/52 [00:16<00:06,  3.28it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:24 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:24 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:24 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:24 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:24 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:24 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:24 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:24 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:24 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:24 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:24 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:24 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:24 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:24 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:24 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:24 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:24 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:24 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.17 GB):  63%|   | 33/52 [00:16<00:05,  3.40it/s]Capturing batches (bs=120 avail_mem=42.17 GB):  63%|   | 33/52 [00:16<00:05,  3.40it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.17 GB):  65%|   | 34/52 [00:17<00:05,  3.05it/s]Capturing batches (bs=112 avail_mem=42.16 GB):  65%|   | 34/52 [00:17<00:05,  3.05it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.16 GB):  67%|   | 35/52 [00:17<00:05,  3.25it/s]Capturing batches (bs=104 avail_mem=42.16 GB):  67%|   | 35/52 [00:17<00:05,  3.25it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:25 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.16 GB):  69%|   | 36/52 [00:17<00:05,  2.96it/s]Capturing batches (bs=96 avail_mem=42.16 GB):  69%|   | 36/52 [00:17<00:05,  2.96it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.16 GB):  71%|   | 37/52 [00:18<00:04,  3.17it/s]Capturing batches (bs=88 avail_mem=42.15 GB):  71%|   | 37/52 [00:18<00:04,  3.17it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.15 GB):  73%|  | 38/52 [00:18<00:04,  2.89it/s]Capturing batches (bs=80 avail_mem=42.15 GB):  73%|  | 38/52 [00:18<00:04,  2.89it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:26 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.15 GB):  75%|  | 39/52 [00:18<00:04,  3.12it/s]Capturing batches (bs=72 avail_mem=42.14 GB):  75%|  | 39/52 [00:18<00:04,  3.12it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.14 GB):  77%|  | 40/52 [00:19<00:04,  2.87it/s]Capturing batches (bs=64 avail_mem=42.14 GB):  77%|  | 40/52 [00:19<00:04,  2.87it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:27 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:27 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:27 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:27 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:27 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:27 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:27 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:32:27 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:27 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:32:27 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.14 GB):  79%|  | 41/52 [00:19<00:03,  3.10it/s]Capturing batches (bs=56 avail_mem=42.13 GB):  79%|  | 41/52 [00:19<00:03,  3.10it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:27 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.13 GB):  81%|  | 42/52 [00:19<00:03,  2.88it/s]Capturing batches (bs=48 avail_mem=42.13 GB):  81%|  | 42/52 [00:19<00:03,  2.88it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.13 GB):  83%| | 43/52 [00:20<00:02,  3.08it/s]Capturing batches (bs=40 avail_mem=42.12 GB):  83%| | 43/52 [00:20<00:02,  3.08it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.12 GB):  85%| | 44/52 [00:20<00:02,  2.87it/s]Capturing batches (bs=32 avail_mem=42.12 GB):  85%| | 44/52 [00:20<00:02,  2.87it/s][aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:28 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:28 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=32 avail_mem=42.12 GB):  87%| | 45/52 [00:20<00:02,  3.07it/s]Capturing batches (bs=24 avail_mem=42.11 GB):  87%| | 45/52 [00:20<00:02,  3.07it/s][aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=24 avail_mem=42.11 GB):  88%| | 46/52 [00:21<00:02,  2.86it/s]Capturing batches (bs=16 avail_mem=42.11 GB):  88%| | 46/52 [00:21<00:02,  2.86it/s][aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:32:29 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=16 avail_mem=42.11 GB):  90%| | 47/52 [00:21<00:01,  3.10it/s]Capturing batches (bs=12 avail_mem=42.11 GB):  90%| | 47/52 [00:21<00:01,  3.10it/s][aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:29 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=12 avail_mem=42.11 GB):  92%|| 48/52 [00:21<00:01,  3.30it/s]Capturing batches (bs=8 avail_mem=42.10 GB):  92%|| 48/52 [00:21<00:01,  3.30it/s] [aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=8 avail_mem=42.10 GB):  94%|| 49/52 [00:22<00:00,  3.47it/s]Capturing batches (bs=4 avail_mem=42.10 GB):  94%|| 49/52 [00:22<00:00,  3.47it/s][aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=4 avail_mem=42.10 GB):  96%|| 50/52 [00:22<00:00,  3.59it/s]Capturing batches (bs=2 avail_mem=42.10 GB):  96%|| 50/52 [00:22<00:00,  3.59it/s][aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:30 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=2 avail_mem=42.10 GB):  98%|| 51/52 [00:22<00:00,  3.69it/s]Capturing batches (bs=1 avail_mem=42.09 GB):  98%|| 51/52 [00:22<00:00,  3.69it/s][aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:31 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:31 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:31 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:31 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:31 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:31 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:31 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:31 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=1 avail_mem=42.09 GB): 100%|| 52/52 [00:23<00:00,  2.42it/s]Capturing batches (bs=1 avail_mem=42.09 GB): 100%|| 52/52 [00:23<00:00,  2.23it/s]
[2025-10-25 15:32:32 TP0] Registering 6396 cuda graph addresses
[2025-10-25 15:32:32 TP4] Capture cuda graph end. Time elapsed: 24.58 s. mem usage=1.52 GB. avail mem=41.72 GB.
[2025-10-25 15:32:32 TP7] Capture cuda graph end. Time elapsed: 24.52 s. mem usage=1.52 GB. avail mem=41.80 GB.
[2025-10-25 15:32:32 TP1] Capture cuda graph end. Time elapsed: 25.60 s. mem usage=1.52 GB. avail mem=41.67 GB.
[2025-10-25 15:32:32 TP5] Capture cuda graph end. Time elapsed: 25.57 s. mem usage=1.52 GB. avail mem=41.81 GB.
[2025-10-25 15:32:32 TP2] Capture cuda graph end. Time elapsed: 25.69 s. mem usage=1.52 GB. avail mem=41.66 GB.
[2025-10-25 15:32:32 TP3] Capture cuda graph end. Time elapsed: 25.59 s. mem usage=1.52 GB. avail mem=41.68 GB.
[2025-10-25 15:32:32 TP6] Capture cuda graph end. Time elapsed: 24.63 s. mem usage=1.52 GB. avail mem=41.79 GB.
[2025-10-25 15:32:32 TP0] Capture cuda graph end. Time elapsed: 24.59 s. mem usage=1.52 GB. avail mem=42.09 GB.
[2025-10-25 15:32:32 TP0] max_total_num_tokens=971748, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=42.09 GB
[2025-10-25 15:32:33] INFO:     Started server process [47]
[2025-10-25 15:32:33] INFO:     Waiting for application startup.
[2025-10-25 15:32:33] INFO:     Application startup complete.
[2025-10-25 15:32:33] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-25 15:32:34] INFO:     127.0.0.1:48642 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-25 15:32:34] INFO:     127.0.0.1:48650 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-25 15:32:34 TP0] Prefill batch [1], #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:34 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:34 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:34 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:34 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:34 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:35 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:35 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:35 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:37] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:32:37] The server is fired up and ready to roll!
[2025-10-25 15:32:41] INFO:     127.0.0.1:39988 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-25 15:32:41 TP0] Prefill batch [10], #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41] INFO:     127.0.0.1:39998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:32:41 TP0] Prefill batch [11], #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:41 TP0] Prefill batch [12], #new-seq: 44, #new-token: 2588, #cached-token: 29348, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:42 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:42 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:42 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:42 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:42 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:42 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:42 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:42 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:42 TP0] Prefill batch [13], #new-seq: 132, #new-token: 8220, #cached-token: 88308, token usage: 0.00, #running-req: 45, #queue-req: 0, 
[2025-10-25 15:32:42 TP0] Prefill batch [14], #new-seq: 135, #new-token: 7888, #cached-token: 90362, token usage: 0.01, #running-req: 177, #queue-req: 0, 
[2025-10-25 15:32:51 TP0] Prefill batch [15], #new-seq: 278, #new-token: 16380, #cached-token: 186116, token usage: 0.02, #running-req: 312, #queue-req: 729, 
[2025-10-25 15:32:53 TP0] Prefill batch [16], #new-seq: 274, #new-token: 16353, #cached-token: 183503, token usage: 0.04, #running-req: 590, #queue-req: 455, 
[2025-10-25 15:32:54 TP0] Prefill batch [17], #new-seq: 160, #new-token: 9988, #cached-token: 107165, token usage: 0.05, #running-req: 864, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-25 15:32:55 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:55 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:32:58] INFO:     127.0.0.1:42876 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP0] Prefill batch [46], #new-seq: 1, #new-token: 43, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP5] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP1] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP7] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP3] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP4] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP0] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP2] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:58 TP6] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59] INFO:     127.0.0.1:42924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:32:59] INFO:     127.0.0.1:40180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:32:59 TP0] Prefill batch [51], #new-seq: 1, #new-token: 41, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP1] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP5] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP3] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP4] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP7] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP2] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP0] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP6] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP0] Decode batch [51], #running-req: 1023, #token: 94358, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1235.26, #queue-req: 293, 
[2025-10-25 15:32:59 TP0] Prefill batch [53], #new-seq: 1, #new-token: 66, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:32:59 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:44516 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00 TP0] Prefill batch [62], #new-seq: 3, #new-token: 238, #cached-token: 2010, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[aiter] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP1] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP4] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP5] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP3] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP7] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP0] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP2] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP6] [fused_moe] using default for (238, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP0] Prefill batch [64], #new-seq: 11, #new-token: 762, #cached-token: 7369, token usage: 0.11, #running-req: 1013, #queue-req: 278, 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP5] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP3] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP0] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP7] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP4] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP1] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP2] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP6] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00] INFO:     127.0.0.1:41042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:48490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00 TP0] Prefill batch [66], #new-seq: 3, #new-token: 178, #cached-token: 2013, token usage: 0.11, #running-req: 1021, #queue-req: 275, 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP5] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP0] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP4] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP7] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP3] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP1] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP2] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP6] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:40500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:42234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:44366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:44936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:00] INFO:     127.0.0.1:45332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:00 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] Prefill batch [68], #new-seq: 8, #new-token: 376, #cached-token: 5363, token usage: 0.11, #running-req: 1016, #queue-req: 267, 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP5] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP3] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP4] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP7] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP1] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP2] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP6] [fused_moe] using default for (376, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:43318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:45146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:48630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] Prefill batch [70], #new-seq: 6, #new-token: 356, #cached-token: 4019, token usage: 0.11, #running-req: 1018, #queue-req: 261, 
[aiter] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP5] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP3] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP7] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP4] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP1] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP2] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP6] [fused_moe] using default for (356, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:43820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] Prefill batch [72], #new-seq: 7, #new-token: 366, #cached-token: 4689, token usage: 0.11, #running-req: 1017, #queue-req: 254, 
[aiter] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP5] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP1] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP7] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP4] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP3] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP2] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP6] [fused_moe] using default for (366, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:46082 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] Prefill batch [74], #new-seq: 2, #new-token: 93, #cached-token: 1340, token usage: 0.11, #running-req: 1022, #queue-req: 252, 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP5] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP3] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP7] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP1] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP4] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP2] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP6] [fused_moe] using default for (93, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:46164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:01 TP0] Prefill batch [76], #new-seq: 8, #new-token: 504, #cached-token: 5358, token usage: 0.11, #running-req: 1016, #queue-req: 244, 
[aiter] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP2] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP0] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP5] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP3] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP4] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP7] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP1] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:01 TP6] [fused_moe] using default for (504, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:41422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:42184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:42334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:47354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:47926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:48834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP0] Prefill batch [78], #new-seq: 9, #new-token: 617, #cached-token: 6032, token usage: 0.11, #running-req: 1015, #queue-req: 235, 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP5] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP7] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP4] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP1] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP0] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP3] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP2] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP6] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02] INFO:     127.0.0.1:41316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:42558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:44176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:45248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02 TP0] Prefill batch [80], #new-seq: 8, #new-token: 519, #cached-token: 5362, token usage: 0.11, #running-req: 1016, #queue-req: 227, 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP5] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP0] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP3] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP4] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP1] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP7] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP2] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP6] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02] INFO:     127.0.0.1:40942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:44940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:45096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02 TP0] Prefill batch [82], #new-seq: 7, #new-token: 453, #cached-token: 4690, token usage: 0.11, #running-req: 1017, #queue-req: 220, 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP5] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP3] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP0] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP7] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP1] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP4] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP2] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP6] [fused_moe] using default for (453, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:44252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:45596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:46556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02 TP0] Prefill batch [84], #new-seq: 6, #new-token: 345, #cached-token: 4017, token usage: 0.12, #running-req: 1018, #queue-req: 214, 
[aiter] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP0] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP5] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP7] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP3] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP1] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP4] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP2] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP6] [fused_moe] using default for (345, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:43992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:45340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02] INFO:     127.0.0.1:47934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:02 TP0] Prefill batch [86], #new-seq: 7, #new-token: 337, #cached-token: 4688, token usage: 0.12, #running-req: 1017, #queue-req: 207, 
[aiter] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP5] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP3] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP0] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP7] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP4] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP1] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP2] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:02 TP6] [fused_moe] using default for (337, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:47986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03 TP0] Prefill batch [88], #new-seq: 11, #new-token: 622, #cached-token: 7369, token usage: 0.12, #running-req: 1013, #queue-req: 196, 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP5] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP0] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP3] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP7] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP1] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP4] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP2] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP6] [fused_moe] using default for (622, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:45682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03 TP0] Prefill batch [90], #new-seq: 7, #new-token: 391, #cached-token: 4688, token usage: 0.12, #running-req: 1017, #queue-req: 189, 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP5] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP0] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP3] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP7] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP4] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP1] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP2] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP6] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:43388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:45092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:46228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03 TP0] Prefill batch [92], #new-seq: 11, #new-token: 667, #cached-token: 7368, token usage: 0.12, #running-req: 1013, #queue-req: 178, 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP0] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP5] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP7] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP3] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP4] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP1] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP2] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP6] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03] INFO:     127.0.0.1:40510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:41960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:42132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:42522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:45974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03 TP0] Prefill batch [94], #new-seq: 11, #new-token: 547, #cached-token: 7368, token usage: 0.12, #running-req: 1013, #queue-req: 167, 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP0] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP5] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP7] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP3] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP4] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP1] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP2] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03 TP6] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:03] INFO:     127.0.0.1:41566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:42362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:42958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:46060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:03] INFO:     127.0.0.1:46102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04 TP0] Prefill batch [96], #new-seq: 8, #new-token: 443, #cached-token: 5362, token usage: 0.12, #running-req: 1016, #queue-req: 159, 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP5] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP0] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP7] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP3] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP1] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP4] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP2] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP6] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04 TP0] Prefill batch [98], #new-seq: 6, #new-token: 315, #cached-token: 4018, token usage: 0.12, #running-req: 1018, #queue-req: 153, 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP5] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP3] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP4] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP7] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP1] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP2] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP0] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP6] [fused_moe] using default for (315, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:46718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04 TP0] Prefill batch [100], #new-seq: 3, #new-token: 191, #cached-token: 2010, token usage: 0.12, #running-req: 1021, #queue-req: 150, 
[aiter] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP0] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP5] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP2] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP7] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP4] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP3] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP1] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP6] [fused_moe] using default for (191, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:41798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:42160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:43454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:47120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:47936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04 TP0] Prefill batch [102], #new-seq: 11, #new-token: 724, #cached-token: 7372, token usage: 0.12, #running-req: 1013, #queue-req: 139, 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP0] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP5] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP3] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP1] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP7] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP4] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP2] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP6] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:42290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:43498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:43526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:44850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:45744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:04] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP0] Prefill batch [104], #new-seq: 12, #new-token: 683, #cached-token: 8039, token usage: 0.12, #running-req: 1012, #queue-req: 127, 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP0] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP5] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP7] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP1] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP3] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP4] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP2] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:04 TP6] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05] INFO:     127.0.0.1:40344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:40642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:45254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP0] Prefill batch [106], #new-seq: 14, #new-token: 986, #cached-token: 9380, token usage: 0.12, #running-req: 1010, #queue-req: 113, 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP5] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP3] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP7] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP0] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP1] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP4] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP2] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP6] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05] INFO:     127.0.0.1:42736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:46118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:46950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05 TP0] Prefill batch [108], #new-seq: 7, #new-token: 479, #cached-token: 4694, token usage: 0.12, #running-req: 1017, #queue-req: 106, 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP5] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP7] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP3] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP1] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP4] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP0] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP2] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP6] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:41476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:43018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:47878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05 TP0] Prefill batch [110], #new-seq: 11, #new-token: 732, #cached-token: 7373, token usage: 0.12, #running-req: 1013, #queue-req: 95, 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP5] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP7] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP0] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP3] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP1] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP4] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP2] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP6] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05] INFO:     127.0.0.1:40256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:41204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:41944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:42468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:45870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:47856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05 TP0] Prefill batch [112], #new-seq: 14, #new-token: 731, #cached-token: 9379, token usage: 0.12, #running-req: 1010, #queue-req: 81, 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP0] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP5] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP1] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP3] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP4] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP7] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP2] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05 TP6] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:05] INFO:     127.0.0.1:41586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:42428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:05] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06 TP0] Prefill batch [114], #new-seq: 8, #new-token: 486, #cached-token: 5357, token usage: 0.12, #running-req: 1016, #queue-req: 73, 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP5] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP3] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP1] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP0] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP7] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP4] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP2] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP6] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06] INFO:     127.0.0.1:40208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:41758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:42412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:43544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:45894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06 TP0] Prefill batch [116], #new-seq: 11, #new-token: 824, #cached-token: 7365, token usage: 0.12, #running-req: 1013, #queue-req: 62, 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP5] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP0] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP7] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP3] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP1] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP4] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP2] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP6] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:43792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:44566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:45708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:48892 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP0] Prefill batch [118], #new-seq: 17, #new-token: 1095, #cached-token: 11390, token usage: 0.12, #running-req: 1007, #queue-req: 45, 
[2025-10-25 15:33:06] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:44238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:44644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:45948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:48432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:49092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06 TP0] Prefill batch [120], #new-seq: 14, #new-token: 822, #cached-token: 9378, token usage: 0.12, #running-req: 1010, #queue-req: 31, 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP5] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP4] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP0] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP1] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP3] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP7] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP2] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP6] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:43662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:45458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06] INFO:     127.0.0.1:48576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:06 TP0] Prefill batch [122], #new-seq: 11, #new-token: 668, #cached-token: 7372, token usage: 0.12, #running-req: 1013, #queue-req: 20, 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP7] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP5] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP3] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP4] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP0] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP1] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP2] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:06 TP6] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07] INFO:     127.0.0.1:40698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:41796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:43130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:44388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07 TP0] Prefill batch [124], #new-seq: 9, #new-token: 596, #cached-token: 6027, token usage: 0.13, #running-req: 1015, #queue-req: 11, 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP0] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP5] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP3] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP7] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP4] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP1] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP2] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP6] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP0] Decode batch [124], #running-req: 1015, #token: 121036, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5070.13, #queue-req: 11, 
[2025-10-25 15:33:07] INFO:     127.0.0.1:40818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:43102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:47900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP0] Prefill batch [126], #new-seq: 11, #new-token: 569, #cached-token: 7374, token usage: 0.13, #running-req: 1011, #queue-req: 0, 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP0] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP5] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP7] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP3] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP4] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP1] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP2] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP6] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:41564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:43548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:43564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:44250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:45590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:40212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:41598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:47170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07] INFO:     127.0.0.1:41660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:42082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:43922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:45406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP5] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP3] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP1] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP4] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP7] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP0] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP2] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07 TP6] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:07] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:44308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:44672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:46964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:07] INFO:     127.0.0.1:48124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:41114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:41268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:42976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:43796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP5] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP3] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP1] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP4] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP7] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP0] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP2] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP6] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:42726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:47894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48420 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP5] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP3] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP1] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP7] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP4] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP0] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP2] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP6] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08] INFO:     127.0.0.1:40318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:40466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:40666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:40800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:43950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:44766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:44952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:45214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:45390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP5] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP3] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP1] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP7] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP4] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP0] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP2] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP6] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08] INFO:     127.0.0.1:41302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:41928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:42536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:43142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:45282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP5] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP3] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP1] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP4] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP7] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP0] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP2] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP6] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08] INFO:     127.0.0.1:40834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48898 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP5] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP1] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP3] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP7] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP4] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP0] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP2] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP6] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:41508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:47692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP5] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP1] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP3] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP4] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP7] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP0] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP2] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP6] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:42104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:42538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:42648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:43972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP5] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP1] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP3] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP7] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP4] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP0] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP2] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08 TP6] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:08] INFO:     127.0.0.1:40270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:40274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:41856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:44662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:45158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:08] INFO:     127.0.0.1:47118 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP7] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP1] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP5] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP3] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP4] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP6] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP2] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP0] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09] INFO:     127.0.0.1:40080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:40334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:40772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:42284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:44992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:45350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:45694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP0] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP2] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP3] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP1] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP4] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP5] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP6] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP7] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09] INFO:     127.0.0.1:40650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:45416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:49104 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP5] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP1] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP0] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP4] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP3] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP7] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP2] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP6] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09] INFO:     127.0.0.1:40120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:40480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP5] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP1] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP4] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP0] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP3] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP7] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP2] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP6] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:42384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:45172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48364 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP5] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP1] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP4] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP3] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP7] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP0] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP2] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP6] [fused_moe] using default for (855, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09] INFO:     127.0.0.1:42194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:45476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:46132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP5] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP1] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP3] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP4] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP7] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP0] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP2] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP6] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09] INFO:     127.0.0.1:40892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:41614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:44012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47804 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP1] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP3] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP5] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP4] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP7] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP0] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP2] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP6] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:42304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:09] INFO:     127.0.0.1:48958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP5] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP1] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP3] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP4] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP7] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP0] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP2] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:09 TP6] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10] INFO:     127.0.0.1:40456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:41684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:49198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP5] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP3] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP1] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP7] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP4] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP0] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP2] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP6] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10] INFO:     127.0.0.1:40504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:42968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:49116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:49790 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP5] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP1] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP4] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP3] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP7] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP0] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP2] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP6] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:47714 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP5] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP3] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP1] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP4] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP7] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP2] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP0] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP6] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:43166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:43868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:47754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:49334 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP5] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP3] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP1] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP7] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP4] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP0] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP2] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP6] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:46096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:47464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP5] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP1] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP3] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP7] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP4] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP0] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP2] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP6] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10] INFO:     127.0.0.1:40838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:41392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:41898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:42214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:47384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:48782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP5] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP3] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP1] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP7] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP4] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP0] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP2] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP6] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10] INFO:     127.0.0.1:40224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:42066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:43010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP5] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP3] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP1] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP4] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP7] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP0] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP2] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP6] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP5] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP1] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP3] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP7] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP4] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP0] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP2] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10 TP6] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:10] INFO:     127.0.0.1:44688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:10] INFO:     127.0.0.1:50286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:41368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:44560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP5] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP3] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP1] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP7] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP4] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP0] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP2] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP6] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11] INFO:     127.0.0.1:40958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:45638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP5] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP3] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP1] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP4] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP7] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP0] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP2] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP6] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11] INFO:     127.0.0.1:40130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:40308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:40856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:42618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:44616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:45796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49170 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP5] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP3] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP1] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP4] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP7] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP0] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP2] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP6] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:40548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:40668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:40878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:41428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:44536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:44718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:50170 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP5] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP3] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP1] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP7] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP4] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP0] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP2] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP6] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11] INFO:     127.0.0.1:42592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:40376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:41550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP5] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP1] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP3] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP4] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP7] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP0] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP2] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP6] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:42908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:45486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:45600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP5] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP3] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP1] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP4] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP7] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP0] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP2] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP6] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:41626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:44958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP3] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP5] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP1] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP4] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP7] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP0] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP2] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP6] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11] INFO:     127.0.0.1:42758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:11] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP5] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP1] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP7] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP4] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP3] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP0] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP2] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:11 TP6] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12] INFO:     127.0.0.1:40072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:44346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:45518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:46760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP5] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP4] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP3] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP1] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP7] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP2] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP6] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12] INFO:     127.0.0.1:40162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:41174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:45766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP5] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP4] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP3] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP1] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP7] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP2] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP6] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12] INFO:     127.0.0.1:40172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:41130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP5] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP7] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP4] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP3] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP1] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP2] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP6] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] Decode batch [166], #running-req: 611, #token: 92364, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6462.50, #queue-req: 0, 
[2025-10-25 15:33:12] INFO:     127.0.0.1:41610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP5] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP7] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP4] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP3] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP1] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP2] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP6] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12] INFO:     127.0.0.1:42670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:44102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:44154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP5] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP3] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP1] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP4] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP7] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP2] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP6] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:40738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP1] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP7] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP3] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP4] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP5] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP2] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP6] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:41976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:43602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:45462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:45954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP5] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP4] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP7] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP3] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP1] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP2] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP6] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12] INFO:     127.0.0.1:40136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:40920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:45048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP5] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP3] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP7] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP1] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP4] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP2] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP6] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12] INFO:     127.0.0.1:42516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:43696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:45650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:12] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP5] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP3] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP7] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP1] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP4] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP0] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP2] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:12 TP6] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:45684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP5] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP1] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP3] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP7] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP4] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP0] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP2] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP6] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:43766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP5] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP3] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP1] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP7] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP4] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP0] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP2] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13 TP6] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:13] INFO:     127.0.0.1:40446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:42122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:43048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:42680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:43450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:45106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:41772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:42434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:41324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:40110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:42306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:40684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:42222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:46314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:43938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:45932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:40258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:41222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:42402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:43154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:45576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:49710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:13] INFO:     127.0.0.1:51650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:40290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:46750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:45750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:45906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:43710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:45550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:46276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:47276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:47814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:48244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:46266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:40986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:41932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:45380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:40404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:41806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:42102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:40236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:46250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:14] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:43486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:44218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:48502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:43798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:45764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15 TP0] Decode batch [206], #running-req: 293, #token: 54187, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6082.89, #queue-req: 0, 
[2025-10-25 15:33:15] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:46324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:40742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:44046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:42320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:47292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:40984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:45360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:48756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:48770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:40728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:42928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:41704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:51298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:15] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:45868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:46628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:40094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:42504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:42046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:46038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:40564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:41442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:44896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:42952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:41578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:42142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:40146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:44292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:50832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:16] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:41002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:43704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:46260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:46828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17 TP0] Decode batch [246], #running-req: 119, #token: 28188, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3909.74, #queue-req: 0, 
[2025-10-25 15:33:17] INFO:     127.0.0.1:47442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:47914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:46034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:43196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:41100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:47102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:40392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:44456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:41294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:44782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:49286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:17] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:47156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:42828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:48268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18 TP0] Decode batch [286], #running-req: 35, #token: 10229, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1807.71, #queue-req: 0, 
[2025-10-25 15:33:18] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:41408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:18] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:43756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:41638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:48996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:41258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19 TP0] Decode batch [326], #running-req: 11, #token: 4065, token usage: 0.00, cuda graph: True, gen throughput (token/s): 697.24, #queue-req: 0, 
[2025-10-25 15:33:19] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:19] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:20] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:20] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:20] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:20] INFO:     127.0.0.1:49064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:20] INFO:     127.0.0.1:50400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:20 TP0] Decode batch [366], #running-req: 1, #token: 960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 181.54, #queue-req: 0, 
[2025-10-25 15:33:20] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:33] INFO:     127.0.0.1:40334 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-25 15:33:33 TP0] Prefill batch [374], #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:33:33] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:33 TP0] Prefill batch [375], #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:33:34 TP0] Prefill batch [376], #new-seq: 40, #new-token: 40, #cached-token: 29096, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:33:34 TP0] Prefill batch [377], #new-seq: 45, #new-token: 45, #cached-token: 32605, token usage: 0.01, #running-req: 41, #queue-req: 0, 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP3] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP2] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP5] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP1] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP4] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP6] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP7] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] Prefill batch [378], #new-seq: 49, #new-token: 49, #cached-token: 35787, token usage: 0.01, #running-req: 86, #queue-req: 0, 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP2] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP5] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP3] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP1] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP4] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP7] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP6] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] Prefill batch [379], #new-seq: 52, #new-token: 52, #cached-token: 38108, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] Prefill batch [380], #new-seq: 56, #new-token: 56, #cached-token: 40540, token usage: 0.02, #running-req: 187, #queue-req: 0, 
[2025-10-25 15:33:34 TP0] Prefill batch [381], #new-seq: 57, #new-token: 57, #cached-token: 41517, token usage: 0.02, #running-req: 243, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP5] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP4] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP6] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP7] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP2] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP3] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP1] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] Prefill batch [382], #new-seq: 61, #new-token: 61, #cached-token: 44435, token usage: 0.02, #running-req: 300, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] Prefill batch [383], #new-seq: 63, #new-token: 63, #cached-token: 45984, token usage: 0.03, #running-req: 361, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP5] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP4] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP7] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP6] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP2] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP1] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP3] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:34 TP0] Prefill batch [384], #new-seq: 66, #new-token: 66, #cached-token: 48031, token usage: 0.03, #running-req: 424, #queue-req: 0, 
[2025-10-25 15:33:35 TP0] Prefill batch [385], #new-seq: 70, #new-token: 70, #cached-token: 50654, token usage: 0.03, #running-req: 490, #queue-req: 0, 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP5] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP1] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP4] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP7] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP2] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP3] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP6] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] Prefill batch [386], #new-seq: 72, #new-token: 72, #cached-token: 52442, token usage: 0.04, #running-req: 560, #queue-req: 0, 
[2025-10-25 15:33:35 TP0] Prefill batch [387], #new-seq: 74, #new-token: 74, #cached-token: 53861, token usage: 0.04, #running-req: 632, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] Prefill batch [388], #new-seq: 77, #new-token: 77, #cached-token: 56023, token usage: 0.05, #running-req: 706, #queue-req: 0, 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP5] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP4] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP7] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP6] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP2] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP1] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP3] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] Prefill batch [389], #new-seq: 84, #new-token: 84, #cached-token: 61303, token usage: 0.05, #running-req: 783, #queue-req: 0, 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP2] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP5] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP1] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP3] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP7] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP4] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP6] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] Prefill batch [390], #new-seq: 79, #new-token: 79, #cached-token: 57556, token usage: 0.06, #running-req: 867, #queue-req: 0, 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP2] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP3] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP1] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP5] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP4] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP7] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP6] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:35 TP0] Prefill batch [391], #new-seq: 38, #new-token: 38, #cached-token: 27776, token usage: 0.06, #running-req: 946, #queue-req: 0, 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP1] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP2] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP3] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP0] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP5] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP4] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP7] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP6] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP0] Prefill batch [392], #new-seq: 1, #new-token: 1, #cached-token: 721, token usage: 0.06, #running-req: 984, #queue-req: 0, 
[2025-10-25 15:33:36 TP0] Prefill batch [393], #new-seq: 39, #new-token: 39, #cached-token: 28757, token usage: 0.06, #running-req: 985, #queue-req: 8, 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP5] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP4] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP6] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP7] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP0] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP2] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP1] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:36 TP3] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:39] INFO:     127.0.0.1:43178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:39 TP0] Prefill batch [422], #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-25 15:33:40] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:40 TP0] Decode batch [427], #running-req: 1024, #token: 95426, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1656.42, #queue-req: 294, 
[2025-10-25 15:33:40 TP0] Prefill batch [428], #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-25 15:33:40] INFO:     127.0.0.1:43218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:40 TP0] Prefill batch [432], #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-25 15:33:41] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:44746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:44896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:40388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:41514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41 TP0] Prefill batch [438], #new-seq: 3, #new-token: 3, #cached-token: 2254, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41] INFO:     127.0.0.1:41998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:42712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:43836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41 TP0] Prefill batch [440], #new-seq: 9, #new-token: 9, #cached-token: 6655, token usage: 0.11, #running-req: 1015, #queue-req: 280, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:41748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:41] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:41 TP0] Prefill batch [442], #new-seq: 4, #new-token: 4, #cached-token: 2916, token usage: 0.11, #running-req: 1020, #queue-req: 276, 
[2025-10-25 15:33:42] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42 TP0] Prefill batch [444], #new-seq: 11, #new-token: 11, #cached-token: 7954, token usage: 0.11, #running-req: 1013, #queue-req: 265, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:41956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42 TP0] Prefill batch [446], #new-seq: 8, #new-token: 8, #cached-token: 5735, token usage: 0.11, #running-req: 1016, #queue-req: 257, 
[2025-10-25 15:33:42] INFO:     127.0.0.1:40860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:43984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP0] Prefill batch [448], #new-seq: 5, #new-token: 5, #cached-token: 3598, token usage: 0.11, #running-req: 1019, #queue-req: 252, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:42] INFO:     127.0.0.1:40738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:44514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:46104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:47752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:42] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:47528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43 TP0] Prefill batch [452], #new-seq: 9, #new-token: 9, #cached-token: 6595, token usage: 0.11, #running-req: 1015, #queue-req: 243, 
[2025-10-25 15:33:43] INFO:     127.0.0.1:41672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:47804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43 TP0] Prefill batch [454], #new-seq: 13, #new-token: 13, #cached-token: 9597, token usage: 0.11, #running-req: 1011, #queue-req: 230, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:43 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:43 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:43 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:43 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:43 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:43 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:43 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:43 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:43] INFO:     127.0.0.1:41316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:45754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43 TP0] Prefill batch [456], #new-seq: 7, #new-token: 7, #cached-token: 5150, token usage: 0.12, #running-req: 1017, #queue-req: 223, 
[2025-10-25 15:33:43] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:44376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:45622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:45958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43 TP0] Prefill batch [458], #new-seq: 8, #new-token: 8, #cached-token: 5838, token usage: 0.12, #running-req: 1016, #queue-req: 215, 
[2025-10-25 15:33:43] INFO:     127.0.0.1:41228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:41534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43 TP0] Prefill batch [460], #new-seq: 9, #new-token: 9, #cached-token: 6422, token usage: 0.12, #running-req: 1015, #queue-req: 206, 
[2025-10-25 15:33:43] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:42036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:42192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:45898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:46090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:43] INFO:     127.0.0.1:46312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44 TP0] Prefill batch [462], #new-seq: 9, #new-token: 9, #cached-token: 6510, token usage: 0.12, #running-req: 1015, #queue-req: 197, 
[2025-10-25 15:33:44] INFO:     127.0.0.1:44708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:44724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:46566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44 TP0] Prefill batch [464], #new-seq: 6, #new-token: 6, #cached-token: 4409, token usage: 0.12, #running-req: 1018, #queue-req: 191, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:42068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:42132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44 TP0] Prefill batch [466], #new-seq: 8, #new-token: 8, #cached-token: 5800, token usage: 0.12, #running-req: 1016, #queue-req: 183, 
[2025-10-25 15:33:44] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:41544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:42234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:42350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:46076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP0] Prefill batch [468], #new-seq: 10, #new-token: 10, #cached-token: 7254, token usage: 0.12, #running-req: 1014, #queue-req: 173, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:44] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:41644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:41818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:42356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:42464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:43562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:44834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:45196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:46168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44] INFO:     127.0.0.1:47444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:44 TP0] Prefill batch [470], #new-seq: 11, #new-token: 11, #cached-token: 7918, token usage: 0.12, #running-req: 1013, #queue-req: 162, 
[2025-10-25 15:33:45] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:47934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45 TP0] Prefill batch [472], #new-seq: 7, #new-token: 7, #cached-token: 5035, token usage: 0.12, #running-req: 1017, #queue-req: 155, 
[2025-10-25 15:33:45] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:43718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45 TP0] Prefill batch [474], #new-seq: 7, #new-token: 7, #cached-token: 5127, token usage: 0.12, #running-req: 1017, #queue-req: 148, 
[2025-10-25 15:33:45] INFO:     127.0.0.1:42102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:43292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:47330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:48128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45 TP0] Prefill batch [476], #new-seq: 8, #new-token: 8, #cached-token: 5923, token usage: 0.12, #running-req: 1016, #queue-req: 140, 
[2025-10-25 15:33:45] INFO:     127.0.0.1:40504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:45026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45 TP0] Prefill batch [478], #new-seq: 8, #new-token: 8, #cached-token: 5806, token usage: 0.12, #running-req: 1016, #queue-req: 132, 
[2025-10-25 15:33:45] INFO:     127.0.0.1:40896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:41074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:42086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:47088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:47778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45] INFO:     127.0.0.1:48212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:45 TP0] Prefill batch [480], #new-seq: 9, #new-token: 9, #cached-token: 6610, token usage: 0.12, #running-req: 1015, #queue-req: 123, 
[2025-10-25 15:33:46] INFO:     127.0.0.1:42492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:42986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:46492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46 TP0] Prefill batch [482], #new-seq: 10, #new-token: 10, #cached-token: 7344, token usage: 0.12, #running-req: 1014, #queue-req: 113, 
[2025-10-25 15:33:46] INFO:     127.0.0.1:40986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:41368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:41630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:43336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46 TP0] Prefill batch [484], #new-seq: 9, #new-token: 9, #cached-token: 6682, token usage: 0.12, #running-req: 1015, #queue-req: 104, 
[2025-10-25 15:33:46] INFO:     127.0.0.1:40800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:41268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:43164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:43946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46 TP0] Prefill batch [486], #new-seq: 12, #new-token: 12, #cached-token: 8746, token usage: 0.12, #running-req: 1012, #queue-req: 92, 
[2025-10-25 15:33:46] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:41720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:43016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:43944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46 TP0] Prefill batch [488], #new-seq: 12, #new-token: 12, #cached-token: 8667, token usage: 0.12, #running-req: 1012, #queue-req: 80, 
[2025-10-25 15:33:46] INFO:     127.0.0.1:40562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:41678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:42536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:43316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:45942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:46] INFO:     127.0.0.1:48226 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:46 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:46 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:46 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:46 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:46 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:46 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:46 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:46 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP0] Prefill batch [490], #new-seq: 16, #new-token: 16, #cached-token: 11773, token usage: 0.13, #running-req: 1008, #queue-req: 64, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:33:47] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:41932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:42016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:43734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:44390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:44946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:45826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP5] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP0] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP2] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP4] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP6] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP1] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP3] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP7] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP0] Prefill batch [492], #new-seq: 19, #new-token: 19, #cached-token: 13996, token usage: 0.13, #running-req: 1005, #queue-req: 45, 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP0] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP2] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP5] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP4] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP6] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP1] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP3] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47 TP7] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:47] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:43612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:45032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47 TP0] Prefill batch [494], #new-seq: 9, #new-token: 9, #cached-token: 6513, token usage: 0.13, #running-req: 1015, #queue-req: 36, 
[2025-10-25 15:33:47] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:43580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:47410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47 TP0] Prefill batch [496], #new-seq: 11, #new-token: 11, #cached-token: 7976, token usage: 0.13, #running-req: 1013, #queue-req: 25, 
[2025-10-25 15:33:47] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:41608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:42432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:47062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47 TP0] Prefill batch [498], #new-seq: 10, #new-token: 10, #cached-token: 7359, token usage: 0.13, #running-req: 1014, #queue-req: 15, 
[2025-10-25 15:33:47 TP0] Decode batch [498], #running-req: 1014, #token: 120643, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5370.64, #queue-req: 15, 
[2025-10-25 15:33:47] INFO:     127.0.0.1:41148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:42616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:44634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:48094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:47] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48 TP0] Prefill batch [500], #new-seq: 15, #new-token: 15, #cached-token: 10892, token usage: 0.13, #running-req: 1007, #queue-req: 0, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48] INFO:     127.0.0.1:41560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:43716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:41764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:44958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:45974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:41936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:43918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:44108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:44686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:59180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP4] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP2] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP6] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP0] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP5] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP1] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP3] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP7] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:44948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:46004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:47130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP4] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP2] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP6] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP0] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP5] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP1] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP3] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP7] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48] INFO:     127.0.0.1:41854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:42186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:46376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP2] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP4] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP6] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP0] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP5] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP1] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP3] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP7] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48] INFO:     127.0.0.1:40876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:41240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:45146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:45256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:48] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP2] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP6] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP4] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP0] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP5] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP1] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP3] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:48 TP7] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:40810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:44120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP2] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP4] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP6] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP0] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP5] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP1] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP3] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP7] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP2] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP6] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP4] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP0] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP5] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP1] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP3] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP7] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:41660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:46064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP2] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP6] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP4] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP0] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP5] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP1] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP3] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP7] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49] INFO:     127.0.0.1:40836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:46898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP2] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP6] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP4] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP0] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP5] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP1] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP3] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP7] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:44586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:40664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP2] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP6] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP4] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP0] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP5] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP1] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP3] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP7] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49] INFO:     127.0.0.1:40408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP2] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP6] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP4] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP0] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP5] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP1] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP3] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP7] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP2] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP6] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP4] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP0] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP5] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP1] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP3] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49 TP7] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:49] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:49] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP2] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP6] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP4] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP0] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP5] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP1] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP3] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP7] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP2] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP6] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP4] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP5] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP0] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP7] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP1] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP3] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50] INFO:     127.0.0.1:41706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:43706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP2] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP6] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP4] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP0] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP5] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP3] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP1] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP7] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50] INFO:     127.0.0.1:41588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:42284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:43508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:43708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:46160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP2] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP6] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP4] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP0] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP5] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP1] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP3] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP7] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:42406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:46262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP2] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP4] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP6] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP0] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP5] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP1] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP3] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP7] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50] INFO:     127.0.0.1:40406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:44884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP0] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP2] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP3] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP1] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP5] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP7] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP4] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP6] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:47954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP1] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP5] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP2] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP0] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP6] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP4] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP3] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP7] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50] INFO:     127.0.0.1:43452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP5] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP1] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP2] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP6] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP0] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP4] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP3] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50 TP7] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:50] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:42172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:44206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:44612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:46334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:50] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP5] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP2] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP6] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP0] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP4] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP1] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP3] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP7] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51] INFO:     127.0.0.1:41214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:41708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:43990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:46210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP2] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP6] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP4] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP0] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP5] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP1] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP3] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP7] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:42602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:45914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:40564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:42674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:43286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:45590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:45828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:47740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP2] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP6] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP4] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP0] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP5] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP1] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP3] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP7] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP2] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP4] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP6] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP5] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP0] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP1] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP3] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP7] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:45514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:60638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:32890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:46472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:41732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:40476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:40886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:41486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:42882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:43602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:45812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:46756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:46810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP2] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP4] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP6] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP0] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP5] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP1] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP3] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51 TP7] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:51] INFO:     127.0.0.1:40726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:44788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:60794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:51] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:44360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:44848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:45184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:45540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:32824 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP2] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP4] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP6] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP0] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP5] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP1] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP3] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP7] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52] INFO:     127.0.0.1:41786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:60806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:60846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:40498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:41094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:42870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:43942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:33020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP2] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP6] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP4] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP0] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP5] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP1] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP3] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP7] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52] INFO:     127.0.0.1:40548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:43066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:44940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP2] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP6] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP4] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP0] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP5] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP1] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP3] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP7] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:45380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:42246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:42632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:59392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP2] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP4] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP6] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP0] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP5] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP1] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP3] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP7] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52] INFO:     127.0.0.1:40516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:45684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:40614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:34082 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP2] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP6] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP4] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP0] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP5] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP1] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP3] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP7] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP0] Decode batch [540], #running-req: 617, #token: 93914, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6618.96, #queue-req: 0, 
[2025-10-25 15:33:52] INFO:     127.0.0.1:40508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:46134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:52] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP2] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP6] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP4] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP0] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP5] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP1] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP3] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:52 TP7] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:33356 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP2] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP6] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP4] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP0] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP5] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP1] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP3] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP7] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53] INFO:     127.0.0.1:40582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP2] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP4] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP6] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP5] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP0] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP1] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP3] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP7] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53] INFO:     127.0.0.1:40722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:41418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:33378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP2] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP4] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP6] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP0] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP5] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP1] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP3] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP7] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53] INFO:     127.0.0.1:40478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP2] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP4] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP6] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP0] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP5] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP1] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP3] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP7] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:45674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:41894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP2] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP4] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP6] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP0] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP5] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP1] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP3] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP7] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53] INFO:     127.0.0.1:41964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:32980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:34240 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP2] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP4] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP6] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP0] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP5] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP1] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP3] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP7] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP4] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP2] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP6] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP0] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP5] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP1] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP3] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53 TP7] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:33:53] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:45414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:45708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:43980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:53] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:43548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:46592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:40452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:40688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:46508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:32878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:40372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:45154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:46422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:41466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:43444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:45604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:45010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:59644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:41908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:43564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:46300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:40532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:34208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:40906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:42130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:33702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:54] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:43112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:40922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:43520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:41650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:40598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:41682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:46462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:32906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:41636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:44302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:43974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55 TP0] Decode batch [580], #running-req: 290, #token: 54450, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6033.01, #queue-req: 0, 
[2025-10-25 15:33:55] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:41028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:41950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:43888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:45774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:42222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:55] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:41524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:42238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:42504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:47472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:59692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:40612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:43366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:43024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:47384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:42280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:41972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:46014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:46696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:42556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:32800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:45468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:41394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:44678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:44756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:56] INFO:     127.0.0.1:33510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:45650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:48064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:42656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:34188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:34032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:43238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:47258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:34102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:41504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:46886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:40776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57 TP0] Decode batch [620], #running-req: 117, #token: 28088, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3882.53, #queue-req: 0, 
[2025-10-25 15:33:57] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:45922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:33806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:34074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:57] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:43470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:45170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:41802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:42858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:46964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:42952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:44238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:44542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:41380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:60186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:43128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:41600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:60786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:32772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:41872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:34296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:43256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:46594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:58] INFO:     127.0.0.1:34144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:42992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59 TP0] Decode batch [660], #running-req: 33, #token: 9681, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1879.05, #queue-req: 0, 
[2025-10-25 15:33:59] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:33298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:34174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:33716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:34196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:43936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:41408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:60902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:33024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:33:59] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:60748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:60510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00 TP0] Decode batch [700], #running-req: 9, #token: 3721, token usage: 0.00, cuda graph: True, gen throughput (token/s): 751.58, #queue-req: 0, 
[2025-10-25 15:34:00] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:00] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:01 TP0] Decode batch [740], #running-req: 2, #token: 1355, token usage: 0.00, cuda graph: True, gen throughput (token/s): 207.84, #queue-req: 0, 
[2025-10-25 15:34:01] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:01 TP0] Decode batch [780], #running-req: 1, #token: 1104, token usage: 0.00, cuda graph: True, gen throughput (token/s): 59.69, #queue-req: 0, 
[2025-10-25 15:34:02] INFO:     127.0.0.1:42158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:15] INFO:     127.0.0.1:42444 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-25 15:34:15 TP0] Prefill batch [798], #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:34:15] INFO:     127.0.0.1:42452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:15 TP0] Prefill batch [799], #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:34:15 TP0] Prefill batch [800], #new-seq: 39, #new-token: 39, #cached-token: 28305, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:34:15 TP0] Prefill batch [801], #new-seq: 48, #new-token: 48, #cached-token: 34922, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-25 15:34:15 TP0] Prefill batch [802], #new-seq: 49, #new-token: 49, #cached-token: 35540, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[2025-10-25 15:34:15 TP0] Prefill batch [803], #new-seq: 58, #new-token: 58, #cached-token: 42627, token usage: 0.01, #running-req: 137, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP0] Prefill batch [804], #new-seq: 60, #new-token: 60, #cached-token: 43540, token usage: 0.02, #running-req: 195, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP5] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP1] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP4] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP7] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP0] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP2] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP3] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:15 TP6] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP0] Prefill batch [805], #new-seq: 59, #new-token: 59, #cached-token: 42899, token usage: 0.02, #running-req: 255, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP5] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP7] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP4] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP6] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP1] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP0] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP3] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP2] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP0] Prefill batch [806], #new-seq: 66, #new-token: 66, #cached-token: 48066, token usage: 0.02, #running-req: 314, #queue-req: 0, 
[2025-10-25 15:34:16 TP0] Prefill batch [807], #new-seq: 64, #new-token: 64, #cached-token: 46843, token usage: 0.03, #running-req: 380, #queue-req: 0, 
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP7] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP5] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP4] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP6] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP2] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP3] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP1] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:34:16 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:34:16 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:34:16 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:34:16 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:34:16 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:34:16 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP0] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:34:16 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-25 15:34:16 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:16 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:16 TP0] Prefill batch [808], #new-seq: 71, #new-token: 71, #cached-token: 51389, token usage: 0.03, #running-req: 444, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP0] Prefill batch [809], #new-seq: 70, #new-token: 70, #cached-token: 50797, token usage: 0.04, #running-req: 515, #queue-req: 0, 
[2025-10-25 15:34:16 TP0] Prefill batch [810], #new-seq: 76, #new-token: 76, #cached-token: 55326, token usage: 0.04, #running-req: 585, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:16 TP0] Prefill batch [811], #new-seq: 74, #new-token: 74, #cached-token: 53879, token usage: 0.05, #running-req: 661, #queue-req: 0, 
[2025-10-25 15:34:17 TP0] Prefill batch [812], #new-seq: 32, #new-token: 32, #cached-token: 23252, token usage: 0.05, #running-req: 735, #queue-req: 0, 
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP1] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:17 TP2] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:17 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:17 TP0] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:17 TP3] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:17 TP7] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:17 TP5] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:17 TP4] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-25 15:34:17 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP6] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-25 15:34:17 TP0] Prefill batch [813], #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.05, #running-req: 767, #queue-req: 0, 
[2025-10-25 15:34:17 TP0] Prefill batch [814], #new-seq: 46, #new-token: 46, #cached-token: 33750, token usage: 0.05, #running-req: 768, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP0] Prefill batch [815], #new-seq: 47, #new-token: 47, #cached-token: 34175, token usage: 0.05, #running-req: 814, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP3] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP1] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP0] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP2] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP5] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP4] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP7] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP6] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP0] Prefill batch [816], #new-seq: 64, #new-token: 64, #cached-token: 46382, token usage: 0.06, #running-req: 861, #queue-req: 0, 
[2025-10-25 15:34:17 TP0] Prefill batch [817], #new-seq: 53, #new-token: 53, #cached-token: 38995, token usage: 0.06, #running-req: 925, #queue-req: 0, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:17 TP0] Prefill batch [818], #new-seq: 46, #new-token: 46, #cached-token: 33799, token usage: 0.06, #running-req: 978, #queue-req: 25, 
[2025-10-25 15:34:20 TP0] Decode batch [841], #running-req: 1024, #token: 85325, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1208.56, #queue-req: 295, 
[2025-10-25 15:34:21] INFO:     127.0.0.1:45366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:21 TP0] Prefill batch [847], #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-25 15:34:21] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:22 TP0] Prefill batch [854], #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-25 15:34:22] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:22 TP0] Prefill batch [856], #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-25 15:34:22] INFO:     127.0.0.1:42680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:22] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:22] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23 TP0] Prefill batch [863], #new-seq: 3, #new-token: 3, #cached-token: 2254, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[2025-10-25 15:34:23] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:48236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23 TP0] Prefill batch [865], #new-seq: 8, #new-token: 8, #cached-token: 5906, token usage: 0.11, #running-req: 1016, #queue-req: 281, 
[2025-10-25 15:34:23] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:43556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23 TP0] Prefill batch [867], #new-seq: 4, #new-token: 4, #cached-token: 2927, token usage: 0.11, #running-req: 1020, #queue-req: 277, 
[2025-10-25 15:34:23] INFO:     127.0.0.1:42724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:42902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:44630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:46980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:23 TP0] Prefill batch [869], #new-seq: 10, #new-token: 10, #cached-token: 7195, token usage: 0.11, #running-req: 1014, #queue-req: 267, 
[2025-10-25 15:34:24] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:45802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24 TP0] Prefill batch [871], #new-seq: 5, #new-token: 5, #cached-token: 3659, token usage: 0.11, #running-req: 1019, #queue-req: 262, 
[2025-10-25 15:34:24] INFO:     127.0.0.1:43194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24 TP0] Prefill batch [873], #new-seq: 6, #new-token: 6, #cached-token: 4310, token usage: 0.11, #running-req: 1018, #queue-req: 256, 
[2025-10-25 15:34:24] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24 TP0] Prefill batch [875], #new-seq: 2, #new-token: 2, #cached-token: 1417, token usage: 0.11, #running-req: 1022, #queue-req: 254, 
[2025-10-25 15:34:24] INFO:     127.0.0.1:42824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:44650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24 TP0] Prefill batch [877], #new-seq: 9, #new-token: 9, #cached-token: 6610, token usage: 0.11, #running-req: 1015, #queue-req: 245, 
[2025-10-25 15:34:24] INFO:     127.0.0.1:42468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:44558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:45538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:24] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25 TP0] Prefill batch [879], #new-seq: 8, #new-token: 8, #cached-token: 5912, token usage: 0.11, #running-req: 1016, #queue-req: 237, 
[2025-10-25 15:34:25 TP0] Prefill batch [881], #new-seq: 7, #new-token: 7, #cached-token: 5132, token usage: 0.11, #running-req: 1017, #queue-req: 230, 
[2025-10-25 15:34:25] INFO:     127.0.0.1:43358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:47160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:47536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:48160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:51496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25 TP0] Prefill batch [883], #new-seq: 7, #new-token: 7, #cached-token: 5132, token usage: 0.12, #running-req: 1017, #queue-req: 223, 
[2025-10-25 15:34:25] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:46704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:51510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25 TP0] Prefill batch [885], #new-seq: 6, #new-token: 6, #cached-token: 4334, token usage: 0.12, #running-req: 1018, #queue-req: 217, 
[2025-10-25 15:34:25] INFO:     127.0.0.1:43344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:43820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:45600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:46420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25 TP0] Prefill batch [887], #new-seq: 8, #new-token: 8, #cached-token: 5752, token usage: 0.12, #running-req: 1016, #queue-req: 209, 
[2025-10-25 15:34:25] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:43334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:44216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:45800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:46086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:47806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:25] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26 TP0] Prefill batch [889], #new-seq: 11, #new-token: 11, #cached-token: 7977, token usage: 0.12, #running-req: 1013, #queue-req: 198, 
[2025-10-25 15:34:26] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:48630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:48854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:49328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26 TP0] Prefill batch [891], #new-seq: 7, #new-token: 7, #cached-token: 5116, token usage: 0.12, #running-req: 1017, #queue-req: 191, 
[2025-10-25 15:34:26] INFO:     127.0.0.1:42458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:44258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:44994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26 TP0] Prefill batch [893], #new-seq: 7, #new-token: 7, #cached-token: 5086, token usage: 0.12, #running-req: 1017, #queue-req: 184, 
[2025-10-25 15:34:26] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:44372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:48392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26 TP0] Prefill batch [895], #new-seq: 8, #new-token: 8, #cached-token: 5775, token usage: 0.12, #running-req: 1016, #queue-req: 176, 
[2025-10-25 15:34:26] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:44474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:44658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:44688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:45488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:47612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26 TP0] Prefill batch [897], #new-seq: 9, #new-token: 9, #cached-token: 6493, token usage: 0.12, #running-req: 1015, #queue-req: 167, 
[2025-10-25 15:34:26] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:26] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27 TP0] Prefill batch [899], #new-seq: 5, #new-token: 5, #cached-token: 3596, token usage: 0.12, #running-req: 1019, #queue-req: 162, 
[2025-10-25 15:34:27] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:46906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:49108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27 TP0] Prefill batch [901], #new-seq: 9, #new-token: 9, #cached-token: 6528, token usage: 0.12, #running-req: 1015, #queue-req: 153, 
[2025-10-25 15:34:27] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27 TP0] Prefill batch [903], #new-seq: 10, #new-token: 10, #cached-token: 7355, token usage: 0.12, #running-req: 1014, #queue-req: 143, 
[2025-10-25 15:34:27] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:43996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:44520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:47598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27 TP0] Prefill batch [905], #new-seq: 10, #new-token: 10, #cached-token: 7307, token usage: 0.12, #running-req: 1014, #queue-req: 133, 
[2025-10-25 15:34:27 TP0] Decode batch [905], #running-req: 1014, #token: 117970, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5665.05, #queue-req: 133, 
[2025-10-25 15:34:27] INFO:     127.0.0.1:43262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:44270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:48874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:27 TP0] Prefill batch [907], #new-seq: 9, #new-token: 9, #cached-token: 6547, token usage: 0.12, #running-req: 1015, #queue-req: 124, 
[2025-10-25 15:34:28] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:44864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:48768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28 TP0] Prefill batch [909], #new-seq: 9, #new-token: 9, #cached-token: 6657, token usage: 0.12, #running-req: 1015, #queue-req: 115, 
[2025-10-25 15:34:28] INFO:     127.0.0.1:43238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:46034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:50642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28 TP0] Prefill batch [911], #new-seq: 13, #new-token: 13, #cached-token: 9630, token usage: 0.12, #running-req: 1011, #queue-req: 102, 
[2025-10-25 15:34:28] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:44064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:44798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:45356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:46274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:49828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28 TP0] Prefill batch [913], #new-seq: 19, #new-token: 19, #cached-token: 13761, token usage: 0.12, #running-req: 1005, #queue-req: 83, 
[2025-10-25 15:34:28] INFO:     127.0.0.1:43138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:44002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:46266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:48450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28 TP0] Prefill batch [915], #new-seq: 11, #new-token: 11, #cached-token: 7994, token usage: 0.12, #running-req: 1013, #queue-req: 72, 
[2025-10-25 15:34:28] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:43880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:44886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:45128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:28 TP0] Prefill batch [917], #new-seq: 14, #new-token: 14, #cached-token: 10354, token usage: 0.12, #running-req: 1010, #queue-req: 58, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:28 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:28 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:28 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:28 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:28 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:28 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:28 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:28 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:29] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:44204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:44602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:45892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:46020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29 TP0] Prefill batch [919], #new-seq: 19, #new-token: 19, #cached-token: 13955, token usage: 0.12, #running-req: 1005, #queue-req: 39, 
[2025-10-25 15:34:29] INFO:     127.0.0.1:43546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:46950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29 TP0] Prefill batch [921], #new-seq: 11, #new-token: 11, #cached-token: 7995, token usage: 0.12, #running-req: 1013, #queue-req: 28, 
[2025-10-25 15:34:29] INFO:     127.0.0.1:44480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29 TP0] Prefill batch [923], #new-seq: 12, #new-token: 12, #cached-token: 8763, token usage: 0.13, #running-req: 1012, #queue-req: 16, 
[2025-10-25 15:34:29] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:47288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:51574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29 TP0] Prefill batch [925], #new-seq: 13, #new-token: 13, #cached-token: 9462, token usage: 0.13, #running-req: 1011, #queue-req: 3, 
[2025-10-25 15:34:29] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:29] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30 TP0] Prefill batch [927], #new-seq: 3, #new-token: 3, #cached-token: 2163, token usage: 0.13, #running-req: 1015, #queue-req: 0, 
[2025-10-25 15:34:30] INFO:     127.0.0.1:43810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:46032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:42660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:44012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP1] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP5] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP4] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP3] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP7] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP0] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP2] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP6] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30] INFO:     127.0.0.1:44096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:44514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:44940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:46958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP1] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP3] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP5] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP4] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP7] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP0] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP2] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP6] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:44926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:45162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:46754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:47546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:44366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:45214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:50488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP1] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP5] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP3] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP7] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP4] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP0] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP2] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP6] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP5] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP3] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP1] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP4] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP7] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP0] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP2] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30 TP6] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:30] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:42988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:43068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:43376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:45790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:47036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:47856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:30] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP1] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP5] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP3] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP4] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP7] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP0] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP6] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP2] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:43796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:44550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:44620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:45644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:46038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP1] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP3] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP5] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP4] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP7] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP0] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP2] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP6] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:48122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:49962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:43922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:45088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:44324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:48830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:51270 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP5] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP1] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP3] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP4] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP7] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP0] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP2] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP6] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31] INFO:     127.0.0.1:42500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:43222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:45544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP5] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP3] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP1] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP4] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP7] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP0] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP2] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP6] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:44972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:46334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:47878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:48010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:49008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:51634 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP1] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP3] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP5] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP4] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP7] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP0] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP2] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP6] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:43754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:44910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:31] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP1] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP5] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP3] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP4] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP7] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP0] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP2] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:31 TP6] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32] INFO:     127.0.0.1:42994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:45464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:49458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP1] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP5] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP3] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP4] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP7] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP0] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP2] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP6] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:45916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:51492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:49396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP5] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP1] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP4] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP3] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP7] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP0] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP2] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP6] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:42856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:42966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP1] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP3] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP5] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP7] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP4] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP0] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP2] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP6] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32] INFO:     127.0.0.1:45042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:49522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP1] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP3] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP4] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP5] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP7] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP0] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP2] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP6] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32] INFO:     127.0.0.1:42834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:47714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP5] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP1] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP3] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP4] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP7] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP0] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP2] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP6] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:51832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:52314 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP1] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP3] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP5] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP4] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP7] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP0] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP2] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP6] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:43952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:45674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:32] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP1] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP3] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP5] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP7] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP4] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP0] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP2] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:32 TP6] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:44358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:49996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51398 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP5] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP1] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP3] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP7] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP4] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP0] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP2] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP6] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51348 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP1] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP5] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP3] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP4] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP7] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP0] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP2] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP6] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP5] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP3] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP1] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP4] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP7] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP0] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP2] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP6] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP1] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP5] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP4] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP7] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP3] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP0] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP2] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP6] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:43524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:45766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:52230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP1] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP5] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP4] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP3] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP7] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP0] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP2] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP6] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:44826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:45232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP1] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP5] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP3] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP4] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP7] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP0] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP2] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP6] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP0] Decode batch [957], #running-req: 723, #token: 101740, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6115.02, #queue-req: 0, 
[2025-10-25 15:34:33] INFO:     127.0.0.1:42540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:43578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:45718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:50352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP1] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP5] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP4] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP3] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP7] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP0] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP2] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP6] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:46082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP3] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP1] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP7] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP5] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP4] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP0] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP2] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP6] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:33] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP5] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP1] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP3] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP4] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:33 TP7] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP0] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP2] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP6] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:45774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP1] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP5] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP3] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP4] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP7] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP0] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP6] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP2] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34] INFO:     127.0.0.1:44682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:49058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:42644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:45248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:45514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:50250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:52424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP5] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP1] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP4] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP3] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP7] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP0] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP2] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP6] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34] INFO:     127.0.0.1:43562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:42952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:44666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:45636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:46214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:47180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP1] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP5] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP3] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP4] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP7] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP0] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP2] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP6] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:43048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:45228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP1] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP3] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP5] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP4] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP7] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP0] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP2] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP6] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:44182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:45106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:45622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:46692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:50564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP4] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP1] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP5] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP3] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP7] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP0] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP2] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP6] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:43226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:34] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP5] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP4] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP1] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP3] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP7] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP0] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP2] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:34 TP6] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35] INFO:     127.0.0.1:43018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:43164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP1] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP5] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP4] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP3] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP7] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP0] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP2] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP6] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35] INFO:     127.0.0.1:43178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP5] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP1] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP3] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP7] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP4] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP0] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP2] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP6] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP1] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP5] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP3] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP4] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP7] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP0] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP2] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP6] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35] INFO:     127.0.0.1:42556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:43328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:43808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:47176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:49098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:51648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:49684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:51806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:53602 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP1] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP5] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP3] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP7] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP4] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP0] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP2] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35 TP6] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:35] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:47122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:47146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:35] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:49916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:43042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:44982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:47356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:47558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:42674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:44452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:45664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:45832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:49198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:45980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:46092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:47778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:44282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:48252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:36] INFO:     127.0.0.1:53748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:49976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37 TP0] Decode batch [997], #running-req: 358, #token: 62937, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6104.55, #queue-req: 0, 
[2025-10-25 15:34:37] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:46740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:46462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:48502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:43932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:47460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:45584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:45870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:48356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:43904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:50022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:54300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:45742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:48192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:44732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:37] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:44036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:46284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:42758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:45658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:46066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:51548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:51708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:43418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:44154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:44498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:44768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:45004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:46410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:47444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:47720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:48064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:38] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:46384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39 TP0] Decode batch [1037], #running-req: 152, #token: 33788, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4528.58, #queue-req: 0, 
[2025-10-25 15:34:39] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:45472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:54084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:46118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:43502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:51760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:45170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:45710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:43892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:39] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:48418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:45512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:45044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:43628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:47236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:52944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:43744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:43280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40 TP0] Decode batch [1077], #running-req: 46, #token: 12796, token usage: 0.01, cuda graph: True, gen throughput (token/s): 2299.02, #queue-req: 0, 
[2025-10-25 15:34:40] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:46934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:40] INFO:     127.0.0.1:53758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:50950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:43864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:46230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:44076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:45266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:41 TP0] Decode batch [1117], #running-req: 13, #token: 4953, token usage: 0.01, cuda graph: True, gen throughput (token/s): 916.32, #queue-req: 0, 
[2025-10-25 15:34:41] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:49556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:42 TP0] Decode batch [1157], #running-req: 1, #token: 950, token usage: 0.00, cuda graph: True, gen throughput (token/s): 265.12, #queue-req: 0, 
[2025-10-25 15:34:43] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:55] INFO:     127.0.0.1:40590 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-25 15:34:56 TP0] Prefill batch [1175], #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:34:56] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:34:56 TP0] Prefill batch [1176], #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:34:56 TP0] Prefill batch [1177], #new-seq: 23, #new-token: 23, #cached-token: 16706, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP1] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP3] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP0] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP5] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP4] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP2] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP7] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP6] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP0] Prefill batch [1179], #new-seq: 5, #new-token: 5, #cached-token: 3670, token usage: 0.00, #running-req: 24, #queue-req: 0, 
[2025-10-25 15:34:56 TP0] Prefill batch [1180], #new-seq: 52, #new-token: 52, #cached-token: 37745, token usage: 0.01, #running-req: 29, #queue-req: 0, 
[2025-10-25 15:34:56 TP0] Prefill batch [1181], #new-seq: 48, #new-token: 48, #cached-token: 34907, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-25 15:34:56 TP0] Prefill batch [1182], #new-seq: 69, #new-token: 69, #cached-token: 50096, token usage: 0.01, #running-req: 129, #queue-req: 0, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP0] Prefill batch [1183], #new-seq: 52, #new-token: 52, #cached-token: 37793, token usage: 0.02, #running-req: 198, #queue-req: 0, 
[2025-10-25 15:34:56 TP0] Prefill batch [1184], #new-seq: 75, #new-token: 75, #cached-token: 54878, token usage: 0.02, #running-req: 250, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP1] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP2] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP3] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP0] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP5] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP4] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP7] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP6] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:56 TP0] Prefill batch [1185], #new-seq: 57, #new-token: 57, #cached-token: 41518, token usage: 0.02, #running-req: 325, #queue-req: 0, 
[2025-10-25 15:34:57 TP0] Prefill batch [1186], #new-seq: 82, #new-token: 82, #cached-token: 59925, token usage: 0.03, #running-req: 382, #queue-req: 0, 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP1] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP0] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP2] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP3] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP5] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP4] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP7] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP6] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP0] Prefill batch [1187], #new-seq: 62, #new-token: 62, #cached-token: 44928, token usage: 0.03, #running-req: 464, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP1] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP5] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP4] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP7] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP2] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP3] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP6] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP0] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP0] Prefill batch [1188], #new-seq: 33, #new-token: 33, #cached-token: 23875, token usage: 0.03, #running-req: 526, #queue-req: 0, 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP1] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP3] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP2] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP5] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP4] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP7] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP6] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP0] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP0] Prefill batch [1189], #new-seq: 4, #new-token: 4, #cached-token: 2886, token usage: 0.03, #running-req: 559, #queue-req: 0, 
[2025-10-25 15:34:57 TP0] Prefill batch [1190], #new-seq: 47, #new-token: 47, #cached-token: 34303, token usage: 0.04, #running-req: 563, #queue-req: 0, 
[2025-10-25 15:34:57 TP0] Prefill batch [1191], #new-seq: 47, #new-token: 47, #cached-token: 34109, token usage: 0.04, #running-req: 610, #queue-req: 0, 
[2025-10-25 15:34:57 TP0] Prefill batch [1192], #new-seq: 60, #new-token: 60, #cached-token: 43660, token usage: 0.04, #running-req: 657, #queue-req: 0, 
[2025-10-25 15:34:57 TP0] Prefill batch [1193], #new-seq: 55, #new-token: 55, #cached-token: 40043, token usage: 0.05, #running-req: 717, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP1] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP3] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP2] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP0] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP7] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP5] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP4] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:57 TP6] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:58 TP0] Prefill batch [1194], #new-seq: 69, #new-token: 69, #cached-token: 50583, token usage: 0.05, #running-req: 772, #queue-req: 0, 
[2025-10-25 15:34:58 TP0] Prefill batch [1195], #new-seq: 62, #new-token: 62, #cached-token: 44863, token usage: 0.06, #running-req: 841, #queue-req: 0, 
[2025-10-25 15:34:58 TP0] Prefill batch [1196], #new-seq: 77, #new-token: 77, #cached-token: 56335, token usage: 0.06, #running-req: 903, #queue-req: 0, 
[2025-10-25 15:34:58 TP0] Prefill batch [1197], #new-seq: 44, #new-token: 44, #cached-token: 32198, token usage: 0.06, #running-req: 980, #queue-req: 27, 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:58 TP2] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:58 TP1] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:58 TP5] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:58 TP3] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:58 TP0] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:58 TP4] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:58 TP7] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:34:58 TP6] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:01 TP0] Decode batch [1219], #running-req: 1024, #token: 84148, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1163.13, #queue-req: 295, 
[2025-10-25 15:35:01] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:02 TP0] Prefill batch [1226], #new-seq: 1, #new-token: 1, #cached-token: 790, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-25 15:35:02] INFO:     127.0.0.1:43518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:02] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:02 TP0] Prefill batch [1231], #new-seq: 1, #new-token: 1, #cached-token: 731, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-25 15:35:02 TP0] Prefill batch [1233], #new-seq: 1, #new-token: 1, #cached-token: 776, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-25 15:35:03] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:03] INFO:     127.0.0.1:43508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:03] INFO:     127.0.0.1:44836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:03] INFO:     127.0.0.1:45146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:03 TP0] Prefill batch [1241], #new-seq: 1, #new-token: 1, #cached-token: 736, token usage: 0.11, #running-req: 1023, #queue-req: 291, 
[2025-10-25 15:35:03] INFO:     127.0.0.1:41878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:03] INFO:     127.0.0.1:44746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:03 TP0] Prefill batch [1243], #new-seq: 5, #new-token: 5, #cached-token: 3686, token usage: 0.11, #running-req: 1019, #queue-req: 286, 
[2025-10-25 15:35:04] INFO:     127.0.0.1:41552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:44134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:46060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04 TP0] Prefill batch [1245], #new-seq: 7, #new-token: 7, #cached-token: 5251, token usage: 0.11, #running-req: 1017, #queue-req: 279, 
[2025-10-25 15:35:04] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04 TP0] Prefill batch [1247], #new-seq: 3, #new-token: 3, #cached-token: 2196, token usage: 0.11, #running-req: 1021, #queue-req: 276, 
[2025-10-25 15:35:04] INFO:     127.0.0.1:42092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:42528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:45254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04 TP0] Prefill batch [1249], #new-seq: 7, #new-token: 7, #cached-token: 5032, token usage: 0.11, #running-req: 1017, #queue-req: 269, 
[2025-10-25 15:35:04] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:43802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:45378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:49056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04 TP0] Prefill batch [1251], #new-seq: 5, #new-token: 5, #cached-token: 3635, token usage: 0.11, #running-req: 1019, #queue-req: 264, 
[2025-10-25 15:35:04] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:41746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:45526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:47196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:04] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05 TP0] Prefill batch [1253], #new-seq: 8, #new-token: 8, #cached-token: 5729, token usage: 0.11, #running-req: 1016, #queue-req: 256, 
[2025-10-25 15:35:05] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05 TP0] Prefill batch [1255], #new-seq: 2, #new-token: 2, #cached-token: 1470, token usage: 0.11, #running-req: 1022, #queue-req: 254, 
[2025-10-25 15:35:05] INFO:     127.0.0.1:40620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:41788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:45032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:46264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:47954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05 TP0] Prefill batch [1257], #new-seq: 12, #new-token: 12, #cached-token: 8723, token usage: 0.11, #running-req: 1012, #queue-req: 242, 
[2025-10-25 15:35:05] INFO:     127.0.0.1:40838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:48610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:48930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:49328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05 TP0] Prefill batch [1259], #new-seq: 9, #new-token: 9, #cached-token: 6627, token usage: 0.11, #running-req: 1015, #queue-req: 233, 
[2025-10-25 15:35:05] INFO:     127.0.0.1:42554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:43910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:48012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05 TP0] Prefill batch [1261], #new-seq: 6, #new-token: 6, #cached-token: 4484, token usage: 0.11, #running-req: 1018, #queue-req: 227, 
[2025-10-25 15:35:05] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:05] INFO:     127.0.0.1:45962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06 TP0] Prefill batch [1263], #new-seq: 6, #new-token: 6, #cached-token: 4379, token usage: 0.12, #running-req: 1018, #queue-req: 221, 
[2025-10-25 15:35:06] INFO:     127.0.0.1:42314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:42588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:44494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:49272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06 TP0] Prefill batch [1265], #new-seq: 7, #new-token: 7, #cached-token: 5104, token usage: 0.12, #running-req: 1017, #queue-req: 214, 
[2025-10-25 15:35:06] INFO:     127.0.0.1:41590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:42276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:43124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:43530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:46754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06 TP0] Prefill batch [1267], #new-seq: 9, #new-token: 9, #cached-token: 6404, token usage: 0.12, #running-req: 1015, #queue-req: 205, 
[2025-10-25 15:35:06] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:43080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:43798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:45162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06 TP0] Prefill batch [1269], #new-seq: 13, #new-token: 13, #cached-token: 9492, token usage: 0.12, #running-req: 1011, #queue-req: 192, 
[2025-10-25 15:35:06] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06] INFO:     127.0.0.1:47160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:06 TP0] Prefill batch [1271], #new-seq: 5, #new-token: 5, #cached-token: 3628, token usage: 0.12, #running-req: 1019, #queue-req: 187, 
[2025-10-25 15:35:07] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:44950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07 TP0] Prefill batch [1273], #new-seq: 8, #new-token: 8, #cached-token: 5825, token usage: 0.12, #running-req: 1016, #queue-req: 179, 
[2025-10-25 15:35:07] INFO:     127.0.0.1:41254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:41858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:41874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:42618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:43010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:44690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07 TP0] Prefill batch [1275], #new-seq: 8, #new-token: 8, #cached-token: 5814, token usage: 0.12, #running-req: 1016, #queue-req: 171, 
[2025-10-25 15:35:07] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:43804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:45350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:46314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07 TP0] Prefill batch [1277], #new-seq: 8, #new-token: 8, #cached-token: 5670, token usage: 0.12, #running-req: 1016, #queue-req: 163, 
[2025-10-25 15:35:07] INFO:     127.0.0.1:41320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:48136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07 TP0] Prefill batch [1279], #new-seq: 9, #new-token: 9, #cached-token: 6523, token usage: 0.12, #running-req: 1015, #queue-req: 154, 
[2025-10-25 15:35:07] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:46948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:07 TP0] Prefill batch [1281], #new-seq: 4, #new-token: 4, #cached-token: 2939, token usage: 0.12, #running-req: 1020, #queue-req: 150, 
[2025-10-25 15:35:08] INFO:     127.0.0.1:40742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:42290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:43562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:44830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:47782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08 TP0] Prefill batch [1283], #new-seq: 13, #new-token: 13, #cached-token: 9544, token usage: 0.12, #running-req: 1011, #queue-req: 137, 
[2025-10-25 15:35:08 TP0] Decode batch [1283], #running-req: 1011, #token: 117127, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5887.81, #queue-req: 137, 
[2025-10-25 15:35:08] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:42574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:45894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08 TP0] Prefill batch [1285], #new-seq: 10, #new-token: 10, #cached-token: 7249, token usage: 0.12, #running-req: 1014, #queue-req: 127, 
[2025-10-25 15:35:08] INFO:     127.0.0.1:42872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:47288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:47356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08 TP0] Prefill batch [1287], #new-seq: 7, #new-token: 7, #cached-token: 5179, token usage: 0.12, #running-req: 1017, #queue-req: 120, 
[2025-10-25 15:35:08] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:41678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08 TP0] Prefill batch [1289], #new-seq: 7, #new-token: 7, #cached-token: 5160, token usage: 0.12, #running-req: 1017, #queue-req: 113, 
[2025-10-25 15:35:08] INFO:     127.0.0.1:41650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:42992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:08 TP0] Prefill batch [1291], #new-seq: 12, #new-token: 12, #cached-token: 8912, token usage: 0.12, #running-req: 1012, #queue-req: 101, 
[2025-10-25 15:35:09] INFO:     127.0.0.1:41322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:41814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:42070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:43024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:47204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:49352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09 TP0] Prefill batch [1293], #new-seq: 17, #new-token: 17, #cached-token: 12280, token usage: 0.12, #running-req: 1007, #queue-req: 84, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:09 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:09 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:09 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:09 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:09 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:09 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:09 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:09 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:09] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:41324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:41484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:43572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09 TP0] Prefill batch [1295], #new-seq: 13, #new-token: 13, #cached-token: 9481, token usage: 0.12, #running-req: 1011, #queue-req: 71, 
[2025-10-25 15:35:09] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:41564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:42482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:43474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:46032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09 TP0] Prefill batch [1297], #new-seq: 13, #new-token: 13, #cached-token: 9600, token usage: 0.12, #running-req: 1011, #queue-req: 58, 
[2025-10-25 15:35:09] INFO:     127.0.0.1:41130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:43876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:43972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:45390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:45940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:46076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:46312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:46430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:47002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:47580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:49306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09 TP0] Prefill batch [1299], #new-seq: 19, #new-token: 19, #cached-token: 13955, token usage: 0.12, #running-req: 1005, #queue-req: 39, 
[2025-10-25 15:35:09] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:44566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:45182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:46188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:48880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:09] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10 TP0] Prefill batch [1301], #new-seq: 11, #new-token: 11, #cached-token: 7995, token usage: 0.13, #running-req: 1013, #queue-req: 28, 
[2025-10-25 15:35:10] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:42530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:43066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:47736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:49480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10 TP0] Prefill batch [1303], #new-seq: 11, #new-token: 11, #cached-token: 8029, token usage: 0.13, #running-req: 1013, #queue-req: 17, 
[2025-10-25 15:35:10] INFO:     127.0.0.1:40986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:41154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:41568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:42382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:42898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:44374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:44418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:47652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10 TP0] Prefill batch [1305], #new-seq: 15, #new-token: 15, #cached-token: 10937, token usage: 0.13, #running-req: 1009, #queue-req: 2, 
[2025-10-25 15:35:10] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:41620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:44010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:44252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:45080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10 TP0] Prefill batch [1307], #new-seq: 2, #new-token: 2, #cached-token: 1422, token usage: 0.13, #running-req: 1014, #queue-req: 0, 
[2025-10-25 15:35:10] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:10] INFO:     127.0.0.1:43984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:43724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP5] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP6] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP2] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP0] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP4] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP1] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP3] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP7] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:41536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP6] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP2] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP4] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP0] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP5] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP1] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP3] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP7] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:41820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46376 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP2] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP6] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP4] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP5] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP0] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP1] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP3] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP7] [fused_moe] using default for (975, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11] INFO:     127.0.0.1:41706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:43154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:43556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:41376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:43992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:49508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP6] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP2] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP4] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP5] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP0] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP1] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP3] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11 TP7] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:11] INFO:     127.0.0.1:40878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:44894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:11] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP2] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP6] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP4] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP5] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP0] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP1] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP3] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP7] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12] INFO:     127.0.0.1:41428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:41554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:42184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:46718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:46962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP6] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP2] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP4] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP0] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP5] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP1] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP3] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP7] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP2] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP6] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP4] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP5] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP0] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP1] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP3] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP7] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:42670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:46246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48886 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP6] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP4] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP2] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP0] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP5] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP1] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP3] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP7] [fused_moe] using default for (880, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12] INFO:     127.0.0.1:40674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:41980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:46508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:49130 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP2] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP6] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP4] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP5] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP0] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP1] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP3] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP7] [fused_moe] using default for (857, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48830 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP6] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP2] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP5] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP0] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP4] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP1] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP3] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP7] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:41768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:46878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:49344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:43436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:44372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:12] INFO:     127.0.0.1:48708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP2] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP6] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP4] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP0] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP5] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP1] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP3] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:12 TP7] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:40958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:41192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:42356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP2] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP6] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP4] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP5] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP0] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP1] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP3] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP7] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP2] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP6] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP4] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP0] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP5] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP1] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP3] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP7] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13] INFO:     127.0.0.1:41802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:41906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:48442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP6] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP5] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP2] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP0] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP4] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP1] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP3] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP7] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP5] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP6] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP2] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP0] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP4] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP1] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP3] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP7] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44920 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP5] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP6] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP2] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP0] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP4] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP1] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP3] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP7] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:49000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP5] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP2] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP6] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP0] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP4] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP1] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP3] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP7] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:41142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:49204 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP5] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP2] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP6] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP0] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP4] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP1] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP3] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13 TP7] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:13] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:43670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:45670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:13] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:41588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:46798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:43888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14 TP0] Decode batch [1336], #running-req: 742, #token: 105177, token usage: 0.11, cuda graph: False, gen throughput (token/s): 6106.59, #queue-req: 0, 
[2025-10-25 15:35:14] INFO:     127.0.0.1:40696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:43938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP5] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP2] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP6] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP0] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP4] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP1] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP3] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP7] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14] INFO:     127.0.0.1:41480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:43166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:43718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:46026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49256 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP5] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP2] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP6] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP0] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP4] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP1] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP3] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP7] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14] INFO:     127.0.0.1:41644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:48328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:48806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50408 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP5] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP2] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP6] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP0] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP4] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP1] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP3] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP7] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14] INFO:     127.0.0.1:40736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:47050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP2] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP6] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP5] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP0] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP4] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP1] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP3] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14 TP7] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:14] INFO:     127.0.0.1:40758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:43204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:46584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:40750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:43792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:47136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:14] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:40642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP6] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP2] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP5] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP0] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP4] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP1] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP7] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP3] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:41872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:42234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:45636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:48824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:48902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:42218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:46266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:43344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:50286 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP5] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP6] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP2] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP0] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP4] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP1] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP3] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP7] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:44466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:49574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:42496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:44646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:46230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:41488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:42366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:43786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:47898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:48114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:48222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:15] INFO:     127.0.0.1:51924 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP5] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP1] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP2] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP6] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP0] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP3] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP7] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:15 TP4] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16] INFO:     127.0.0.1:42246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP5] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP2] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP6] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP1] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP0] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP4] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP3] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP7] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16] INFO:     127.0.0.1:40690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP5] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP6] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP4] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP7] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP0] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP2] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP1] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP3] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:41882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP4] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP5] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP0] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP2] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP6] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP1] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP3] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16 TP7] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:16] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:41438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:41972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:44728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:40628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:41912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:41736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:42866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:43740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:45282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:46178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:16] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:42018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:43188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:46122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:41464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:44158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:43826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:44090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:45862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:46526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:48986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:45514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:41656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:41068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:42602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:46548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:47180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:47620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:47856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17 TP0] Decode batch [1376], #running-req: 352, #token: 60205, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6068.09, #queue-req: 0, 
[2025-10-25 15:35:17] INFO:     127.0.0.1:40754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:43756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:44552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:43450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:44514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:44610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:45764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:41672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:17] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:43274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:45222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:41072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:41676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:43838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:42424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:42376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:46592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:40662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:42886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:42944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:40856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:42342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:41526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:45118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:43054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:18] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:43068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:40652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:43096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:43952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:45742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:46098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:46892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:44884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:41890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:45124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:41846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:43464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:45832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:47622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:48244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:43388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:41242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:52094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19 TP0] Decode batch [1416], #running-req: 147, #token: 31926, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4482.99, #queue-req: 0, 
[2025-10-25 15:35:19] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:43234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:44624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:43616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:19] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:52032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:42408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:51760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:44102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:42512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:44416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:52008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:49760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:43716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:49406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:48962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:42292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:41938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:47148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:52190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:45034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:45594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:20] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:50494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:49684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21 TP0] Decode batch [1456], #running-req: 47, #token: 12115, token usage: 0.01, cuda graph: True, gen throughput (token/s): 2252.38, #queue-req: 0, 
[2025-10-25 15:35:21] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:41412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:21] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:41752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:43590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22 TP0] Decode batch [1496], #running-req: 16, #token: 5763, token usage: 0.01, cuda graph: True, gen throughput (token/s): 947.86, #queue-req: 0, 
[2025-10-25 15:35:22] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:44174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:47386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:22] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:23] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:23 TP0] Decode batch [1536], #running-req: 3, #token: 1691, token usage: 0.00, cuda graph: True, gen throughput (token/s): 312.74, #queue-req: 0, 
[2025-10-25 15:35:23] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:23] INFO:     127.0.0.1:49714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:23] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:36] INFO:     127.0.0.1:37386 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-25 15:35:36 TP0] Prefill batch [1566], #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:35:36] INFO:     127.0.0.1:37396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:36 TP0] Prefill batch [1567], #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:35:36 TP0] Prefill batch [1568], #new-seq: 40, #new-token: 40, #cached-token: 29032, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:35:37 TP0] Prefill batch [1569], #new-seq: 47, #new-token: 47, #cached-token: 34173, token usage: 0.01, #running-req: 41, #queue-req: 0, 
[2025-10-25 15:35:37 TP0] Prefill batch [1570], #new-seq: 50, #new-token: 50, #cached-token: 36621, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP0] Prefill batch [1571], #new-seq: 55, #new-token: 55, #cached-token: 40071, token usage: 0.01, #running-req: 138, #queue-req: 0, 
[2025-10-25 15:35:37 TP0] Prefill batch [1572], #new-seq: 53, #new-token: 53, #cached-token: 38626, token usage: 0.02, #running-req: 193, #queue-req: 0, 
[2025-10-25 15:35:37 TP0] Prefill batch [1576], #new-seq: 9, #new-token: 9, #cached-token: 6487, token usage: 0.02, #running-req: 246, #queue-req: 0, 
[2025-10-25 15:35:37 TP0] Prefill batch [1577], #new-seq: 68, #new-token: 68, #cached-token: 49300, token usage: 0.02, #running-req: 255, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:37 TP0] Prefill batch [1578], #new-seq: 50, #new-token: 50, #cached-token: 36570, token usage: 0.02, #running-req: 323, #queue-req: 0, 
[2025-10-25 15:35:37 TP0] Prefill batch [1579], #new-seq: 79, #new-token: 79, #cached-token: 57664, token usage: 0.03, #running-req: 373, #queue-req: 0, 
[2025-10-25 15:35:38 TP0] Prefill batch [1580], #new-seq: 55, #new-token: 55, #cached-token: 39846, token usage: 0.03, #running-req: 452, #queue-req: 0, 
[2025-10-25 15:35:38 TP0] Prefill batch [1581], #new-seq: 86, #new-token: 86, #cached-token: 62501, token usage: 0.04, #running-req: 507, #queue-req: 0, 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP2] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP0] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP1] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP4] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP5] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP3] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP7] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP6] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP0] Prefill batch [1582], #new-seq: 66, #new-token: 66, #cached-token: 47923, token usage: 0.04, #running-req: 593, #queue-req: 0, 
[2025-10-25 15:35:38 TP0] Prefill batch [1583], #new-seq: 87, #new-token: 87, #cached-token: 63323, token usage: 0.05, #running-req: 659, #queue-req: 0, 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP0] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP1] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP2] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP5] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP3] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP4] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP7] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP6] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP0] Prefill batch [1584], #new-seq: 70, #new-token: 70, #cached-token: 51209, token usage: 0.05, #running-req: 746, #queue-req: 0, 
[2025-10-25 15:35:38 TP0] Prefill batch [1585], #new-seq: 99, #new-token: 99, #cached-token: 71730, token usage: 0.06, #running-req: 816, #queue-req: 0, 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP2] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP1] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP0] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP5] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP7] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP4] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP3] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP6] [fused_moe] using default for (99, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:38 TP0] Prefill batch [1586], #new-seq: 68, #new-token: 68, #cached-token: 49930, token usage: 0.06, #running-req: 915, #queue-req: 0, 
[2025-10-25 15:35:39 TP0] Prefill batch [1587], #new-seq: 41, #new-token: 41, #cached-token: 30191, token usage: 0.06, #running-req: 983, #queue-req: 63, 
[2025-10-25 15:35:40 TP0] Decode batch [1595], #running-req: 1024, #token: 70704, token usage: 0.07, cuda graph: False, gen throughput (token/s): 472.90, #queue-req: 295, 
[2025-10-25 15:35:42] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:42 TP0] Prefill batch [1616], #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-25 15:35:43] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:43 TP0] Prefill batch [1620], #new-seq: 1, #new-token: 1, #cached-token: 709, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-25 15:35:43] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:43 TP0] Prefill batch [1622], #new-seq: 1, #new-token: 1, #cached-token: 735, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-25 15:35:43] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:43 TP0] Prefill batch [1627], #new-seq: 1, #new-token: 1, #cached-token: 729, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-10-25 15:35:44] INFO:     127.0.0.1:38142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44 TP0] Prefill batch [1629], #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 290, 
[2025-10-25 15:35:44] INFO:     127.0.0.1:37436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44 TP0] Prefill batch [1631], #new-seq: 2, #new-token: 2, #cached-token: 1569, token usage: 0.10, #running-req: 1022, #queue-req: 288, 
[2025-10-25 15:35:44] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44] INFO:     127.0.0.1:39330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44 TP0] Prefill batch [1633], #new-seq: 2, #new-token: 2, #cached-token: 1456, token usage: 0.11, #running-req: 1022, #queue-req: 286, 
[2025-10-25 15:35:44] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44] INFO:     127.0.0.1:40950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44] INFO:     127.0.0.1:41650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44 TP0] Prefill batch [1635], #new-seq: 4, #new-token: 4, #cached-token: 2938, token usage: 0.11, #running-req: 1020, #queue-req: 282, 
[2025-10-25 15:35:44] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44] INFO:     127.0.0.1:41764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44] INFO:     127.0.0.1:41894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:44 TP0] Prefill batch [1637], #new-seq: 4, #new-token: 4, #cached-token: 2934, token usage: 0.11, #running-req: 1020, #queue-req: 278, 
[2025-10-25 15:35:45] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:40116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45 TP0] Prefill batch [1639], #new-seq: 4, #new-token: 4, #cached-token: 2924, token usage: 0.11, #running-req: 1020, #queue-req: 274, 
[2025-10-25 15:35:45] INFO:     127.0.0.1:37746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:40274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:45606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45 TP0] Prefill batch [1641], #new-seq: 3, #new-token: 3, #cached-token: 2157, token usage: 0.11, #running-req: 1021, #queue-req: 271, 
[2025-10-25 15:35:45] INFO:     127.0.0.1:39572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:42066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45 TP0] Prefill batch [1643], #new-seq: 5, #new-token: 5, #cached-token: 3600, token usage: 0.11, #running-req: 1019, #queue-req: 266, 
[2025-10-25 15:35:45] INFO:     127.0.0.1:37858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:38486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:38978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:43176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:43946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:45742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45 TP0] Prefill batch [1645], #new-seq: 10, #new-token: 10, #cached-token: 7209, token usage: 0.11, #running-req: 1014, #queue-req: 256, 
[2025-10-25 15:35:45] INFO:     127.0.0.1:37414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:38474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:41074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:45] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46 TP0] Prefill batch [1647], #new-seq: 6, #new-token: 6, #cached-token: 4318, token usage: 0.11, #running-req: 1018, #queue-req: 250, 
[2025-10-25 15:35:46] INFO:     127.0.0.1:38344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46 TP0] Prefill batch [1649], #new-seq: 2, #new-token: 2, #cached-token: 1518, token usage: 0.11, #running-req: 1022, #queue-req: 248, 
[2025-10-25 15:35:46 TP0] Decode batch [1649], #running-req: 1022, #token: 108460, token usage: 0.11, cuda graph: False, gen throughput (token/s): 6767.67, #queue-req: 248, 
[2025-10-25 15:35:46] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:44372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:44668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:45796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46 TP0] Prefill batch [1651], #new-seq: 9, #new-token: 9, #cached-token: 6682, token usage: 0.11, #running-req: 1015, #queue-req: 239, 
[2025-10-25 15:35:46] INFO:     127.0.0.1:39840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:40240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:44746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:45022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:46274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46 TP0] Prefill batch [1653], #new-seq: 6, #new-token: 6, #cached-token: 4280, token usage: 0.11, #running-req: 1018, #queue-req: 233, 
[2025-10-25 15:35:46] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:38636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:40758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:44722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46 TP0] Prefill batch [1655], #new-seq: 6, #new-token: 6, #cached-token: 4455, token usage: 0.11, #running-req: 1018, #queue-req: 227, 
[2025-10-25 15:35:46] INFO:     127.0.0.1:38034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:40362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:41908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:42182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:42770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:46] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47 TP0] Prefill batch [1657], #new-seq: 10, #new-token: 10, #cached-token: 7284, token usage: 0.12, #running-req: 1014, #queue-req: 217, 
[2025-10-25 15:35:47] INFO:     127.0.0.1:38358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:41406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:41526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:42660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:42994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:44958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47 TP0] Prefill batch [1659], #new-seq: 14, #new-token: 14, #cached-token: 10076, token usage: 0.12, #running-req: 1010, #queue-req: 203, 
[2025-10-25 15:35:47] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:40428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:40592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:41204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47 TP0] Prefill batch [1661], #new-seq: 9, #new-token: 9, #cached-token: 6575, token usage: 0.12, #running-req: 1015, #queue-req: 194, 
[2025-10-25 15:35:47] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:37798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:38776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:42464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:44006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47 TP0] Prefill batch [1663], #new-seq: 10, #new-token: 10, #cached-token: 7282, token usage: 0.12, #running-req: 1014, #queue-req: 184, 
[2025-10-25 15:35:47] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:38830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:42706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47 TP0] Prefill batch [1665], #new-seq: 7, #new-token: 7, #cached-token: 5072, token usage: 0.12, #running-req: 1017, #queue-req: 177, 
[2025-10-25 15:35:47] INFO:     127.0.0.1:37758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:39884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:42056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:44014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:47] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48 TP0] Prefill batch [1667], #new-seq: 8, #new-token: 8, #cached-token: 5790, token usage: 0.12, #running-req: 1016, #queue-req: 169, 
[2025-10-25 15:35:48] INFO:     127.0.0.1:39960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:40690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48 TP0] Prefill batch [1669], #new-seq: 4, #new-token: 4, #cached-token: 2821, token usage: 0.12, #running-req: 1020, #queue-req: 165, 
[2025-10-25 15:35:48] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:39122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:39628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:41716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:42254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48 TP0] Prefill batch [1671], #new-seq: 10, #new-token: 10, #cached-token: 7273, token usage: 0.12, #running-req: 1014, #queue-req: 155, 
[2025-10-25 15:35:48] INFO:     127.0.0.1:37558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:43050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48 TP0] Prefill batch [1673], #new-seq: 6, #new-token: 6, #cached-token: 4355, token usage: 0.12, #running-req: 1018, #queue-req: 149, 
[2025-10-25 15:35:48] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:38006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:40670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:43758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48] INFO:     127.0.0.1:45390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:48 TP0] Prefill batch [1675], #new-seq: 10, #new-token: 10, #cached-token: 7362, token usage: 0.12, #running-req: 1014, #queue-req: 139, 
[2025-10-25 15:35:49] INFO:     127.0.0.1:38954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:39790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:41638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:41758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49 TP0] Prefill batch [1677], #new-seq: 11, #new-token: 11, #cached-token: 7992, token usage: 0.12, #running-req: 1013, #queue-req: 128, 
[2025-10-25 15:35:49] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:42222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:42720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:43936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:44454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49 TP0] Prefill batch [1679], #new-seq: 13, #new-token: 13, #cached-token: 9591, token usage: 0.12, #running-req: 1011, #queue-req: 115, 
[2025-10-25 15:35:49] INFO:     127.0.0.1:37750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:38448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:41868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:41948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:42538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:44696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:45124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49 TP0] Prefill batch [1681], #new-seq: 13, #new-token: 13, #cached-token: 9594, token usage: 0.12, #running-req: 1011, #queue-req: 102, 
[2025-10-25 15:35:49] INFO:     127.0.0.1:38848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:40780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:42590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:45146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49 TP0] Prefill batch [1683], #new-seq: 11, #new-token: 11, #cached-token: 8032, token usage: 0.12, #running-req: 1013, #queue-req: 91, 
[2025-10-25 15:35:49] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:38114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:40356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:40378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:41370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:43066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:44990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:49 TP0] Prefill batch [1685], #new-seq: 12, #new-token: 12, #cached-token: 8667, token usage: 0.12, #running-req: 1012, #queue-req: 79, 
[2025-10-25 15:35:50] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:41496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:42328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:46086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50 TP0] Prefill batch [1687], #new-seq: 8, #new-token: 8, #cached-token: 5871, token usage: 0.12, #running-req: 1016, #queue-req: 71, 
[2025-10-25 15:35:50] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:39648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:40096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:41032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:43864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:45190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50 TP0] Prefill batch [1689], #new-seq: 8, #new-token: 8, #cached-token: 5948, token usage: 0.12, #running-req: 1016, #queue-req: 63, 
[2025-10-25 15:35:50] INFO:     127.0.0.1:39692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:42138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:42976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:45148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50 TP0] Prefill batch [1691], #new-seq: 6, #new-token: 6, #cached-token: 4372, token usage: 0.13, #running-req: 1018, #queue-req: 57, 
[2025-10-25 15:35:50] INFO:     127.0.0.1:37720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:39718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:41826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:42866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:42896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:43280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:43966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:44948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:44972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50] INFO:     127.0.0.1:45456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:50 TP0] Prefill batch [1693], #new-seq: 20, #new-token: 20, #cached-token: 14658, token usage: 0.13, #running-req: 1004, #queue-req: 37, 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:50 TP0] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:50 TP2] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:50 TP5] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:50 TP4] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:50 TP1] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:50 TP6] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:50 TP3] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:50 TP7] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:51] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:39182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:41192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:41846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:43002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:44466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:45562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:46220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51 TP0] Prefill batch [1695], #new-seq: 11, #new-token: 11, #cached-token: 7966, token usage: 0.13, #running-req: 1013, #queue-req: 26, 
[2025-10-25 15:35:51] INFO:     127.0.0.1:38290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:38820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:39728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:40188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:40646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:42300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:42442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:44438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51 TP0] Prefill batch [1697], #new-seq: 13, #new-token: 13, #cached-token: 9524, token usage: 0.13, #running-req: 1011, #queue-req: 13, 
[2025-10-25 15:35:51] INFO:     127.0.0.1:37614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:38858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:39814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:40498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:43678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:44002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51 TP0] Prefill batch [1699], #new-seq: 11, #new-token: 11, #cached-token: 7988, token usage: 0.13, #running-req: 1013, #queue-req: 2, 
[2025-10-25 15:35:51] INFO:     127.0.0.1:38956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:39300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:39702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:40456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:41946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:43344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:44998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:46014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51 TP0] Prefill batch [1701], #new-seq: 2, #new-token: 2, #cached-token: 1457, token usage: 0.13, #running-req: 1011, #queue-req: 0, 
[2025-10-25 15:35:51] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:41222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:42642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:44238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:51] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:51 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:51 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:51 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:51 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:51 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:51 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:51 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:51 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:37704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP4] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP5] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP0] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP2] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP6] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP1] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP3] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP7] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45376 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP4] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP0] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP5] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP2] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP6] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP1] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP3] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP7] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52] INFO:     127.0.0.1:37658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:37952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:40340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:40796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:44482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45572 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP2] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP4] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP6] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP0] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP5] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP1] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP3] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP7] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP2] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP4] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP6] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP5] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP0] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP1] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP3] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP7] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52] INFO:     127.0.0.1:38062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:41190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:44706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:44836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP2] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP4] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP6] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP0] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP5] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP1] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP3] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP7] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52] INFO:     127.0.0.1:38152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:42432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:44992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP2] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP4] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP6] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP0] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP5] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP1] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP3] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP7] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52] INFO:     127.0.0.1:37762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:37960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:38610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:40280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:52] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP4] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP2] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP6] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP0] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP5] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP1] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP3] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:52 TP7] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53] INFO:     127.0.0.1:37464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:38014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:41590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:42758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45344 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP2] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP4] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP6] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP5] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP0] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP1] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP3] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP7] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:39970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:46030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:46360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:37490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:37754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:38374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:38766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:39346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:40852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP2] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP4] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP6] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP0] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP5] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP1] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP3] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP7] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53] INFO:     127.0.0.1:38524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:42492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP2] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP4] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP6] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP0] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP5] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP1] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP3] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP7] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:39746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:40276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:40768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP2] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP4] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP6] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP5] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP0] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP1] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP3] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP7] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP0] Decode batch [1716], #running-req: 878, #token: 115147, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5402.16, #queue-req: 0, 
[2025-10-25 15:35:53] INFO:     127.0.0.1:38222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:38320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:38876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:46336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP2] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP4] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP6] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP0] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP5] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP1] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP3] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP7] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53] INFO:     127.0.0.1:37548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:37872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:38304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:38696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:41132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:42128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:42298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:42744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:44036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45520 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP2] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP4] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP6] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP0] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP5] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP1] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP3] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP7] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53] INFO:     127.0.0.1:37990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:40154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:43998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:45932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:53] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP2] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP4] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP6] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP0] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP5] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP1] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP3] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:53 TP7] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:40536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:43120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45532 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP2] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP4] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP6] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP0] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP5] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP1] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP3] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP7] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:38806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:43666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:43718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP4] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP2] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP6] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP0] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP5] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP1] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP3] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP7] [fused_moe] using default for (803, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:43918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:46384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP2] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP4] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP6] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP0] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP5] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP1] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP3] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP7] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:37982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:39222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP4] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP2] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:38156 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP6] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP5] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:43162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54 TP0] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45962 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP1] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP3] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP7] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:38410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:38716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:39504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:40212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:43128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP4] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP2] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP6] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP5] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP0] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP1] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP3] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54 TP7] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:54] INFO:     127.0.0.1:38184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:41566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:42856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:43630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:45792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:54] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:37506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:37804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:37964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:39540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:42166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:45010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:45974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:38398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP4] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP2] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP6] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP0] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP5] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP1] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP3] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP7] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55] INFO:     127.0.0.1:38988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:46184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP4] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP2] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP6] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP0] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP5] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP1] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP3] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP7] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55] INFO:     127.0.0.1:39958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:40510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:46958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:47048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:45298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:47026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:47276 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP4] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP2] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP6] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP0] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP5] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP1] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP3] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP7] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55] INFO:     127.0.0.1:37598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:42212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:45674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP4] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP2] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP6] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP0] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP5] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP1] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP3] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP7] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55] INFO:     127.0.0.1:37646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:38420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:42326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:46518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:47034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:38248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:39430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:39976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:41296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:46246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:55] INFO:     127.0.0.1:47640 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP4] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP2] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP6] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP0] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP5] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP1] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP3] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:55 TP7] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56] INFO:     127.0.0.1:37568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:39604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:41030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:45474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:45660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:37584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:38552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:39040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP4] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP2] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP6] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP0] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP5] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP1] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP3] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP7] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56] INFO:     127.0.0.1:37694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:37756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:37922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:38026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:43200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:45452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP2] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP4] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP6] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP0] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP5] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP1] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP3] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP7] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56] INFO:     127.0.0.1:38470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:42450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:42686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP4] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP2] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP6] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP0] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP5] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP1] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP3] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP7] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56] INFO:     127.0.0.1:38168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:39868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:45774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:47838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:38408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:39292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:39378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:45696 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP4] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP2] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP6] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP0] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP5] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP1] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP3] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP7] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56] INFO:     127.0.0.1:37522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:39316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:39994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:43024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP4] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP2] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP6] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP0] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP5] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP1] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP7] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP3] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56] INFO:     127.0.0.1:39126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:41272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:43728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP2] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP4] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP6] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP0] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP5] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP1] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP3] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56 TP7] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:56] INFO:     127.0.0.1:38076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:40404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:41484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:43036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:44652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:46628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:47454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:56] INFO:     127.0.0.1:48472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:45438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47970 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:57 TP2] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:57 TP4] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:57 TP6] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:57 TP0] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:57 TP5] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:57 TP1] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:57 TP3] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:57 TP7] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-25 15:35:57] INFO:     127.0.0.1:37816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:42736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:45332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:37958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:37480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:42238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:45780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:45826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:45276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:42024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:48094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:39560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57 TP0] Decode batch [1756], #running-req: 461, #token: 74705, token usage: 0.08, cuda graph: True, gen throughput (token/s): 6282.39, #queue-req: 0, 
[2025-10-25 15:35:57] INFO:     127.0.0.1:37426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:38572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:40176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:43722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:37644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:41334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:57] INFO:     127.0.0.1:48278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:43364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:40458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:43336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:45202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:43826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:38390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:37906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:38932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:45352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:39252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:41016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:45642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:39142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:39418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:44350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:37586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:38218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:44278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:41060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:44546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:44510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:45750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:37840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:45602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:39240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:44926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:47806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:46712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:58] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:38544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:38770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:39622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:42510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:41256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:39878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:41286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:41318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:42792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:37786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:40660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:43016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:43418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:39472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:41044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:40904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:44358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:43880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:45856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:38386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:43712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:37890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:40286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:47534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:44294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:35:59] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:39000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:38080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:39782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:40466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00 TP0] Decode batch [1796], #running-req: 211, #token: 42849, token usage: 0.04, cuda graph: True, gen throughput (token/s): 5392.58, #queue-req: 0, 
[2025-10-25 15:36:00] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:42588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:43702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:39758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:41574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:39590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:40398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:42678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:41636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:44302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:39278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:44974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:41442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:39440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:40316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:47904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:00] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:37526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:38338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:45470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:38194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:40038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:45000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:45228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:37842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:43906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:38332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:43088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:39948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:42186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:39658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:45576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:41918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:44206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:40220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:41268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:38206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:39586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01 TP0] Decode batch [1836], #running-req: 72, #token: 19213, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2999.08, #queue-req: 0, 
[2025-10-25 15:36:01] INFO:     127.0.0.1:40086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:44376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:01] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:38588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:41118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:41258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:44262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:41818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:46200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:38264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:45500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:46846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:43656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:02] INFO:     127.0.0.1:45406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:46870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03 TP0] Decode batch [1876], #running-req: 21, #token: 6894, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1285.58, #queue-req: 0, 
[2025-10-25 15:36:03] INFO:     127.0.0.1:38946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:40166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:47110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:46930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:44096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:03] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:04] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:04 TP0] Decode batch [1916], #running-req: 3, #token: 1351, token usage: 0.00, cuda graph: True, gen throughput (token/s): 439.34, #queue-req: 0, 
[2025-10-25 15:36:04] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:04] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:04] INFO:     127.0.0.1:47546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:15] INFO:     127.0.0.1:57518 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-25 15:36:22] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:22 TP0] Prefill batch [1930], #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:36:22 TP0] Decode batch [1957], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.50, #queue-req: 0, 
[2025-10-25 15:36:24] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24 TP0] Prefill batch [1963], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:36:24] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24 TP0] Prefill batch [1964], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-25 15:36:24] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24 TP0] Prefill batch [1965], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 21, 
[2025-10-25 15:36:24] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:24] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:36:25 TP0] Prefill batch [1966], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-10-25 15:36:25 TP0] Prefill batch [1967], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-10-25 15:36:26 TP0] Prefill batch [1968], #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-10-25 15:36:27 TP0] Prefill batch [1969], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-10-25 15:36:28 TP0] Prefill batch [1970], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-10-25 15:36:29 TP0] Prefill batch [1971], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-10-25 15:36:29 TP0] Prefill batch [1972], #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-10-25 15:36:30 TP0] Prefill batch [1973], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-10-25 15:36:31 TP0] Prefill batch [1974], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-10-25 15:36:32 TP0] Prefill batch [1975], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-10-25 15:36:33 TP0] Prefill batch [1976], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 62, 
[2025-10-25 15:36:33 TP0] Prefill batch [1977], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.22, #running-req: 66, #queue-req: 57, 
[2025-10-25 15:36:34 TP0] Prefill batch [1978], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.23, #running-req: 71, #queue-req: 52, 
[2025-10-25 15:36:35 TP0] Prefill batch [1979], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.25, #running-req: 76, #queue-req: 47, 
[2025-10-25 15:36:36 TP0] Prefill batch [1980], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.27, #running-req: 81, #queue-req: 42, 
[2025-10-25 15:36:37 TP0] Prefill batch [1981], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 86, #queue-req: 37, 
[2025-10-25 15:36:37 TP0] Prefill batch [1982], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.30, #running-req: 91, #queue-req: 32, 
[2025-10-25 15:36:38 TP0] Prefill batch [1983], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 96, #queue-req: 27, 
[2025-10-25 15:36:39 TP0] Prefill batch [1984], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.33, #running-req: 101, #queue-req: 22, 
[2025-10-25 15:36:40 TP0] Prefill batch [1985], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.35, #running-req: 106, #queue-req: 17, 
[2025-10-25 15:36:41 TP0] Prefill batch [1986], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.37, #running-req: 111, #queue-req: 12, 
[2025-10-25 15:36:42 TP0] Prefill batch [1987], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.38, #running-req: 116, #queue-req: 7, 
[2025-10-25 15:36:42 TP0] Prefill batch [1988], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.40, #running-req: 121, #queue-req: 2, 
[2025-10-25 15:36:43 TP0] Prefill batch [1989], #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.41, #running-req: 126, #queue-req: 0, 
[2025-10-25 15:36:46 TP0] Decode batch [2024], #running-req: 128, #token: 413855, token usage: 0.43, cuda graph: True, gen throughput (token/s): 185.26, #queue-req: 0, 
[2025-10-25 15:36:48 TP0] Decode batch [2064], #running-req: 128, #token: 418975, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2535.50, #queue-req: 0, 
[2025-10-25 15:36:50 TP0] Decode batch [2104], #running-req: 128, #token: 424095, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2516.57, #queue-req: 0, 
[2025-10-25 15:36:52 TP0] Decode batch [2144], #running-req: 128, #token: 429215, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2506.22, #queue-req: 0, 
[2025-10-25 15:36:54 TP0] Decode batch [2184], #running-req: 128, #token: 434335, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2495.41, #queue-req: 0, 
[2025-10-25 15:36:56 TP0] Decode batch [2224], #running-req: 128, #token: 439455, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2489.96, #queue-req: 0, 
[2025-10-25 15:36:58 TP0] Decode batch [2264], #running-req: 128, #token: 444575, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2483.95, #queue-req: 0, 
[2025-10-25 15:37:00 TP0] Decode batch [2304], #running-req: 128, #token: 449695, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2478.31, #queue-req: 0, 
[2025-10-25 15:37:02 TP0] Decode batch [2344], #running-req: 128, #token: 454815, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2471.63, #queue-req: 0, 
[2025-10-25 15:37:04 TP0] Decode batch [2384], #running-req: 128, #token: 459935, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2463.35, #queue-req: 0, 
[2025-10-25 15:37:06 TP0] Decode batch [2424], #running-req: 128, #token: 465055, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2461.22, #queue-req: 0, 
[2025-10-25 15:37:09 TP0] Decode batch [2464], #running-req: 128, #token: 470175, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2458.11, #queue-req: 0, 
[2025-10-25 15:37:11 TP0] Decode batch [2504], #running-req: 128, #token: 475295, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2453.72, #queue-req: 0, 
[2025-10-25 15:37:13 TP0] Decode batch [2544], #running-req: 128, #token: 480415, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2446.34, #queue-req: 0, 
[2025-10-25 15:37:15 TP0] Decode batch [2584], #running-req: 128, #token: 485535, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2440.11, #queue-req: 0, 
[2025-10-25 15:37:17 TP0] Decode batch [2624], #running-req: 128, #token: 490655, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2438.15, #queue-req: 0, 
[2025-10-25 15:37:19 TP0] Decode batch [2664], #running-req: 128, #token: 495775, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2431.11, #queue-req: 0, 
[2025-10-25 15:37:21 TP0] Decode batch [2704], #running-req: 128, #token: 500895, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2427.17, #queue-req: 0, 
[2025-10-25 15:37:23 TP0] Decode batch [2744], #running-req: 128, #token: 506015, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2424.10, #queue-req: 0, 
[2025-10-25 15:37:25 TP0] Decode batch [2784], #running-req: 128, #token: 511135, token usage: 0.53, cuda graph: True, gen throughput (token/s): 2419.74, #queue-req: 0, 
[2025-10-25 15:37:26] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26 TP0] Prefill batch [2790], #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:37:26] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26 TP0] Prefill batch [2791], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-25 15:37:26] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26 TP0] Prefill batch [2792], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 40, 
[2025-10-25 15:37:26] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:26] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:27] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:27] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:27] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:27] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:27] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:27] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:27] INFO:     127.0.0.1:59336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:27] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:37:27 TP0] Prefill batch [2793], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-10-25 15:37:28 TP0] Prefill batch [2794], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-10-25 15:37:28 TP0] Prefill batch [2795], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-10-25 15:37:29 TP0] Prefill batch [2796], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-10-25 15:37:30 TP0] Prefill batch [2797], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-10-25 15:37:31 TP0] Prefill batch [2798], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-10-25 15:37:32 TP0] Prefill batch [2799], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-10-25 15:37:33 TP0] Prefill batch [2800], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-10-25 15:37:33 TP0] Prefill batch [2801], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-10-25 15:37:34 TP0] Prefill batch [2802], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-10-25 15:37:35 TP0] Prefill batch [2803], #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.20, #running-req: 61, #queue-req: 61, 
[2025-10-25 15:37:36 TP0] Prefill batch [2804], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.22, #running-req: 67, #queue-req: 56, 
[2025-10-25 15:37:37 TP0] Prefill batch [2805], #new-seq: 5, #new-token: 15977, #cached-token: 28, token usage: 0.23, #running-req: 72, #queue-req: 51, 
[2025-10-25 15:37:38 TP0] Prefill batch [2806], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.25, #running-req: 77, #queue-req: 46, 
[2025-10-25 15:37:38 TP0] Prefill batch [2807], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.27, #running-req: 82, #queue-req: 41, 
[2025-10-25 15:37:39 TP0] Prefill batch [2808], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.28, #running-req: 87, #queue-req: 36, 
[2025-10-25 15:37:40 TP0] Prefill batch [2809], #new-seq: 6, #new-token: 15994, #cached-token: 3212, token usage: 0.30, #running-req: 92, #queue-req: 30, 
[2025-10-25 15:37:41 TP0] Prefill batch [2810], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.32, #running-req: 98, #queue-req: 25, 
[2025-10-25 15:37:42 TP0] Prefill batch [2811], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.34, #running-req: 103, #queue-req: 20, 
[2025-10-25 15:37:42 TP0] Prefill batch [2812], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.35, #running-req: 108, #queue-req: 15, 
[2025-10-25 15:37:43 TP0] Prefill batch [2813], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.37, #running-req: 113, #queue-req: 10, 
[2025-10-25 15:37:44 TP0] Prefill batch [2814], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.39, #running-req: 118, #queue-req: 5, 
[2025-10-25 15:37:45 TP0] Prefill batch [2815], #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.40, #running-req: 123, #queue-req: 0, 
[2025-10-25 15:37:48 TP0] Decode batch [2850], #running-req: 128, #token: 407453, token usage: 0.42, cuda graph: True, gen throughput (token/s): 226.91, #queue-req: 0, 
[2025-10-25 15:37:50 TP0] Decode batch [2890], #running-req: 128, #token: 412573, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2517.77, #queue-req: 0, 
[2025-10-25 15:37:52 TP0] Decode batch [2930], #running-req: 128, #token: 417693, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2502.23, #queue-req: 0, 
[2025-10-25 15:37:54 TP0] Decode batch [2970], #running-req: 128, #token: 422813, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2494.40, #queue-req: 0, 
[2025-10-25 15:37:56 TP0] Decode batch [3010], #running-req: 128, #token: 427933, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2487.23, #queue-req: 0, 
[2025-10-25 15:37:58 TP0] Decode batch [3050], #running-req: 128, #token: 433053, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2479.50, #queue-req: 0, 
[2025-10-25 15:38:00 TP0] Decode batch [3090], #running-req: 128, #token: 438173, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2474.80, #queue-req: 0, 
[2025-10-25 15:38:02 TP0] Decode batch [3130], #running-req: 128, #token: 443293, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2468.08, #queue-req: 0, 
[2025-10-25 15:38:04 TP0] Decode batch [3170], #running-req: 128, #token: 448413, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2463.10, #queue-req: 0, 
[2025-10-25 15:38:07 TP0] Decode batch [3210], #running-req: 128, #token: 453533, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2458.20, #queue-req: 0, 
[2025-10-25 15:38:09 TP0] Decode batch [3250], #running-req: 128, #token: 458653, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2456.41, #queue-req: 0, 
[2025-10-25 15:38:11 TP0] Decode batch [3290], #running-req: 128, #token: 463773, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2452.67, #queue-req: 0, 
[2025-10-25 15:38:13 TP0] Decode batch [3330], #running-req: 128, #token: 468893, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2446.12, #queue-req: 0, 
[2025-10-25 15:38:15 TP0] Decode batch [3370], #running-req: 128, #token: 474013, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2440.78, #queue-req: 0, 
[2025-10-25 15:38:17 TP0] Decode batch [3410], #running-req: 128, #token: 479133, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2436.91, #queue-req: 0, 
[2025-10-25 15:38:19 TP0] Decode batch [3450], #running-req: 128, #token: 484253, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2432.37, #queue-req: 0, 
[2025-10-25 15:38:21 TP0] Decode batch [3490], #running-req: 128, #token: 489373, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2426.14, #queue-req: 0, 
[2025-10-25 15:38:23 TP0] Decode batch [3530], #running-req: 128, #token: 494493, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2425.57, #queue-req: 0, 
[2025-10-25 15:38:25 TP0] Decode batch [3570], #running-req: 128, #token: 499613, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2419.45, #queue-req: 0, 
[2025-10-25 15:38:28 TP0] Decode batch [3610], #running-req: 128, #token: 504733, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2417.01, #queue-req: 0, 
[2025-10-25 15:38:28] INFO:     127.0.0.1:59322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28 TP0] Prefill batch [3616], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:38:28] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28 TP0] Prefill batch [3617], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-25 15:38:28] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28 TP0] Prefill batch [3618], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-10-25 15:38:28] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:59996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:28] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:38:29 TP0] Prefill batch [3619], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-10-25 15:38:30 TP0] Prefill batch [3620], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-10-25 15:38:31 TP0] Prefill batch [3621], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-10-25 15:38:31 TP0] Prefill batch [3622], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-10-25 15:38:32 TP0] Prefill batch [3623], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-10-25 15:38:33 TP0] Prefill batch [3624], #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 36, #queue-req: 86, 
[2025-10-25 15:38:34 TP0] Prefill batch [3625], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 42, #queue-req: 81, 
[2025-10-25 15:38:35 TP0] Prefill batch [3626], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 76, 
[2025-10-25 15:38:36 TP0] Prefill batch [3627], #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.17, #running-req: 52, #queue-req: 71, 
[2025-10-25 15:38:36 TP0] Prefill batch [3628], #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.19, #running-req: 57, #queue-req: 65, 
[2025-10-25 15:38:37 TP0] Prefill batch [3629], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.21, #running-req: 63, #queue-req: 60, 
[2025-10-25 15:38:38 TP0] Prefill batch [3630], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.22, #running-req: 68, #queue-req: 55, 
[2025-10-25 15:38:39 TP0] Prefill batch [3631], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.24, #running-req: 73, #queue-req: 50, 
[2025-10-25 15:38:40 TP0] Prefill batch [3632], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.26, #running-req: 78, #queue-req: 45, 
[2025-10-25 15:38:40 TP0] Prefill batch [3633], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.27, #running-req: 83, #queue-req: 40, 
[2025-10-25 15:38:41 TP0] Prefill batch [3634], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.29, #running-req: 88, #queue-req: 35, 
[2025-10-25 15:38:42 TP0] Prefill batch [3635], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.31, #running-req: 93, #queue-req: 30, 
[2025-10-25 15:38:43 TP0] Prefill batch [3636], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.32, #running-req: 98, #queue-req: 25, 
[2025-10-25 15:38:44 TP0] Prefill batch [3637], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.34, #running-req: 103, #queue-req: 20, 
[2025-10-25 15:38:45 TP0] Prefill batch [3638], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.36, #running-req: 108, #queue-req: 15, 
[2025-10-25 15:38:45 TP0] Prefill batch [3639], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.37, #running-req: 113, #queue-req: 10, 
[2025-10-25 15:38:46 TP0] Prefill batch [3640], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.39, #running-req: 118, #queue-req: 5, 
[2025-10-25 15:38:47 TP0] Prefill batch [3641], #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.40, #running-req: 123, #queue-req: 0, 
[2025-10-25 15:38:50 TP0] Decode batch [3676], #running-req: 128, #token: 413852, token usage: 0.43, cuda graph: True, gen throughput (token/s): 226.16, #queue-req: 0, 
[2025-10-25 15:38:52 TP0] Decode batch [3716], #running-req: 128, #token: 418972, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2513.82, #queue-req: 0, 
[2025-10-25 15:38:54 TP0] Decode batch [3756], #running-req: 128, #token: 424092, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2492.65, #queue-req: 0, 
[2025-10-25 15:38:56 TP0] Decode batch [3796], #running-req: 128, #token: 429212, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2484.31, #queue-req: 0, 
[2025-10-25 15:38:58 TP0] Decode batch [3836], #running-req: 128, #token: 434332, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2476.34, #queue-req: 0, 
[2025-10-25 15:39:00 TP0] Decode batch [3876], #running-req: 128, #token: 439452, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2468.21, #queue-req: 0, 
[2025-10-25 15:39:03 TP0] Decode batch [3916], #running-req: 128, #token: 444572, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2463.84, #queue-req: 0, 
[2025-10-25 15:39:05 TP0] Decode batch [3956], #running-req: 128, #token: 449692, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2460.39, #queue-req: 0, 
[2025-10-25 15:39:07 TP0] Decode batch [3996], #running-req: 128, #token: 454812, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2454.01, #queue-req: 0, 
[2025-10-25 15:39:09 TP0] Decode batch [4036], #running-req: 128, #token: 459932, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2448.87, #queue-req: 0, 
[2025-10-25 15:39:11 TP0] Decode batch [4076], #running-req: 128, #token: 465052, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2444.83, #queue-req: 0, 
[2025-10-25 15:39:13 TP0] Decode batch [4116], #running-req: 128, #token: 470172, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2441.84, #queue-req: 0, 
[2025-10-25 15:39:15 TP0] Decode batch [4156], #running-req: 128, #token: 475292, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2438.62, #queue-req: 0, 
[2025-10-25 15:39:17 TP0] Decode batch [4196], #running-req: 128, #token: 480412, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2431.78, #queue-req: 0, 
[2025-10-25 15:39:19 TP0] Decode batch [4236], #running-req: 128, #token: 485532, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2424.60, #queue-req: 0, 
[2025-10-25 15:39:21 TP0] Decode batch [4276], #running-req: 128, #token: 490652, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2423.18, #queue-req: 0, 
[2025-10-25 15:39:24 TP0] Decode batch [4316], #running-req: 128, #token: 495772, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2414.94, #queue-req: 0, 
[2025-10-25 15:39:26 TP0] Decode batch [4356], #running-req: 128, #token: 500892, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2411.40, #queue-req: 0, 
[2025-10-25 15:39:28 TP0] Decode batch [4396], #running-req: 128, #token: 506012, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2408.12, #queue-req: 0, 
[2025-10-25 15:39:30 TP0] Decode batch [4436], #running-req: 128, #token: 511132, token usage: 0.53, cuda graph: True, gen throughput (token/s): 2402.13, #queue-req: 0, 
[2025-10-25 15:39:30] INFO:     127.0.0.1:42482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30 TP0] Prefill batch [4442], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:39:30] INFO:     127.0.0.1:42488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30 TP0] Prefill batch [4443], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-25 15:39:30] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:30] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31 TP0] Prefill batch [4444], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 39, 
[2025-10-25 15:39:31] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:39:31 TP0] Prefill batch [4445], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 100, 
[2025-10-25 15:39:32 TP0] Prefill batch [4446], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 95, 
[2025-10-25 15:39:33 TP0] Prefill batch [4447], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 90, 
[2025-10-25 15:39:34 TP0] Prefill batch [4448], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.09, #running-req: 26, #queue-req: 85, 
[2025-10-25 15:39:35 TP0] Prefill batch [4449], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 80, 
[2025-10-25 15:39:36 TP0] Prefill batch [4450], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 75, 
[2025-10-25 15:39:36 TP0] Prefill batch [4451], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.13, #running-req: 41, #queue-req: 70, 
[2025-10-25 15:39:37 TP0] Prefill batch [4452], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 65, 
[2025-10-25 15:39:38 TP0] Prefill batch [4453], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.17, #running-req: 51, #queue-req: 60, 
[2025-10-25 15:39:39 TP0] Prefill batch [4454], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.18, #running-req: 56, #queue-req: 55, 
[2025-10-25 15:39:40 TP0] Prefill batch [4455], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 50, 
[2025-10-25 15:39:40 TP0] Prefill batch [4456], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.22, #running-req: 66, #queue-req: 45, 
[2025-10-25 15:39:41 TP0] Prefill batch [4457], #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.24, #running-req: 71, #queue-req: 39, 
[2025-10-25 15:39:42 TP0] Prefill batch [4458], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.25, #running-req: 77, #queue-req: 34, 
[2025-10-25 15:39:43 TP0] Prefill batch [4459], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.27, #running-req: 82, #queue-req: 29, 
[2025-10-25 15:39:44 TP0] Prefill batch [4460], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.29, #running-req: 87, #queue-req: 24, 
[2025-10-25 15:39:45 TP0] Prefill batch [4461], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.30, #running-req: 92, #queue-req: 19, 
[2025-10-25 15:39:45 TP0] Prefill batch [4462], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.32, #running-req: 97, #queue-req: 14, 
[2025-10-25 15:39:46 TP0] Prefill batch [4463], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.34, #running-req: 102, #queue-req: 9, 
[2025-10-25 15:39:47 TP0] Prefill batch [4464], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.35, #running-req: 107, #queue-req: 4, 
[2025-10-25 15:39:48 TP0] Prefill batch [4465], #new-seq: 4, #new-token: 12787, #cached-token: 17, token usage: 0.37, #running-req: 112, #queue-req: 0, 
[2025-10-25 15:39:51 TP0] Decode batch [4500], #running-req: 116, #token: 375058, token usage: 0.39, cuda graph: True, gen throughput (token/s): 225.18, #queue-req: 0, 
[2025-10-25 15:39:53 TP0] Decode batch [4540], #running-req: 116, #token: 379698, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2328.36, #queue-req: 0, 
[2025-10-25 15:39:55 TP0] Decode batch [4580], #running-req: 116, #token: 384338, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2319.77, #queue-req: 0, 
[2025-10-25 15:39:57 TP0] Decode batch [4620], #running-req: 116, #token: 388978, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2311.35, #queue-req: 0, 
[2025-10-25 15:39:59 TP0] Decode batch [4660], #running-req: 116, #token: 393618, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2305.26, #queue-req: 0, 
[2025-10-25 15:40:01 TP0] Decode batch [4700], #running-req: 116, #token: 398258, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2300.15, #queue-req: 0, 
[2025-10-25 15:40:03 TP0] Decode batch [4740], #running-req: 116, #token: 402898, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2294.81, #queue-req: 0, 
[2025-10-25 15:40:05 TP0] Decode batch [4780], #running-req: 116, #token: 407538, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2293.54, #queue-req: 0, 
[2025-10-25 15:40:07 TP0] Decode batch [4820], #running-req: 116, #token: 412178, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2287.69, #queue-req: 0, 
[2025-10-25 15:40:09 TP0] Decode batch [4860], #running-req: 116, #token: 416818, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2288.89, #queue-req: 0, 
[2025-10-25 15:40:11 TP0] Decode batch [4900], #running-req: 116, #token: 421458, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2283.21, #queue-req: 0, 
[2025-10-25 15:40:13 TP0] Decode batch [4940], #running-req: 116, #token: 426098, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2280.77, #queue-req: 0, 
[2025-10-25 15:40:15 TP0] Decode batch [4980], #running-req: 116, #token: 430738, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2275.39, #queue-req: 0, 
[2025-10-25 15:40:17 TP0] Decode batch [5020], #running-req: 116, #token: 435378, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2267.99, #queue-req: 0, 
[2025-10-25 15:40:19 TP0] Decode batch [5060], #running-req: 116, #token: 440018, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2264.06, #queue-req: 0, 
[2025-10-25 15:40:21 TP0] Decode batch [5100], #running-req: 116, #token: 444658, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2260.09, #queue-req: 0, 
[2025-10-25 15:40:23 TP0] Decode batch [5140], #running-req: 116, #token: 449298, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2255.81, #queue-req: 0, 
[2025-10-25 15:40:25 TP0] Decode batch [5180], #running-req: 116, #token: 453938, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2250.04, #queue-req: 0, 
[2025-10-25 15:40:27 TP0] Decode batch [5220], #running-req: 116, #token: 458578, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2246.70, #queue-req: 0, 
[2025-10-25 15:40:29 TP0] Decode batch [5260], #running-req: 116, #token: 463218, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2243.53, #queue-req: 0, 
[2025-10-25 15:40:30] INFO:     127.0.0.1:48834 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-25 15:40:47] INFO:     127.0.0.1:42932 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-25 15:40:53] INFO:     127.0.0.1:42936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:53 TP0] Prefill batch [5266], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:40:55] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55 TP0] Prefill batch [5299], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:40:55] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55 TP0] Prefill batch [5300], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-25 15:40:55] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:55] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56 TP0] Prefill batch [5301], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 22, 
[2025-10-25 15:40:56] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:40:56 TP0] Prefill batch [5302], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-25 15:40:57 TP0] Prefill batch [5303], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-25 15:40:58 TP0] Prefill batch [5304], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-25 15:40:59 TP0] Prefill batch [5305], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-25 15:41:00 TP0] Prefill batch [5306], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-25 15:41:00 TP0] Prefill batch [5307], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-10-25 15:41:01 TP0] Prefill batch [5308], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-10-25 15:41:02 TP0] Prefill batch [5309], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-10-25 15:41:03 TP0] Prefill batch [5310], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-10-25 15:41:04 TP0] Prefill batch [5311], #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.19, #running-req: 56, #queue-req: 2, 
[2025-10-25 15:41:04 TP0] Prefill batch [5312], #new-seq: 2, #new-token: 6398, #cached-token: 4, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-10-25 15:41:06 TP0] Decode batch [5315], #running-req: 64, #token: 204904, token usage: 0.21, cuda graph: True, gen throughput (token/s): 23.73, #queue-req: 0, 
[2025-10-25 15:41:07 TP0] Decode batch [5355], #running-req: 64, #token: 207464, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1554.12, #queue-req: 0, 
[2025-10-25 15:41:09 TP0] Decode batch [5395], #running-req: 64, #token: 210024, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1526.76, #queue-req: 0, 
[2025-10-25 15:41:11 TP0] Decode batch [5435], #running-req: 64, #token: 212584, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1511.16, #queue-req: 0, 
[2025-10-25 15:41:12 TP0] Decode batch [5475], #running-req: 64, #token: 215144, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1505.73, #queue-req: 0, 
[2025-10-25 15:41:14 TP0] Decode batch [5515], #running-req: 64, #token: 217704, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1502.39, #queue-req: 0, 
[2025-10-25 15:41:16 TP0] Decode batch [5555], #running-req: 64, #token: 220264, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.77, #queue-req: 0, 
[2025-10-25 15:41:17 TP0] Decode batch [5595], #running-req: 64, #token: 222824, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1493.02, #queue-req: 0, 
[2025-10-25 15:41:19 TP0] Decode batch [5635], #running-req: 64, #token: 225384, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.82, #queue-req: 0, 
[2025-10-25 15:41:21 TP0] Decode batch [5675], #running-req: 64, #token: 227944, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.11, #queue-req: 0, 
[2025-10-25 15:41:23 TP0] Decode batch [5715], #running-req: 64, #token: 230504, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1484.87, #queue-req: 0, 
[2025-10-25 15:41:24 TP0] Decode batch [5755], #running-req: 64, #token: 233064, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1486.62, #queue-req: 0, 
[2025-10-25 15:41:26 TP0] Decode batch [5795], #running-req: 64, #token: 235624, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1487.30, #queue-req: 0, 
[2025-10-25 15:41:28 TP0] Decode batch [5835], #running-req: 64, #token: 238184, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1483.54, #queue-req: 0, 
[2025-10-25 15:41:29 TP0] Decode batch [5875], #running-req: 64, #token: 240744, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.92, #queue-req: 0, 
[2025-10-25 15:41:31 TP0] Decode batch [5915], #running-req: 64, #token: 243304, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1479.44, #queue-req: 0, 
[2025-10-25 15:41:33 TP0] Decode batch [5955], #running-req: 64, #token: 245864, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1479.26, #queue-req: 0, 
[2025-10-25 15:41:35 TP0] Decode batch [5995], #running-req: 64, #token: 248424, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1474.24, #queue-req: 0, 
[2025-10-25 15:41:36 TP0] Decode batch [6035], #running-req: 64, #token: 250984, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1474.42, #queue-req: 0, 
[2025-10-25 15:41:38 TP0] Decode batch [6075], #running-req: 64, #token: 253544, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1472.65, #queue-req: 0, 
[2025-10-25 15:41:40] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40 TP0] Prefill batch [6113], #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:41:40] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:44998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40 TP0] Prefill batch [6114], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-25 15:41:40] INFO:     127.0.0.1:45034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40 TP0] Prefill batch [6115], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 43, 
[2025-10-25 15:41:40] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:40] INFO:     127.0.0.1:45470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:41:41 TP0] Prefill batch [6116], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-25 15:41:42 TP0] Prefill batch [6117], #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.06, #running-req: 16, #queue-req: 42, 
[2025-10-25 15:41:43 TP0] Prefill batch [6118], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-10-25 15:41:43 TP0] Prefill batch [6119], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.09, #running-req: 27, #queue-req: 32, 
[2025-10-25 15:41:44 TP0] Prefill batch [6120], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.11, #running-req: 32, #queue-req: 27, 
[2025-10-25 15:41:45 TP0] Prefill batch [6121], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.12, #running-req: 37, #queue-req: 22, 
[2025-10-25 15:41:46 TP0] Prefill batch [6122], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-10-25 15:41:47 TP0] Prefill batch [6123], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-10-25 15:41:47 TP0] Prefill batch [6124], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-10-25 15:41:48 TP0] Prefill batch [6125], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.19, #running-req: 57, #queue-req: 2, 
[2025-10-25 15:41:49 TP0] Prefill batch [6126], #new-seq: 2, #new-token: 6393, #cached-token: 9, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-10-25 15:41:50 TP0] Decode batch [6129], #running-req: 64, #token: 204887, token usage: 0.21, cuda graph: True, gen throughput (token/s): 211.22, #queue-req: 0, 
[2025-10-25 15:41:52 TP0] Decode batch [6169], #running-req: 64, #token: 207447, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1549.07, #queue-req: 0, 
[2025-10-25 15:41:54 TP0] Decode batch [6209], #running-req: 64, #token: 210007, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1521.66, #queue-req: 0, 
[2025-10-25 15:41:55 TP0] Decode batch [6249], #running-req: 64, #token: 212567, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1513.70, #queue-req: 0, 
[2025-10-25 15:41:57 TP0] Decode batch [6289], #running-req: 64, #token: 215127, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1509.77, #queue-req: 0, 
[2025-10-25 15:41:59 TP0] Decode batch [6329], #running-req: 64, #token: 217687, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1506.25, #queue-req: 0, 
[2025-10-25 15:42:00 TP0] Decode batch [6369], #running-req: 64, #token: 220247, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1502.78, #queue-req: 0, 
[2025-10-25 15:42:02 TP0] Decode batch [6409], #running-req: 64, #token: 222807, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1498.31, #queue-req: 0, 
[2025-10-25 15:42:04 TP0] Decode batch [6449], #running-req: 64, #token: 225367, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1495.75, #queue-req: 0, 
[2025-10-25 15:42:06 TP0] Decode batch [6489], #running-req: 64, #token: 227927, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1494.54, #queue-req: 0, 
[2025-10-25 15:42:07 TP0] Decode batch [6529], #running-req: 64, #token: 230487, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1493.14, #queue-req: 0, 
[2025-10-25 15:42:09 TP0] Decode batch [6569], #running-req: 64, #token: 233047, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.47, #queue-req: 0, 
[2025-10-25 15:42:11 TP0] Decode batch [6609], #running-req: 64, #token: 235607, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1492.02, #queue-req: 0, 
[2025-10-25 15:42:12 TP0] Decode batch [6649], #running-req: 64, #token: 238167, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1492.32, #queue-req: 0, 
[2025-10-25 15:42:14 TP0] Decode batch [6689], #running-req: 64, #token: 240727, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1491.39, #queue-req: 0, 
[2025-10-25 15:42:16 TP0] Decode batch [6729], #running-req: 64, #token: 243287, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1487.90, #queue-req: 0, 
[2025-10-25 15:42:18 TP0] Decode batch [6769], #running-req: 64, #token: 245847, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1486.23, #queue-req: 0, 
[2025-10-25 15:42:19 TP0] Decode batch [6809], #running-req: 64, #token: 248407, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1478.92, #queue-req: 0, 
[2025-10-25 15:42:21 TP0] Decode batch [6849], #running-req: 64, #token: 250967, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1480.83, #queue-req: 0, 
[2025-10-25 15:42:23 TP0] Decode batch [6889], #running-req: 64, #token: 253527, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1478.33, #queue-req: 0, 
[2025-10-25 15:42:24] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24 TP0] Prefill batch [6927], #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:42:24] INFO:     127.0.0.1:53978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:24] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25 TP0] Prefill batch [6928], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 11, 
[2025-10-25 15:42:25] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25 TP0] Prefill batch [6929], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 41, 
[2025-10-25 15:42:25] INFO:     127.0.0.1:54442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:25] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:42:26 TP0] Prefill batch [6930], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-25 15:42:26 TP0] Prefill batch [6931], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-25 15:42:27 TP0] Prefill batch [6932], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-25 15:42:28 TP0] Prefill batch [6933], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-25 15:42:29 TP0] Prefill batch [6934], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-25 15:42:30 TP0] Prefill batch [6935], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-10-25 15:42:30 TP0] Prefill batch [6936], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-10-25 15:42:31 TP0] Prefill batch [6937], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-10-25 15:42:32 TP0] Prefill batch [6938], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-10-25 15:42:33 TP0] Prefill batch [6939], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-10-25 15:42:34 TP0] Prefill batch [6940], #new-seq: 3, #new-token: 9592, #cached-token: 11, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-10-25 15:42:35 TP0] Decode batch [6943], #running-req: 64, #token: 204901, token usage: 0.21, cuda graph: True, gen throughput (token/s): 209.34, #queue-req: 0, 
[2025-10-25 15:42:37 TP0] Decode batch [6983], #running-req: 64, #token: 207461, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1544.39, #queue-req: 0, 
[2025-10-25 15:42:38 TP0] Decode batch [7023], #running-req: 64, #token: 210021, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1520.66, #queue-req: 0, 
[2025-10-25 15:42:40 TP0] Decode batch [7063], #running-req: 64, #token: 212581, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1509.12, #queue-req: 0, 
[2025-10-25 15:42:42 TP0] Decode batch [7103], #running-req: 64, #token: 215141, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.43, #queue-req: 0, 
[2025-10-25 15:42:43 TP0] Decode batch [7143], #running-req: 64, #token: 217701, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1496.66, #queue-req: 0, 
[2025-10-25 15:42:45 TP0] Decode batch [7183], #running-req: 64, #token: 220261, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1493.39, #queue-req: 0, 
[2025-10-25 15:42:47 TP0] Decode batch [7223], #running-req: 64, #token: 222821, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.92, #queue-req: 0, 
[2025-10-25 15:42:49 TP0] Decode batch [7263], #running-req: 64, #token: 225381, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1485.77, #queue-req: 0, 
[2025-10-25 15:42:50 TP0] Decode batch [7303], #running-req: 64, #token: 227941, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1484.15, #queue-req: 0, 
[2025-10-25 15:42:52 TP0] Decode batch [7343], #running-req: 64, #token: 230501, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1485.05, #queue-req: 0, 
[2025-10-25 15:42:54 TP0] Decode batch [7383], #running-req: 64, #token: 233061, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1485.25, #queue-req: 0, 
[2025-10-25 15:42:55 TP0] Decode batch [7423], #running-req: 64, #token: 235621, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1485.74, #queue-req: 0, 
[2025-10-25 15:42:57 TP0] Decode batch [7463], #running-req: 64, #token: 238181, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1484.68, #queue-req: 0, 
[2025-10-25 15:42:59 TP0] Decode batch [7503], #running-req: 64, #token: 240741, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1484.42, #queue-req: 0, 
[2025-10-25 15:43:01 TP0] Decode batch [7543], #running-req: 64, #token: 243301, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.54, #queue-req: 0, 
[2025-10-25 15:43:02 TP0] Decode batch [7583], #running-req: 64, #token: 245861, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1479.14, #queue-req: 0, 
[2025-10-25 15:43:04 TP0] Decode batch [7623], #running-req: 64, #token: 248421, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1475.72, #queue-req: 0, 
[2025-10-25 15:43:06 TP0] Decode batch [7663], #running-req: 64, #token: 250981, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1472.95, #queue-req: 0, 
[2025-10-25 15:43:08 TP0] Decode batch [7703], #running-req: 64, #token: 253541, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1474.64, #queue-req: 0, 
[2025-10-25 15:43:09] INFO:     127.0.0.1:38170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09 TP0] Prefill batch [7741], #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:43:09] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09 TP0] Prefill batch [7742], #new-seq: 6, #new-token: 15987, #cached-token: 3219, token usage: 0.01, #running-req: 1, #queue-req: 11, 
[2025-10-25 15:43:09] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:09] INFO:     127.0.0.1:38520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10 TP0] Prefill batch [7743], #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.02, #running-req: 7, #queue-req: 39, 
[2025-10-25 15:43:10] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:10 TP0] Prefill batch [7744], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 12, #queue-req: 47, 
[2025-10-25 15:43:11 TP0] Prefill batch [7745], #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.06, #running-req: 17, #queue-req: 42, 
[2025-10-25 15:43:12 TP0] Prefill batch [7746], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-10-25 15:43:13 TP0] Prefill batch [7747], #new-seq: 7, #new-token: 15993, #cached-token: 6414, token usage: 0.10, #running-req: 27, #queue-req: 30, 
[2025-10-25 15:43:14 TP0] Prefill batch [7748], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 34, #queue-req: 25, 
[2025-10-25 15:43:14 TP0] Prefill batch [7749], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.13, #running-req: 39, #queue-req: 20, 
[2025-10-25 15:43:15 TP0] Prefill batch [7750], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.14, #running-req: 44, #queue-req: 15, 
[2025-10-25 15:43:16 TP0] Prefill batch [7751], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.16, #running-req: 49, #queue-req: 10, 
[2025-10-25 15:43:17 TP0] Prefill batch [7752], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.18, #running-req: 54, #queue-req: 5, 
[2025-10-25 15:43:18 TP0] Prefill batch [7753], #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.19, #running-req: 59, #queue-req: 0, 
[2025-10-25 15:43:19 TP0] Decode batch [7756], #running-req: 64, #token: 201690, token usage: 0.21, cuda graph: True, gen throughput (token/s): 219.92, #queue-req: 0, 
[2025-10-25 15:43:21 TP0] Decode batch [7796], #running-req: 64, #token: 204250, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1538.75, #queue-req: 0, 
[2025-10-25 15:43:23 TP0] Decode batch [7836], #running-req: 64, #token: 206810, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1510.85, #queue-req: 0, 
[2025-10-25 15:43:24 TP0] Decode batch [7876], #running-req: 64, #token: 209370, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1501.44, #queue-req: 0, 
[2025-10-25 15:43:26 TP0] Decode batch [7916], #running-req: 64, #token: 211930, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1504.19, #queue-req: 0, 
[2025-10-25 15:43:28 TP0] Decode batch [7956], #running-req: 64, #token: 214490, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.24, #queue-req: 0, 
[2025-10-25 15:43:29 TP0] Decode batch [7996], #running-req: 64, #token: 217050, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1496.77, #queue-req: 0, 
[2025-10-25 15:43:31 TP0] Decode batch [8036], #running-req: 64, #token: 219610, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1495.42, #queue-req: 0, 
[2025-10-25 15:43:33 TP0] Decode batch [8076], #running-req: 64, #token: 222170, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1494.89, #queue-req: 0, 
[2025-10-25 15:43:35 TP0] Decode batch [8116], #running-req: 64, #token: 224730, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1492.97, #queue-req: 0, 
[2025-10-25 15:43:36 TP0] Decode batch [8156], #running-req: 64, #token: 227290, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1484.15, #queue-req: 0, 
[2025-10-25 15:43:38 TP0] Decode batch [8196], #running-req: 64, #token: 229850, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1489.82, #queue-req: 0, 
[2025-10-25 15:43:40 TP0] Decode batch [8236], #running-req: 64, #token: 232410, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.06, #queue-req: 0, 
[2025-10-25 15:43:41 TP0] Decode batch [8276], #running-req: 64, #token: 234970, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1489.26, #queue-req: 0, 
[2025-10-25 15:43:43 TP0] Decode batch [8316], #running-req: 64, #token: 237530, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1488.66, #queue-req: 0, 
[2025-10-25 15:43:45 TP0] Decode batch [8356], #running-req: 64, #token: 240090, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1489.74, #queue-req: 0, 
[2025-10-25 15:43:47 TP0] Decode batch [8396], #running-req: 64, #token: 242650, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1489.40, #queue-req: 0, 
[2025-10-25 15:43:48 TP0] Decode batch [8436], #running-req: 64, #token: 245210, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1487.61, #queue-req: 0, 
[2025-10-25 15:43:50 TP0] Decode batch [8476], #running-req: 64, #token: 247770, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1483.99, #queue-req: 0, 
[2025-10-25 15:43:52 TP0] Decode batch [8516], #running-req: 64, #token: 250330, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1485.91, #queue-req: 0, 
[2025-10-25 15:43:53] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53 TP0] Prefill batch [8554], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:43:53] INFO:     127.0.0.1:41240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:53] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54 TP0] Prefill batch [8555], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-25 15:43:54] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54 TP0] Prefill batch [8556], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 37, 
[2025-10-25 15:43:54] INFO:     127.0.0.1:41638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:54] INFO:     127.0.0.1:41722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:43:55 TP0] Prefill batch [8557], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-25 15:43:55 TP0] Prefill batch [8558], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-25 15:43:56 TP0] Prefill batch [8559], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-25 15:43:57 TP0] Prefill batch [8560], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-25 15:43:58 TP0] Prefill batch [8561], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-25 15:43:59 TP0] Prefill batch [8562], #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 36, #queue-req: 22, 
[2025-10-25 15:43:59 TP0] Prefill batch [8563], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-10-25 15:44:00 TP0] Prefill batch [8564], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-10-25 15:44:01 TP0] Prefill batch [8565], #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-10-25 15:44:02 TP0] Prefill batch [8566], #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.19, #running-req: 57, #queue-req: 1, 
[2025-10-25 15:44:03 TP0] Prefill batch [8567], #new-seq: 1, #new-token: 3193, #cached-token: 8, token usage: 0.21, #running-req: 63, #queue-req: 0, 
[2025-10-25 15:44:04 TP0] Decode batch [8570], #running-req: 64, #token: 204890, token usage: 0.21, cuda graph: True, gen throughput (token/s): 214.34, #queue-req: 0, 
[2025-10-25 15:44:05 TP0] Decode batch [8610], #running-req: 64, #token: 207450, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1539.62, #queue-req: 0, 
[2025-10-25 15:44:07 TP0] Decode batch [8650], #running-req: 64, #token: 210010, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1515.62, #queue-req: 0, 
[2025-10-25 15:44:09 TP0] Decode batch [8690], #running-req: 64, #token: 212570, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1508.93, #queue-req: 0, 
[2025-10-25 15:44:10 TP0] Decode batch [8730], #running-req: 64, #token: 215130, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1506.25, #queue-req: 0, 
[2025-10-25 15:44:12 TP0] Decode batch [8770], #running-req: 64, #token: 217690, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1500.16, #queue-req: 0, 
[2025-10-25 15:44:14 TP0] Decode batch [8810], #running-req: 64, #token: 220250, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1498.62, #queue-req: 0, 
[2025-10-25 15:44:16 TP0] Decode batch [8850], #running-req: 64, #token: 222810, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1495.33, #queue-req: 0, 
[2025-10-25 15:44:17 TP0] Decode batch [8890], #running-req: 64, #token: 225370, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1492.39, #queue-req: 0, 
[2025-10-25 15:44:19 TP0] Decode batch [8930], #running-req: 64, #token: 227930, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.52, #queue-req: 0, 
[2025-10-25 15:44:21 TP0] Decode batch [8970], #running-req: 64, #token: 230490, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.18, #queue-req: 0, 
[2025-10-25 15:44:22 TP0] Decode batch [9010], #running-req: 64, #token: 233050, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1492.56, #queue-req: 0, 
[2025-10-25 15:44:24 TP0] Decode batch [9050], #running-req: 64, #token: 235610, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1494.78, #queue-req: 0, 
[2025-10-25 15:44:26 TP0] Decode batch [9090], #running-req: 64, #token: 238170, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1499.09, #queue-req: 0, 
[2025-10-25 15:44:28 TP0] Decode batch [9130], #running-req: 64, #token: 240730, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1496.75, #queue-req: 0, 
[2025-10-25 15:44:29 TP0] Decode batch [9170], #running-req: 64, #token: 243290, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1494.81, #queue-req: 0, 
[2025-10-25 15:44:31 TP0] Decode batch [9210], #running-req: 64, #token: 245850, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1496.92, #queue-req: 0, 
[2025-10-25 15:44:33 TP0] Decode batch [9250], #running-req: 64, #token: 248410, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1491.77, #queue-req: 0, 
[2025-10-25 15:44:34 TP0] Decode batch [9290], #running-req: 64, #token: 250970, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1493.60, #queue-req: 0, 
[2025-10-25 15:44:36 TP0] Decode batch [9330], #running-req: 64, #token: 253530, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1488.71, #queue-req: 0, 
[2025-10-25 15:44:38] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38 TP0] Prefill batch [9368], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:44:38] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38 TP0] Prefill batch [9369], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-25 15:44:38] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38 TP0] Prefill batch [9370], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-10-25 15:44:38] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:38] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:44:39 TP0] Prefill batch [9371], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-25 15:44:40 TP0] Prefill batch [9372], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-25 15:44:41 TP0] Prefill batch [9373], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-25 15:44:41 TP0] Prefill batch [9374], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-25 15:44:42 TP0] Prefill batch [9375], #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-25 15:44:43 TP0] Prefill batch [9376], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-10-25 15:44:44 TP0] Prefill batch [9377], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-10-25 15:44:45 TP0] Prefill batch [9378], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-10-25 15:44:45 TP0] Prefill batch [9379], #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-10-25 15:44:46 TP0] Prefill batch [9380], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-10-25 15:44:47 TP0] Prefill batch [9381], #new-seq: 3, #new-token: 9597, #cached-token: 6, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-10-25 15:44:48 TP0] Decode batch [9384], #running-req: 64, #token: 204904, token usage: 0.21, cuda graph: True, gen throughput (token/s): 209.64, #queue-req: 0, 
[2025-10-25 15:44:50 TP0] Decode batch [9424], #running-req: 64, #token: 207464, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1554.53, #queue-req: 0, 
[2025-10-25 15:44:52 TP0] Decode batch [9464], #running-req: 64, #token: 210024, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1522.34, #queue-req: 0, 
[2025-10-25 15:44:53 TP0] Decode batch [9504], #running-req: 64, #token: 212584, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1510.70, #queue-req: 0, 
[2025-10-25 15:44:55 TP0] Decode batch [9544], #running-req: 64, #token: 215144, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1506.39, #queue-req: 0, 
[2025-10-25 15:44:57 TP0] Decode batch [9584], #running-req: 64, #token: 217704, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.69, #queue-req: 0, 
[2025-10-25 15:44:58 TP0] Decode batch [9624], #running-req: 64, #token: 220264, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1502.00, #queue-req: 0, 
[2025-10-25 15:45:00 TP0] Decode batch [9664], #running-req: 64, #token: 222824, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1502.26, #queue-req: 0, 
[2025-10-25 15:45:02 TP0] Decode batch [9704], #running-req: 64, #token: 225384, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1500.40, #queue-req: 0, 
[2025-10-25 15:45:04 TP0] Decode batch [9744], #running-req: 64, #token: 227944, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1501.32, #queue-req: 0, 
[2025-10-25 15:45:05 TP0] Decode batch [9784], #running-req: 64, #token: 230504, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1500.22, #queue-req: 0, 
[2025-10-25 15:45:07 TP0] Decode batch [9824], #running-req: 64, #token: 233064, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1500.68, #queue-req: 0, 
[2025-10-25 15:45:09 TP0] Decode batch [9864], #running-req: 64, #token: 235624, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1505.17, #queue-req: 0, 
[2025-10-25 15:45:10 TP0] Decode batch [9904], #running-req: 64, #token: 238184, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1505.42, #queue-req: 0, 
[2025-10-25 15:45:12 TP0] Decode batch [9944], #running-req: 64, #token: 240744, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1507.09, #queue-req: 0, 
[2025-10-25 15:45:14 TP0] Decode batch [9984], #running-req: 64, #token: 243304, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1501.91, #queue-req: 0, 
[2025-10-25 15:45:16 TP0] Decode batch [10024], #running-req: 64, #token: 245864, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1502.07, #queue-req: 0, 
[2025-10-25 15:45:17 TP0] Decode batch [10064], #running-req: 64, #token: 248424, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1492.87, #queue-req: 0, 
[2025-10-25 15:45:19 TP0] Decode batch [10104], #running-req: 64, #token: 250984, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1492.35, #queue-req: 0, 
[2025-10-25 15:45:21 TP0] Decode batch [10144], #running-req: 64, #token: 253544, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1493.13, #queue-req: 0, 
[2025-10-25 15:45:22] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22 TP0] Prefill batch [10182], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:45:22] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22 TP0] Prefill batch [10183], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-25 15:45:22] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:22] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23 TP0] Prefill batch [10184], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-10-25 15:45:23] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:45:23 TP0] Prefill batch [10185], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-10-25 15:45:24 TP0] Prefill batch [10186], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-25 15:45:25 TP0] Prefill batch [10187], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-10-25 15:45:26 TP0] Prefill batch [10188], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-10-25 15:45:27 TP0] Prefill batch [10189], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-10-25 15:45:28 TP0] Prefill batch [10190], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-10-25 15:45:28 TP0] Prefill batch [10191], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-10-25 15:45:29 TP0] Prefill batch [10192], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-10-25 15:45:30 TP0] Prefill batch [10193], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-10-25 15:45:31 TP0] Prefill batch [10194], #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-10-25 15:45:32 TP0] Prefill batch [10195], #new-seq: 3, #new-token: 9593, #cached-token: 10, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-10-25 15:45:33 TP0] Decode batch [10198], #running-req: 64, #token: 204894, token usage: 0.21, cuda graph: True, gen throughput (token/s): 209.40, #queue-req: 0, 
[2025-10-25 15:45:35 TP0] Decode batch [10238], #running-req: 64, #token: 207454, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1551.30, #queue-req: 0, 
[2025-10-25 15:45:36 TP0] Decode batch [10278], #running-req: 64, #token: 210014, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1529.77, #queue-req: 0, 
[2025-10-25 15:45:38 TP0] Decode batch [10318], #running-req: 64, #token: 212574, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1517.75, #queue-req: 0, 
[2025-10-25 15:45:40 TP0] Decode batch [10358], #running-req: 64, #token: 215134, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1511.20, #queue-req: 0, 
[2025-10-25 15:45:41 TP0] Decode batch [10398], #running-req: 64, #token: 217694, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1506.88, #queue-req: 0, 
[2025-10-25 15:45:43 TP0] Decode batch [10438], #running-req: 64, #token: 220254, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1504.31, #queue-req: 0, 
[2025-10-25 15:45:45 TP0] Decode batch [10478], #running-req: 64, #token: 222814, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1499.80, #queue-req: 0, 
[2025-10-25 15:45:46 TP0] Decode batch [10518], #running-req: 64, #token: 225374, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1498.75, #queue-req: 0, 
[2025-10-25 15:45:48 TP0] Decode batch [10558], #running-req: 64, #token: 227934, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1498.84, #queue-req: 0, 
[2025-10-25 15:45:50 TP0] Decode batch [10598], #running-req: 64, #token: 230494, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1498.24, #queue-req: 0, 
[2025-10-25 15:45:52 TP0] Decode batch [10638], #running-req: 64, #token: 233054, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1500.36, #queue-req: 0, 
[2025-10-25 15:45:53 TP0] Decode batch [10678], #running-req: 64, #token: 235614, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1503.85, #queue-req: 0, 
[2025-10-25 15:45:55 TP0] Decode batch [10718], #running-req: 64, #token: 238174, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1500.82, #queue-req: 0, 
[2025-10-25 15:45:57 TP0] Decode batch [10758], #running-req: 64, #token: 240734, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1498.19, #queue-req: 0, 
[2025-10-25 15:45:58 TP0] Decode batch [10798], #running-req: 64, #token: 243294, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1494.39, #queue-req: 0, 
[2025-10-25 15:46:00 TP0] Decode batch [10838], #running-req: 64, #token: 245854, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1489.48, #queue-req: 0, 
[2025-10-25 15:46:02 TP0] Decode batch [10878], #running-req: 64, #token: 248414, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1490.82, #queue-req: 0, 
[2025-10-25 15:46:04 TP0] Decode batch [10918], #running-req: 64, #token: 250974, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1491.51, #queue-req: 0, 
[2025-10-25 15:46:05 TP0] Decode batch [10958], #running-req: 64, #token: 253534, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1488.47, #queue-req: 0, 
[2025-10-25 15:46:07] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07 TP0] Prefill batch [10996], #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:46:07] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07 TP0] Prefill batch [10997], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-25 15:46:07] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:07 TP0] Prefill batch [10998], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-10-25 15:46:07] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:46:08 TP0] Prefill batch [10999], #new-seq: 6, #new-token: 15988, #cached-token: 3218, token usage: 0.04, #running-req: 11, #queue-req: 35, 
[2025-10-25 15:46:09 TP0] Prefill batch [11000], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.06, #running-req: 17, #queue-req: 30, 
[2025-10-25 15:46:10 TP0] Prefill batch [11001], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 25, 
[2025-10-25 15:46:10 TP0] Prefill batch [11002], #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.09, #running-req: 27, #queue-req: 20, 
[2025-10-25 15:46:11 TP0] Prefill batch [11003], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.11, #running-req: 32, #queue-req: 15, 
[2025-10-25 15:46:12 TP0] Prefill batch [11004], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 37, #queue-req: 10, 
[2025-10-25 15:46:13 TP0] Prefill batch [11005], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.14, #running-req: 42, #queue-req: 5, 
[2025-10-25 15:46:14 TP0] Prefill batch [11006], #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.15, #running-req: 47, #queue-req: 0, 
[2025-10-25 15:46:15 TP0] Decode batch [11009], #running-req: 52, #token: 166478, token usage: 0.17, cuda graph: True, gen throughput (token/s): 251.35, #queue-req: 0, 
[2025-10-25 15:46:17 TP0] Decode batch [11049], #running-req: 52, #token: 168558, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1303.51, #queue-req: 0, 
[2025-10-25 15:46:19 TP0] Decode batch [11089], #running-req: 52, #token: 170638, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1285.23, #queue-req: 0, 
[2025-10-25 15:46:20 TP0] Decode batch [11129], #running-req: 52, #token: 172718, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1277.80, #queue-req: 0, 
[2025-10-25 15:46:22 TP0] Decode batch [11169], #running-req: 52, #token: 174798, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1272.15, #queue-req: 0, 
[2025-10-25 15:46:23 TP0] Decode batch [11209], #running-req: 52, #token: 176878, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1267.88, #queue-req: 0, 
[2025-10-25 15:46:25 TP0] Decode batch [11249], #running-req: 52, #token: 178958, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1264.97, #queue-req: 0, 
[2025-10-25 15:46:27 TP0] Decode batch [11289], #running-req: 52, #token: 181038, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1265.88, #queue-req: 0, 
[2025-10-25 15:46:28 TP0] Decode batch [11329], #running-req: 52, #token: 183118, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1262.35, #queue-req: 0, 
[2025-10-25 15:46:30 TP0] Decode batch [11369], #running-req: 52, #token: 185198, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1261.10, #queue-req: 0, 
[2025-10-25 15:46:32 TP0] Decode batch [11409], #running-req: 52, #token: 187278, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1260.95, #queue-req: 0, 
[2025-10-25 15:46:33 TP0] Decode batch [11449], #running-req: 52, #token: 189358, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1258.77, #queue-req: 0, 
[2025-10-25 15:46:35 TP0] Decode batch [11489], #running-req: 52, #token: 191438, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1264.10, #queue-req: 0, 
[2025-10-25 15:46:37 TP0] Decode batch [11529], #running-req: 52, #token: 193518, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1260.94, #queue-req: 0, 
[2025-10-25 15:46:38 TP0] Decode batch [11569], #running-req: 52, #token: 195598, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1258.96, #queue-req: 0, 
[2025-10-25 15:46:40 TP0] Decode batch [11609], #running-req: 52, #token: 197678, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1256.15, #queue-req: 0, 
[2025-10-25 15:46:42 TP0] Decode batch [11649], #running-req: 52, #token: 199758, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1257.54, #queue-req: 0, 
[2025-10-25 15:46:43 TP0] Decode batch [11689], #running-req: 52, #token: 201838, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1255.12, #queue-req: 0, 
[2025-10-25 15:46:45 TP0] Decode batch [11729], #running-req: 52, #token: 203918, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1252.46, #queue-req: 0, 
[2025-10-25 15:46:47 TP0] Decode batch [11769], #running-req: 52, #token: 205998, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1253.68, #queue-req: 0, 
[2025-10-25 15:46:48] INFO:     127.0.0.1:51950 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-25 15:47:06] INFO:     127.0.0.1:50010 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-25 15:47:12] INFO:     127.0.0.1:50022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:12 TP0] Prefill batch [11807], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:47:12 TP0] Decode batch [11810], #running-req: 1, #token: 3204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 78.17, #queue-req: 0, 
[2025-10-25 15:47:13] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:13] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:13 TP0] Prefill batch [11840], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:47:13] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:13] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:13] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:13] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:13] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:14 TP0] Prefill batch [11841], #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-25 15:47:14 TP0] Prefill batch [11842], #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-25 15:47:14 TP0] Prefill batch [11843], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-25 15:47:16 TP0] Decode batch [11854], #running-req: 16, #token: 51362, token usage: 0.05, cuda graph: True, gen throughput (token/s): 43.38, #queue-req: 0, 
[2025-10-25 15:47:17 TP0] Decode batch [11894], #running-req: 16, #token: 52002, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.69, #queue-req: 0, 
[2025-10-25 15:47:18 TP0] Decode batch [11934], #running-req: 16, #token: 52642, token usage: 0.05, cuda graph: True, gen throughput (token/s): 619.98, #queue-req: 0, 
[2025-10-25 15:47:19 TP0] Decode batch [11974], #running-req: 16, #token: 53282, token usage: 0.05, cuda graph: True, gen throughput (token/s): 619.43, #queue-req: 0, 
[2025-10-25 15:47:20 TP0] Decode batch [12014], #running-req: 16, #token: 53922, token usage: 0.06, cuda graph: True, gen throughput (token/s): 618.73, #queue-req: 0, 
[2025-10-25 15:47:21 TP0] Decode batch [12054], #running-req: 16, #token: 54562, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.59, #queue-req: 0, 
[2025-10-25 15:47:22 TP0] Decode batch [12094], #running-req: 16, #token: 55202, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.36, #queue-req: 0, 
[2025-10-25 15:47:23 TP0] Decode batch [12134], #running-req: 16, #token: 55842, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.95, #queue-req: 0, 
[2025-10-25 15:47:25 TP0] Decode batch [12174], #running-req: 16, #token: 56482, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.65, #queue-req: 0, 
[2025-10-25 15:47:26 TP0] Decode batch [12214], #running-req: 16, #token: 57122, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.37, #queue-req: 0, 
[2025-10-25 15:47:27 TP0] Decode batch [12254], #running-req: 16, #token: 57762, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.76, #queue-req: 0, 
[2025-10-25 15:47:28 TP0] Decode batch [12294], #running-req: 16, #token: 58402, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.82, #queue-req: 0, 
[2025-10-25 15:47:29 TP0] Decode batch [12334], #running-req: 16, #token: 59042, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.84, #queue-req: 0, 
[2025-10-25 15:47:30 TP0] Decode batch [12374], #running-req: 16, #token: 59682, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.58, #queue-req: 0, 
[2025-10-25 15:47:31 TP0] Decode batch [12414], #running-req: 16, #token: 60322, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.54, #queue-req: 0, 
[2025-10-25 15:47:32 TP0] Decode batch [12454], #running-req: 16, #token: 60962, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.72, #queue-req: 0, 
[2025-10-25 15:47:33 TP0] Decode batch [12494], #running-req: 16, #token: 61602, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.42, #queue-req: 0, 
[2025-10-25 15:47:34 TP0] Decode batch [12534], #running-req: 16, #token: 62242, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.59, #queue-req: 0, 
[2025-10-25 15:47:35 TP0] Decode batch [12574], #running-req: 16, #token: 62882, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.38, #queue-req: 0, 
[2025-10-25 15:47:36 TP0] Decode batch [12614], #running-req: 16, #token: 63522, token usage: 0.07, cuda graph: True, gen throughput (token/s): 607.26, #queue-req: 0, 
[2025-10-25 15:47:37] INFO:     127.0.0.1:40054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37 TP0] Prefill batch [12644], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:47:37] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:47:37 TP0] Prefill batch [12645], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-25 15:47:37 TP0] Prefill batch [12646], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-25 15:47:38 TP0] Prefill batch [12647], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-25 15:47:40 TP0] Decode batch [12658], #running-req: 16, #token: 51356, token usage: 0.05, cuda graph: True, gen throughput (token/s): 174.30, #queue-req: 0, 
[2025-10-25 15:47:41 TP0] Decode batch [12698], #running-req: 16, #token: 51996, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.59, #queue-req: 0, 
[2025-10-25 15:47:42 TP0] Decode batch [12738], #running-req: 16, #token: 52636, token usage: 0.05, cuda graph: True, gen throughput (token/s): 607.90, #queue-req: 0, 
[2025-10-25 15:47:43 TP0] Decode batch [12778], #running-req: 16, #token: 53276, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.75, #queue-req: 0, 
[2025-10-25 15:47:44 TP0] Decode batch [12818], #running-req: 16, #token: 53916, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.99, #queue-req: 0, 
[2025-10-25 15:47:45 TP0] Decode batch [12858], #running-req: 16, #token: 54556, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.85, #queue-req: 0, 
[2025-10-25 15:47:46 TP0] Decode batch [12898], #running-req: 16, #token: 55196, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.87, #queue-req: 0, 
[2025-10-25 15:47:47 TP0] Decode batch [12938], #running-req: 16, #token: 55836, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.69, #queue-req: 0, 
[2025-10-25 15:47:48 TP0] Decode batch [12978], #running-req: 16, #token: 56476, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.84, #queue-req: 0, 
[2025-10-25 15:47:49 TP0] Decode batch [13018], #running-req: 16, #token: 57116, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.33, #queue-req: 0, 
[2025-10-25 15:47:50 TP0] Decode batch [13058], #running-req: 16, #token: 57756, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.57, #queue-req: 0, 
[2025-10-25 15:47:51 TP0] Decode batch [13098], #running-req: 16, #token: 58396, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.42, #queue-req: 0, 
[2025-10-25 15:47:52 TP0] Decode batch [13138], #running-req: 16, #token: 59036, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.23, #queue-req: 0, 
[2025-10-25 15:47:53 TP0] Decode batch [13178], #running-req: 16, #token: 59676, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.09, #queue-req: 0, 
[2025-10-25 15:47:54 TP0] Decode batch [13218], #running-req: 16, #token: 60316, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.13, #queue-req: 0, 
[2025-10-25 15:47:55 TP0] Decode batch [13258], #running-req: 16, #token: 60956, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.04, #queue-req: 0, 
[2025-10-25 15:47:57 TP0] Decode batch [13298], #running-req: 16, #token: 61596, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.46, #queue-req: 0, 
[2025-10-25 15:47:58 TP0] Decode batch [13338], #running-req: 16, #token: 62236, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.71, #queue-req: 0, 
[2025-10-25 15:47:59 TP0] Decode batch [13378], #running-req: 16, #token: 62876, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.34, #queue-req: 0, 
[2025-10-25 15:48:00 TP0] Decode batch [13418], #running-req: 16, #token: 63516, token usage: 0.07, cuda graph: True, gen throughput (token/s): 607.43, #queue-req: 0, 
[2025-10-25 15:48:00] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:00] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:00 TP0] Prefill batch [13448], #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:48:00] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:00] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:00] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:00] INFO:     127.0.0.1:34658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:00] INFO:     127.0.0.1:34668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01] INFO:     127.0.0.1:34696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01] INFO:     127.0.0.1:34722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01] INFO:     127.0.0.1:34742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:01 TP0] Prefill batch [13449], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-25 15:48:01 TP0] Prefill batch [13450], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-25 15:48:02 TP0] Prefill batch [13451], #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-25 15:48:03 TP0] Decode batch [13462], #running-req: 16, #token: 51358, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.53, #queue-req: 0, 
[2025-10-25 15:48:04 TP0] Decode batch [13502], #running-req: 16, #token: 51998, token usage: 0.05, cuda graph: True, gen throughput (token/s): 625.80, #queue-req: 0, 
[2025-10-25 15:48:05 TP0] Decode batch [13542], #running-req: 16, #token: 52638, token usage: 0.05, cuda graph: True, gen throughput (token/s): 625.47, #queue-req: 0, 
[2025-10-25 15:48:06 TP0] Decode batch [13582], #running-req: 16, #token: 53278, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.04, #queue-req: 0, 
[2025-10-25 15:48:07 TP0] Decode batch [13622], #running-req: 16, #token: 53918, token usage: 0.06, cuda graph: True, gen throughput (token/s): 618.63, #queue-req: 0, 
[2025-10-25 15:48:09 TP0] Decode batch [13662], #running-req: 16, #token: 54558, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.01, #queue-req: 0, 
[2025-10-25 15:48:10 TP0] Decode batch [13702], #running-req: 16, #token: 55198, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.93, #queue-req: 0, 
[2025-10-25 15:48:11 TP0] Decode batch [13742], #running-req: 16, #token: 55838, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.83, #queue-req: 0, 
[2025-10-25 15:48:12 TP0] Decode batch [13782], #running-req: 16, #token: 56478, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.65, #queue-req: 0, 
[2025-10-25 15:48:13 TP0] Decode batch [13822], #running-req: 16, #token: 57118, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.25, #queue-req: 0, 
[2025-10-25 15:48:14 TP0] Decode batch [13862], #running-req: 16, #token: 57758, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.88, #queue-req: 0, 
[2025-10-25 15:48:15 TP0] Decode batch [13902], #running-req: 16, #token: 58398, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.75, #queue-req: 0, 
[2025-10-25 15:48:16 TP0] Decode batch [13942], #running-req: 16, #token: 59038, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.81, #queue-req: 0, 
[2025-10-25 15:48:17 TP0] Decode batch [13982], #running-req: 16, #token: 59678, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.33, #queue-req: 0, 
[2025-10-25 15:48:18 TP0] Decode batch [14022], #running-req: 16, #token: 60318, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.55, #queue-req: 0, 
[2025-10-25 15:48:19 TP0] Decode batch [14062], #running-req: 16, #token: 60958, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.96, #queue-req: 0, 
[2025-10-25 15:48:20 TP0] Decode batch [14102], #running-req: 16, #token: 61598, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.15, #queue-req: 0, 
[2025-10-25 15:48:21 TP0] Decode batch [14142], #running-req: 16, #token: 62238, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.05, #queue-req: 0, 
[2025-10-25 15:48:22 TP0] Decode batch [14182], #running-req: 16, #token: 62878, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.46, #queue-req: 0, 
[2025-10-25 15:48:23 TP0] Decode batch [14222], #running-req: 16, #token: 63518, token usage: 0.07, cuda graph: True, gen throughput (token/s): 611.65, #queue-req: 0, 
[2025-10-25 15:48:24] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24 TP0] Prefill batch [14252], #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:48:24] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24] INFO:     127.0.0.1:34790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:24 TP0] Prefill batch [14253], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-25 15:48:24 TP0] Prefill batch [14254], #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-25 15:48:25 TP0] Prefill batch [14255], #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-25 15:48:27 TP0] Decode batch [14266], #running-req: 16, #token: 51361, token usage: 0.05, cuda graph: True, gen throughput (token/s): 174.55, #queue-req: 0, 
[2025-10-25 15:48:28 TP0] Decode batch [14306], #running-req: 16, #token: 52001, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.89, #queue-req: 0, 
[2025-10-25 15:48:29 TP0] Decode batch [14346], #running-req: 16, #token: 52641, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.26, #queue-req: 0, 
[2025-10-25 15:48:30 TP0] Decode batch [14386], #running-req: 16, #token: 53281, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.72, #queue-req: 0, 
[2025-10-25 15:48:31 TP0] Decode batch [14426], #running-req: 16, #token: 53921, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.76, #queue-req: 0, 
[2025-10-25 15:48:32 TP0] Decode batch [14466], #running-req: 16, #token: 54561, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.95, #queue-req: 0, 
[2025-10-25 15:48:33 TP0] Decode batch [14506], #running-req: 16, #token: 55201, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.27, #queue-req: 0, 
[2025-10-25 15:48:34 TP0] Decode batch [14546], #running-req: 16, #token: 55841, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.69, #queue-req: 0, 
[2025-10-25 15:48:35 TP0] Decode batch [14586], #running-req: 16, #token: 56481, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.71, #queue-req: 0, 
[2025-10-25 15:48:36 TP0] Decode batch [14626], #running-req: 16, #token: 57121, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.07, #queue-req: 0, 
[2025-10-25 15:48:37 TP0] Decode batch [14666], #running-req: 16, #token: 57761, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.40, #queue-req: 0, 
[2025-10-25 15:48:38 TP0] Decode batch [14706], #running-req: 16, #token: 58401, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.53, #queue-req: 0, 
[2025-10-25 15:48:39 TP0] Decode batch [14746], #running-req: 16, #token: 59041, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.30, #queue-req: 0, 
[2025-10-25 15:48:40 TP0] Decode batch [14786], #running-req: 16, #token: 59681, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.77, #queue-req: 0, 
[2025-10-25 15:48:42 TP0] Decode batch [14826], #running-req: 16, #token: 60321, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.37, #queue-req: 0, 
[2025-10-25 15:48:43 TP0] Decode batch [14866], #running-req: 16, #token: 60961, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.58, #queue-req: 0, 
[2025-10-25 15:48:44 TP0] Decode batch [14906], #running-req: 16, #token: 61601, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.59, #queue-req: 0, 
[2025-10-25 15:48:45 TP0] Decode batch [14946], #running-req: 16, #token: 62241, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.96, #queue-req: 0, 
[2025-10-25 15:48:46 TP0] Decode batch [14986], #running-req: 16, #token: 62881, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.32, #queue-req: 0, 
[2025-10-25 15:48:47 TP0] Decode batch [15026], #running-req: 16, #token: 63521, token usage: 0.07, cuda graph: True, gen throughput (token/s): 609.16, #queue-req: 0, 
[2025-10-25 15:48:48] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48 TP0] Prefill batch [15056], #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:48:48] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48] INFO:     127.0.0.1:57778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:48:48 TP0] Prefill batch [15057], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-25 15:48:48 TP0] Prefill batch [15058], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-25 15:48:49 TP0] Prefill batch [15059], #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-25 15:48:50 TP0] Decode batch [15070], #running-req: 16, #token: 51356, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.14, #queue-req: 0, 
[2025-10-25 15:48:51 TP0] Decode batch [15110], #running-req: 16, #token: 51996, token usage: 0.05, cuda graph: True, gen throughput (token/s): 625.34, #queue-req: 0, 
[2025-10-25 15:48:53 TP0] Decode batch [15150], #running-req: 16, #token: 52636, token usage: 0.05, cuda graph: True, gen throughput (token/s): 623.88, #queue-req: 0, 
[2025-10-25 15:48:54 TP0] Decode batch [15190], #running-req: 16, #token: 53276, token usage: 0.05, cuda graph: True, gen throughput (token/s): 623.48, #queue-req: 0, 
[2025-10-25 15:48:55 TP0] Decode batch [15230], #running-req: 16, #token: 53916, token usage: 0.06, cuda graph: True, gen throughput (token/s): 619.30, #queue-req: 0, 
[2025-10-25 15:48:56 TP0] Decode batch [15270], #running-req: 16, #token: 54556, token usage: 0.06, cuda graph: True, gen throughput (token/s): 618.80, #queue-req: 0, 
[2025-10-25 15:48:57 TP0] Decode batch [15310], #running-req: 16, #token: 55196, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.12, #queue-req: 0, 
[2025-10-25 15:48:58 TP0] Decode batch [15350], #running-req: 16, #token: 55836, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.03, #queue-req: 0, 
[2025-10-25 15:48:59 TP0] Decode batch [15390], #running-req: 16, #token: 56476, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.84, #queue-req: 0, 
[2025-10-25 15:49:00 TP0] Decode batch [15430], #running-req: 16, #token: 57116, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.53, #queue-req: 0, 
[2025-10-25 15:49:01 TP0] Decode batch [15470], #running-req: 16, #token: 57756, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.48, #queue-req: 0, 
[2025-10-25 15:49:02 TP0] Decode batch [15510], #running-req: 16, #token: 58396, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.46, #queue-req: 0, 
[2025-10-25 15:49:03 TP0] Decode batch [15550], #running-req: 16, #token: 59036, token usage: 0.06, cuda graph: True, gen throughput (token/s): 622.95, #queue-req: 0, 
[2025-10-25 15:49:04 TP0] Decode batch [15590], #running-req: 16, #token: 59676, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.48, #queue-req: 0, 
[2025-10-25 15:49:05 TP0] Decode batch [15630], #running-req: 16, #token: 60316, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.73, #queue-req: 0, 
[2025-10-25 15:49:06 TP0] Decode batch [15670], #running-req: 16, #token: 60956, token usage: 0.06, cuda graph: True, gen throughput (token/s): 619.16, #queue-req: 0, 
[2025-10-25 15:49:07 TP0] Decode batch [15710], #running-req: 16, #token: 61596, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.57, #queue-req: 0, 
[2025-10-25 15:49:08 TP0] Decode batch [15750], #running-req: 16, #token: 62236, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.10, #queue-req: 0, 
[2025-10-25 15:49:09 TP0] Decode batch [15790], #running-req: 16, #token: 62876, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.90, #queue-req: 0, 
[2025-10-25 15:49:10 TP0] Decode batch [15830], #running-req: 16, #token: 63516, token usage: 0.07, cuda graph: True, gen throughput (token/s): 613.75, #queue-req: 0, 
[2025-10-25 15:49:11] INFO:     127.0.0.1:40128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11 TP0] Prefill batch [15860], #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:49:11] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11] INFO:     127.0.0.1:40272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:11 TP0] Prefill batch [15861], #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-25 15:49:11 TP0] Prefill batch [15862], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-25 15:49:12 TP0] Prefill batch [15863], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-25 15:49:14 TP0] Decode batch [15874], #running-req: 16, #token: 51356, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.89, #queue-req: 0, 
[2025-10-25 15:49:15 TP0] Decode batch [15914], #running-req: 16, #token: 51996, token usage: 0.05, cuda graph: True, gen throughput (token/s): 623.45, #queue-req: 0, 
[2025-10-25 15:49:16 TP0] Decode batch [15954], #running-req: 16, #token: 52636, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.53, #queue-req: 0, 
[2025-10-25 15:49:17 TP0] Decode batch [15994], #running-req: 16, #token: 53276, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.19, #queue-req: 0, 
[2025-10-25 15:49:18 TP0] Decode batch [16034], #running-req: 16, #token: 53916, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.07, #queue-req: 0, 
[2025-10-25 15:49:19 TP0] Decode batch [16074], #running-req: 16, #token: 54556, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.27, #queue-req: 0, 
[2025-10-25 15:49:20 TP0] Decode batch [16114], #running-req: 16, #token: 55196, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.83, #queue-req: 0, 
[2025-10-25 15:49:21 TP0] Decode batch [16154], #running-req: 16, #token: 55836, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.29, #queue-req: 0, 
[2025-10-25 15:49:22 TP0] Decode batch [16194], #running-req: 16, #token: 56476, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.76, #queue-req: 0, 
[2025-10-25 15:49:23 TP0] Decode batch [16234], #running-req: 16, #token: 57116, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.17, #queue-req: 0, 
[2025-10-25 15:49:24 TP0] Decode batch [16274], #running-req: 16, #token: 57756, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.19, #queue-req: 0, 
[2025-10-25 15:49:25 TP0] Decode batch [16314], #running-req: 16, #token: 58396, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.74, #queue-req: 0, 
[2025-10-25 15:49:26 TP0] Decode batch [16354], #running-req: 16, #token: 59036, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.10, #queue-req: 0, 
[2025-10-25 15:49:27 TP0] Decode batch [16394], #running-req: 16, #token: 59676, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.72, #queue-req: 0, 
[2025-10-25 15:49:29 TP0] Decode batch [16434], #running-req: 16, #token: 60316, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.30, #queue-req: 0, 
[2025-10-25 15:49:30 TP0] Decode batch [16474], #running-req: 16, #token: 60956, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.15, #queue-req: 0, 
[2025-10-25 15:49:31 TP0] Decode batch [16514], #running-req: 16, #token: 61596, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.58, #queue-req: 0, 
[2025-10-25 15:49:32 TP0] Decode batch [16554], #running-req: 16, #token: 62236, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.34, #queue-req: 0, 
[2025-10-25 15:49:33 TP0] Decode batch [16594], #running-req: 16, #token: 62876, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.91, #queue-req: 0, 
[2025-10-25 15:49:34 TP0] Decode batch [16634], #running-req: 16, #token: 63516, token usage: 0.07, cuda graph: True, gen throughput (token/s): 610.20, #queue-req: 0, 
[2025-10-25 15:49:35] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35 TP0] Prefill batch [16664], #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:49:35] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:35 TP0] Prefill batch [16665], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-25 15:49:35 TP0] Prefill batch [16666], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-25 15:49:36 TP0] Prefill batch [16667], #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-25 15:49:37 TP0] Decode batch [16678], #running-req: 16, #token: 51358, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.82, #queue-req: 0, 
[2025-10-25 15:49:38 TP0] Decode batch [16718], #running-req: 16, #token: 51998, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.84, #queue-req: 0, 
[2025-10-25 15:49:40 TP0] Decode batch [16758], #running-req: 16, #token: 52638, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.58, #queue-req: 0, 
[2025-10-25 15:49:41 TP0] Decode batch [16798], #running-req: 16, #token: 53278, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.32, #queue-req: 0, 
[2025-10-25 15:49:42 TP0] Decode batch [16838], #running-req: 16, #token: 53918, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.31, #queue-req: 0, 
[2025-10-25 15:49:43 TP0] Decode batch [16878], #running-req: 16, #token: 54558, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.50, #queue-req: 0, 
[2025-10-25 15:49:44 TP0] Decode batch [16918], #running-req: 16, #token: 55198, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.78, #queue-req: 0, 
[2025-10-25 15:49:45 TP0] Decode batch [16958], #running-req: 16, #token: 55838, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.75, #queue-req: 0, 
[2025-10-25 15:49:46 TP0] Decode batch [16998], #running-req: 16, #token: 56478, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.59, #queue-req: 0, 
[2025-10-25 15:49:47 TP0] Decode batch [17038], #running-req: 16, #token: 57118, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.72, #queue-req: 0, 
[2025-10-25 15:49:48 TP0] Decode batch [17078], #running-req: 16, #token: 57758, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.62, #queue-req: 0, 
[2025-10-25 15:49:49 TP0] Decode batch [17118], #running-req: 16, #token: 58398, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.04, #queue-req: 0, 
[2025-10-25 15:49:50 TP0] Decode batch [17158], #running-req: 16, #token: 59038, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.79, #queue-req: 0, 
[2025-10-25 15:49:51 TP0] Decode batch [17198], #running-req: 16, #token: 59678, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.06, #queue-req: 0, 
[2025-10-25 15:49:52 TP0] Decode batch [17238], #running-req: 16, #token: 60318, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.47, #queue-req: 0, 
[2025-10-25 15:49:53 TP0] Decode batch [17278], #running-req: 16, #token: 60958, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.56, #queue-req: 0, 
[2025-10-25 15:49:54 TP0] Decode batch [17318], #running-req: 16, #token: 61598, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.59, #queue-req: 0, 
[2025-10-25 15:49:55 TP0] Decode batch [17358], #running-req: 16, #token: 62238, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.02, #queue-req: 0, 
[2025-10-25 15:49:56 TP0] Decode batch [17398], #running-req: 16, #token: 62878, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.22, #queue-req: 0, 
[2025-10-25 15:49:57 TP0] Decode batch [17438], #running-req: 16, #token: 63518, token usage: 0.07, cuda graph: True, gen throughput (token/s): 605.56, #queue-req: 0, 
[2025-10-25 15:49:58] INFO:     127.0.0.1:43666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58 TP0] Prefill batch [17468], #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:49:58] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58] INFO:     127.0.0.1:43798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:49:58 TP0] Prefill batch [17469], #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-25 15:49:59 TP0] Prefill batch [17470], #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-25 15:49:59 TP0] Prefill batch [17471], #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-25 15:50:01 TP0] Decode batch [17482], #running-req: 16, #token: 51359, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.65, #queue-req: 0, 
[2025-10-25 15:50:02 TP0] Decode batch [17522], #running-req: 16, #token: 51999, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.98, #queue-req: 0, 
[2025-10-25 15:50:03 TP0] Decode batch [17562], #running-req: 16, #token: 52639, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.82, #queue-req: 0, 
[2025-10-25 15:50:04 TP0] Decode batch [17602], #running-req: 16, #token: 53279, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.20, #queue-req: 0, 
[2025-10-25 15:50:05 TP0] Decode batch [17642], #running-req: 16, #token: 53919, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.41, #queue-req: 0, 
[2025-10-25 15:50:06 TP0] Decode batch [17682], #running-req: 16, #token: 54559, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.29, #queue-req: 0, 
[2025-10-25 15:50:07 TP0] Decode batch [17722], #running-req: 16, #token: 55199, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.39, #queue-req: 0, 
[2025-10-25 15:50:09 TP0] Decode batch [17762], #running-req: 16, #token: 55839, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.53, #queue-req: 0, 
[2025-10-25 15:50:10 TP0] Decode batch [17802], #running-req: 16, #token: 56479, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.12, #queue-req: 0, 
[2025-10-25 15:50:11 TP0] Decode batch [17842], #running-req: 16, #token: 57119, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.19, #queue-req: 0, 
[2025-10-25 15:50:12 TP0] Decode batch [17882], #running-req: 16, #token: 57759, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.46, #queue-req: 0, 
[2025-10-25 15:50:13 TP0] Decode batch [17922], #running-req: 16, #token: 58399, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.09, #queue-req: 0, 
[2025-10-25 15:50:14 TP0] Decode batch [17962], #running-req: 16, #token: 59039, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.04, #queue-req: 0, 
[2025-10-25 15:50:15 TP0] Decode batch [18002], #running-req: 16, #token: 59679, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.66, #queue-req: 0, 
[2025-10-25 15:50:16 TP0] Decode batch [18042], #running-req: 16, #token: 60319, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.19, #queue-req: 0, 
[2025-10-25 15:50:17 TP0] Decode batch [18082], #running-req: 16, #token: 60959, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.32, #queue-req: 0, 
[2025-10-25 15:50:18 TP0] Decode batch [18122], #running-req: 16, #token: 61599, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.24, #queue-req: 0, 
[2025-10-25 15:50:19 TP0] Decode batch [18162], #running-req: 16, #token: 62239, token usage: 0.06, cuda graph: True, gen throughput (token/s): 602.90, #queue-req: 0, 
[2025-10-25 15:50:20 TP0] Decode batch [18202], #running-req: 16, #token: 62879, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.12, #queue-req: 0, 
[2025-10-25 15:50:21 TP0] Decode batch [18242], #running-req: 16, #token: 63519, token usage: 0.07, cuda graph: True, gen throughput (token/s): 603.08, #queue-req: 0, 
[2025-10-25 15:50:22] INFO:     127.0.0.1:42306 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-25 15:50:39] INFO:     127.0.0.1:49064 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-25 15:50:45] INFO:     127.0.0.1:37350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:50:45 TP0] Prefill batch [18272], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:50:45 TP0] Decode batch [18283], #running-req: 1, #token: 3212, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.29, #queue-req: 0, 
[2025-10-25 15:50:47] INFO:     127.0.0.1:37366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:50:47] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:50:47 TP0] Prefill batch [18305], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:50:47] INFO:     127.0.0.1:37374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:50:47] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:50:47 TP0] Prefill batch [18306], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:50:47 TP0] Decode batch [18325], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 47.28, #queue-req: 0, 
[2025-10-25 15:50:48 TP0] Decode batch [18365], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-25 15:50:49 TP0] Decode batch [18405], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-25 15:50:50 TP0] Decode batch [18445], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-25 15:50:51 TP0] Decode batch [18485], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:50:51 TP0] Decode batch [18525], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:50:52 TP0] Decode batch [18565], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-25 15:50:53 TP0] Decode batch [18605], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:50:54 TP0] Decode batch [18645], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:50:55 TP0] Decode batch [18685], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:50:56 TP0] Decode batch [18725], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:50:56 TP0] Decode batch [18765], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:50:57 TP0] Decode batch [18805], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:50:58 TP0] Decode batch [18845], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:50:59 TP0] Decode batch [18885], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:51:00 TP0] Decode batch [18925], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:51:01 TP0] Decode batch [18965], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:51:01 TP0] Decode batch [19005], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:51:02 TP0] Decode batch [19045], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:51:03 TP0] Decode batch [19085], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:51:03] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:03] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:03] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:03 TP0] Prefill batch [19107], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:51:03] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:04 TP0] Prefill batch [19108], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:51:04 TP0] Decode batch [19127], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.76, #queue-req: 0, 
[2025-10-25 15:51:05 TP0] Decode batch [19167], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-25 15:51:06 TP0] Decode batch [19207], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-25 15:51:06 TP0] Decode batch [19247], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-25 15:51:07 TP0] Decode batch [19287], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:51:08 TP0] Decode batch [19327], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-25 15:51:09 TP0] Decode batch [19367], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:51:10 TP0] Decode batch [19407], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:51:11 TP0] Decode batch [19447], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:51:11 TP0] Decode batch [19487], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:51:12 TP0] Decode batch [19527], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:51:13 TP0] Decode batch [19567], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:51:14 TP0] Decode batch [19607], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:51:15 TP0] Decode batch [19647], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:51:16 TP0] Decode batch [19687], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:51:16 TP0] Decode batch [19727], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:51:17 TP0] Decode batch [19767], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:51:18 TP0] Decode batch [19807], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:51:19 TP0] Decode batch [19847], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 189.20, #queue-req: 0, 
[2025-10-25 15:51:20 TP0] Decode batch [19887], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:51:20] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:20] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:20] INFO:     127.0.0.1:51440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:20 TP0] Prefill batch [19909], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:51:20] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:20 TP0] Prefill batch [19910], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:51:21 TP0] Decode batch [19929], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.35, #queue-req: 0, 
[2025-10-25 15:51:22 TP0] Decode batch [19969], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-25 15:51:22 TP0] Decode batch [20009], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-25 15:51:23 TP0] Decode batch [20049], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-25 15:51:24 TP0] Decode batch [20089], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-25 15:51:25 TP0] Decode batch [20129], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-25 15:51:26 TP0] Decode batch [20169], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-25 15:51:27 TP0] Decode batch [20209], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-25 15:51:27 TP0] Decode batch [20249], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:51:28 TP0] Decode batch [20289], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:51:29 TP0] Decode batch [20329], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:51:30 TP0] Decode batch [20369], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:51:31 TP0] Decode batch [20409], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:51:31 TP0] Decode batch [20449], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:51:32 TP0] Decode batch [20489], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:51:33 TP0] Decode batch [20529], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-25 15:51:34 TP0] Decode batch [20569], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:51:35 TP0] Decode batch [20609], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:51:36 TP0] Decode batch [20649], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:51:36 TP0] Decode batch [20689], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:51:37] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:37] INFO:     127.0.0.1:47870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:37 TP0] Prefill batch [20711], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:51:37] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:37] INFO:     127.0.0.1:47900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:37 TP0] Prefill batch [20712], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:51:37 TP0] Decode batch [20731], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.49, #queue-req: 0, 
[2025-10-25 15:51:38 TP0] Decode batch [20771], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-25 15:51:39 TP0] Decode batch [20811], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-25 15:51:40 TP0] Decode batch [20851], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:51:41 TP0] Decode batch [20891], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:51:42 TP0] Decode batch [20931], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-25 15:51:42 TP0] Decode batch [20971], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-25 15:51:43 TP0] Decode batch [21011], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:51:44 TP0] Decode batch [21051], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:51:45 TP0] Decode batch [21091], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:51:46 TP0] Decode batch [21131], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:51:47 TP0] Decode batch [21171], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:51:47 TP0] Decode batch [21211], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:51:48 TP0] Decode batch [21251], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:51:49 TP0] Decode batch [21291], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:51:50 TP0] Decode batch [21331], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:51:51 TP0] Decode batch [21371], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:51:51 TP0] Decode batch [21411], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 189.36, #queue-req: 0, 
[2025-10-25 15:51:52 TP0] Decode batch [21451], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:51:53 TP0] Decode batch [21491], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:51:54] INFO:     127.0.0.1:55324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:54] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:54] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:54 TP0] Prefill batch [21513], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:51:54] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:51:54 TP0] Prefill batch [21514], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:51:54 TP0] Decode batch [21533], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.88, #queue-req: 0, 
[2025-10-25 15:51:55 TP0] Decode batch [21573], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-25 15:51:56 TP0] Decode batch [21613], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-25 15:51:57 TP0] Decode batch [21653], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-25 15:51:57 TP0] Decode batch [21693], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-25 15:51:58 TP0] Decode batch [21733], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-25 15:51:59 TP0] Decode batch [21773], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:52:00 TP0] Decode batch [21813], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:52:01 TP0] Decode batch [21853], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:52:02 TP0] Decode batch [21893], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-25 15:52:02 TP0] Decode batch [21933], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:52:03 TP0] Decode batch [21973], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:52:04 TP0] Decode batch [22013], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:52:05 TP0] Decode batch [22053], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:52:06 TP0] Decode batch [22093], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:52:07 TP0] Decode batch [22133], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:52:07 TP0] Decode batch [22173], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:52:08 TP0] Decode batch [22213], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:52:09 TP0] Decode batch [22253], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:52:10 TP0] Decode batch [22293], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:52:10] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:10] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:10] INFO:     127.0.0.1:50250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:10 TP0] Prefill batch [22315], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:52:10] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:10 TP0] Prefill batch [22316], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:52:11 TP0] Decode batch [22335], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.41, #queue-req: 0, 
[2025-10-25 15:52:12 TP0] Decode batch [22375], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-25 15:52:13 TP0] Decode batch [22415], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-25 15:52:13 TP0] Decode batch [22455], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-25 15:52:14 TP0] Decode batch [22495], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:52:15 TP0] Decode batch [22535], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:52:16 TP0] Decode batch [22575], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:52:17 TP0] Decode batch [22615], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:52:17 TP0] Decode batch [22655], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:52:18 TP0] Decode batch [22695], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-25 15:52:19 TP0] Decode batch [22735], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:52:20 TP0] Decode batch [22775], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:52:21 TP0] Decode batch [22815], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:52:22 TP0] Decode batch [22855], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:52:22 TP0] Decode batch [22895], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:52:23 TP0] Decode batch [22935], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:52:24 TP0] Decode batch [22975], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:52:25 TP0] Decode batch [23015], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 190.15, #queue-req: 0, 
[2025-10-25 15:52:26 TP0] Decode batch [23055], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:52:27 TP0] Decode batch [23095], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:52:27] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:27] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:27] INFO:     127.0.0.1:44286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:27 TP0] Prefill batch [23117], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:52:27] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:27 TP0] Prefill batch [23118], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:52:28 TP0] Decode batch [23137], #running-req: 4, #token: 12872, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.32, #queue-req: 0, 
[2025-10-25 15:52:28 TP0] Decode batch [23177], #running-req: 4, #token: 13032, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-25 15:52:29 TP0] Decode batch [23217], #running-req: 4, #token: 13192, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-25 15:52:30 TP0] Decode batch [23257], #running-req: 4, #token: 13352, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-25 15:52:31 TP0] Decode batch [23297], #running-req: 4, #token: 13512, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:52:32 TP0] Decode batch [23337], #running-req: 4, #token: 13672, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:52:33 TP0] Decode batch [23377], #running-req: 4, #token: 13832, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:52:33 TP0] Decode batch [23417], #running-req: 4, #token: 13992, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:52:34 TP0] Decode batch [23457], #running-req: 4, #token: 14152, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:52:35 TP0] Decode batch [23497], #running-req: 4, #token: 14312, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:52:36 TP0] Decode batch [23537], #running-req: 4, #token: 14472, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:52:37 TP0] Decode batch [23577], #running-req: 4, #token: 14632, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:52:37 TP0] Decode batch [23617], #running-req: 4, #token: 14792, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:52:38 TP0] Decode batch [23657], #running-req: 4, #token: 14952, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:52:39 TP0] Decode batch [23697], #running-req: 4, #token: 15112, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:52:40 TP0] Decode batch [23737], #running-req: 4, #token: 15272, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:52:41 TP0] Decode batch [23777], #running-req: 4, #token: 15432, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:52:42 TP0] Decode batch [23817], #running-req: 4, #token: 15592, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:52:42 TP0] Decode batch [23857], #running-req: 4, #token: 15752, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:52:43 TP0] Decode batch [23897], #running-req: 4, #token: 15912, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:52:44] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:44] INFO:     127.0.0.1:39152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:44] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:44 TP0] Prefill batch [23919], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:52:44] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:52:44 TP0] Prefill batch [23920], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:52:44 TP0] Decode batch [23939], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.40, #queue-req: 0, 
[2025-10-25 15:52:45 TP0] Decode batch [23979], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:52:46 TP0] Decode batch [24019], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:52:47 TP0] Decode batch [24059], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:52:48 TP0] Decode batch [24099], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:52:48 TP0] Decode batch [24139], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:52:49 TP0] Decode batch [24179], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:52:50 TP0] Decode batch [24219], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:52:51 TP0] Decode batch [24259], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:52:52 TP0] Decode batch [24299], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:52:53 TP0] Decode batch [24339], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:52:53 TP0] Decode batch [24379], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:52:54 TP0] Decode batch [24419], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:52:55 TP0] Decode batch [24459], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:52:56 TP0] Decode batch [24499], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:52:57 TP0] Decode batch [24539], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:52:58 TP0] Decode batch [24579], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 189.01, #queue-req: 0, 
[2025-10-25 15:52:58 TP0] Decode batch [24619], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:52:59 TP0] Decode batch [24659], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-25 15:53:00 TP0] Decode batch [24699], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:53:00] INFO:     127.0.0.1:51748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:00] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:00] INFO:     127.0.0.1:51758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:00 TP0] Prefill batch [24721], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:53:00] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:01 TP0] Prefill batch [24722], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:53:01 TP0] Decode batch [24741], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.36, #queue-req: 0, 
[2025-10-25 15:53:02 TP0] Decode batch [24781], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-10-25 15:53:03 TP0] Decode batch [24821], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-25 15:53:03 TP0] Decode batch [24861], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-25 15:53:04 TP0] Decode batch [24901], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:53:05 TP0] Decode batch [24941], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:53:06 TP0] Decode batch [24981], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:53:07 TP0] Decode batch [25021], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:53:08 TP0] Decode batch [25061], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:53:08 TP0] Decode batch [25101], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:53:09 TP0] Decode batch [25141], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:53:10 TP0] Decode batch [25181], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:53:11 TP0] Decode batch [25221], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:53:12 TP0] Decode batch [25261], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-25 15:53:13 TP0] Decode batch [25301], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:53:13 TP0] Decode batch [25341], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:53:14 TP0] Decode batch [25381], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:53:15 TP0] Decode batch [25421], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:53:16 TP0] Decode batch [25461], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:53:17 TP0] Decode batch [25501], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:53:17] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:17] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:17] INFO:     127.0.0.1:46204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:17 TP0] Prefill batch [25523], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:53:17] INFO:     127.0.0.1:46214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:17 TP0] Prefill batch [25524], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:53:18 TP0] Decode batch [25543], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.42, #queue-req: 0, 
[2025-10-25 15:53:19 TP0] Decode batch [25583], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:53:19 TP0] Decode batch [25623], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:53:20 TP0] Decode batch [25663], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:53:21 TP0] Decode batch [25703], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:53:22 TP0] Decode batch [25743], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:53:23 TP0] Decode batch [25783], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:53:23 TP0] Decode batch [25823], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:53:24 TP0] Decode batch [25863], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:53:25 TP0] Decode batch [25903], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:53:26 TP0] Decode batch [25943], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:53:27 TP0] Decode batch [25983], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:53:28 TP0] Decode batch [26023], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:53:28 TP0] Decode batch [26063], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:53:29 TP0] Decode batch [26103], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:53:30 TP0] Decode batch [26143], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 190.47, #queue-req: 0, 
[2025-10-25 15:53:31 TP0] Decode batch [26183], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:53:32 TP0] Decode batch [26223], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:53:33 TP0] Decode batch [26263], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:53:33 TP0] Decode batch [26303], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:53:34] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:34] INFO:     127.0.0.1:39962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:34] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:34 TP0] Prefill batch [26325], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:53:34] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:34 TP0] Prefill batch [26326], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:53:34 TP0] Decode batch [26345], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.32, #queue-req: 0, 
[2025-10-25 15:53:35 TP0] Decode batch [26385], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-25 15:53:36 TP0] Decode batch [26425], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:53:37 TP0] Decode batch [26465], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:53:38 TP0] Decode batch [26505], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-25 15:53:39 TP0] Decode batch [26545], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:53:39 TP0] Decode batch [26585], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:53:40 TP0] Decode batch [26625], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:53:41 TP0] Decode batch [26665], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:53:42 TP0] Decode batch [26705], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:53:43 TP0] Decode batch [26745], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:53:44 TP0] Decode batch [26785], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:53:44 TP0] Decode batch [26825], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:53:45 TP0] Decode batch [26865], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:53:46 TP0] Decode batch [26905], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:53:47 TP0] Decode batch [26945], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:53:48 TP0] Decode batch [26985], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:53:48 TP0] Decode batch [27025], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:53:49 TP0] Decode batch [27065], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-25 15:53:50 TP0] Decode batch [27105], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:53:51] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:51] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:51] INFO:     127.0.0.1:39582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:51 TP0] Prefill batch [27127], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:53:51] INFO:     127.0.0.1:39596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:53:51 TP0] Prefill batch [27128], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:53:51 TP0] Decode batch [27147], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.45, #queue-req: 0, 
[2025-10-25 15:53:52 TP0] Decode batch [27187], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-10-25 15:53:53 TP0] Decode batch [27227], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-25 15:53:54 TP0] Decode batch [27267], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-25 15:53:54 TP0] Decode batch [27307], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-25 15:53:55 TP0] Decode batch [27347], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-25 15:53:56 TP0] Decode batch [27387], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:53:57 TP0] Decode batch [27427], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-25 15:53:58 TP0] Decode batch [27467], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:53:59 TP0] Decode batch [27507], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:53:59 TP0] Decode batch [27547], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:54:00 TP0] Decode batch [27587], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:54:01 TP0] Decode batch [27627], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:54:02 TP0] Decode batch [27667], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:54:03 TP0] Decode batch [27707], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 190.05, #queue-req: 0, 
[2025-10-25 15:54:04 TP0] Decode batch [27747], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:54:04 TP0] Decode batch [27787], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:54:05 TP0] Decode batch [27827], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:54:06 TP0] Decode batch [27867], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:54:07 TP0] Decode batch [27907], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-25 15:54:07] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:07] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:07] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:07 TP0] Prefill batch [27929], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:54:07] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:07 TP0] Prefill batch [27930], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:54:08 TP0] Decode batch [27949], #running-req: 4, #token: 12873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.62, #queue-req: 0, 
[2025-10-25 15:54:09 TP0] Decode batch [27989], #running-req: 4, #token: 13033, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:54:10 TP0] Decode batch [28029], #running-req: 4, #token: 13193, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:54:10 TP0] Decode batch [28069], #running-req: 4, #token: 13353, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:54:11 TP0] Decode batch [28109], #running-req: 4, #token: 13513, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:54:12 TP0] Decode batch [28149], #running-req: 4, #token: 13673, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-25 15:54:13 TP0] Decode batch [28189], #running-req: 4, #token: 13833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:54:14 TP0] Decode batch [28229], #running-req: 4, #token: 13993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:54:14 TP0] Decode batch [28269], #running-req: 4, #token: 14153, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:54:15 TP0] Decode batch [28309], #running-req: 4, #token: 14313, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:54:16 TP0] Decode batch [28349], #running-req: 4, #token: 14473, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:54:17 TP0] Decode batch [28389], #running-req: 4, #token: 14633, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:54:18 TP0] Decode batch [28429], #running-req: 4, #token: 14793, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:54:19 TP0] Decode batch [28469], #running-req: 4, #token: 14953, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:54:19 TP0] Decode batch [28509], #running-req: 4, #token: 15113, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:54:20 TP0] Decode batch [28549], #running-req: 4, #token: 15273, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:54:21 TP0] Decode batch [28589], #running-req: 4, #token: 15433, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:54:22 TP0] Decode batch [28629], #running-req: 4, #token: 15593, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:54:23 TP0] Decode batch [28669], #running-req: 4, #token: 15753, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:54:24 TP0] Decode batch [28709], #running-req: 4, #token: 15913, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:54:24] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:24] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:24] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:24 TP0] Prefill batch [28731], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:54:24] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:24 TP0] Prefill batch [28732], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:54:25 TP0] Decode batch [28751], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.97, #queue-req: 0, 
[2025-10-25 15:54:25 TP0] Decode batch [28791], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-10-25 15:54:26 TP0] Decode batch [28831], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-25 15:54:27 TP0] Decode batch [28871], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:54:28 TP0] Decode batch [28911], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:54:29 TP0] Decode batch [28951], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:54:30 TP0] Decode batch [28991], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:54:30 TP0] Decode batch [29031], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:54:31 TP0] Decode batch [29071], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-25 15:54:32 TP0] Decode batch [29111], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:54:33 TP0] Decode batch [29151], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:54:34 TP0] Decode batch [29191], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:54:34 TP0] Decode batch [29231], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:54:35 TP0] Decode batch [29271], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 191.34, #queue-req: 0, 
[2025-10-25 15:54:36 TP0] Decode batch [29311], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:54:37 TP0] Decode batch [29351], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:54:38 TP0] Decode batch [29391], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:54:39 TP0] Decode batch [29431], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:54:39 TP0] Decode batch [29471], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:54:40 TP0] Decode batch [29511], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:54:41] INFO:     127.0.0.1:34322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:41] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:41 TP0] Prefill batch [29533], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:54:41] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:41] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:41 TP0] Prefill batch [29534], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:54:41 TP0] Decode batch [29553], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.91, #queue-req: 0, 
[2025-10-25 15:54:42 TP0] Decode batch [29593], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:54:43 TP0] Decode batch [29633], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:54:44 TP0] Decode batch [29673], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:54:45 TP0] Decode batch [29713], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:54:45 TP0] Decode batch [29753], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-25 15:54:46 TP0] Decode batch [29793], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:54:47 TP0] Decode batch [29833], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:54:48 TP0] Decode batch [29873], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:54:49 TP0] Decode batch [29913], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:54:50 TP0] Decode batch [29953], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:54:50 TP0] Decode batch [29993], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:54:51 TP0] Decode batch [30033], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-25 15:54:52 TP0] Decode batch [30073], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:54:53 TP0] Decode batch [30113], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:54:54 TP0] Decode batch [30153], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:54:55 TP0] Decode batch [30193], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:54:55 TP0] Decode batch [30233], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-25 15:54:56 TP0] Decode batch [30273], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:54:57 TP0] Decode batch [30313], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:54:57] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:57] INFO:     127.0.0.1:46894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:57] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:57 TP0] Prefill batch [30335], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:54:57] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:54:58 TP0] Prefill batch [30336], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:54:58 TP0] Decode batch [30355], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.33, #queue-req: 0, 
[2025-10-25 15:54:59 TP0] Decode batch [30395], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:55:00 TP0] Decode batch [30435], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-25 15:55:00 TP0] Decode batch [30475], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:55:01 TP0] Decode batch [30515], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:55:02 TP0] Decode batch [30555], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:55:03 TP0] Decode batch [30595], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:55:04 TP0] Decode batch [30635], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:55:05 TP0] Decode batch [30675], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:55:05 TP0] Decode batch [30715], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:55:06 TP0] Decode batch [30755], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:55:07 TP0] Decode batch [30795], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-25 15:55:08 TP0] Decode batch [30835], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:55:09 TP0] Decode batch [30875], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 191.32, #queue-req: 0, 
[2025-10-25 15:55:10 TP0] Decode batch [30915], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-25 15:55:10 TP0] Decode batch [30955], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-25 15:55:11 TP0] Decode batch [30995], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:55:12 TP0] Decode batch [31035], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:55:13 TP0] Decode batch [31075], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:55:14 TP0] Decode batch [31115], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:55:14] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:14] INFO:     127.0.0.1:44322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:14] INFO:     127.0.0.1:44336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:14 TP0] Prefill batch [31137], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:55:14] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:14 TP0] Prefill batch [31138], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:55:15 TP0] Decode batch [31157], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.26, #queue-req: 0, 
[2025-10-25 15:55:16 TP0] Decode batch [31197], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:55:16 TP0] Decode batch [31237], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:55:17 TP0] Decode batch [31277], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:55:18 TP0] Decode batch [31317], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:55:19 TP0] Decode batch [31357], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:55:20 TP0] Decode batch [31397], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:55:21 TP0] Decode batch [31437], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:55:21 TP0] Decode batch [31477], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:55:22 TP0] Decode batch [31517], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:55:23 TP0] Decode batch [31557], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:55:24 TP0] Decode batch [31597], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:55:25 TP0] Decode batch [31637], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:55:25 TP0] Decode batch [31677], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:55:26 TP0] Decode batch [31717], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:55:27 TP0] Decode batch [31757], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:55:28 TP0] Decode batch [31797], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:55:29 TP0] Decode batch [31837], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:55:30 TP0] Decode batch [31877], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-25 15:55:30 TP0] Decode batch [31917], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:55:31] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:31] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:31] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:31 TP0] Prefill batch [31939], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:55:31] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:31 TP0] Prefill batch [31940], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:55:31 TP0] Decode batch [31959], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.76, #queue-req: 0, 
[2025-10-25 15:55:32 TP0] Decode batch [31999], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-25 15:55:33 TP0] Decode batch [32039], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:55:34 TP0] Decode batch [32079], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-25 15:55:35 TP0] Decode batch [32119], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:55:36 TP0] Decode batch [32159], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:55:36 TP0] Decode batch [32199], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:55:37 TP0] Decode batch [32239], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-25 15:55:38 TP0] Decode batch [32279], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:55:39 TP0] Decode batch [32319], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:55:40 TP0] Decode batch [32359], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:55:41 TP0] Decode batch [32399], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:55:41 TP0] Decode batch [32439], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:55:42 TP0] Decode batch [32479], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:55:43 TP0] Decode batch [32519], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:55:44 TP0] Decode batch [32559], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:55:45 TP0] Decode batch [32599], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:55:46 TP0] Decode batch [32639], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:55:46 TP0] Decode batch [32679], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:55:47 TP0] Decode batch [32719], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:55:48] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:48] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:48] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:48 TP0] Prefill batch [32741], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:55:48] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:55:48 TP0] Prefill batch [32742], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:55:48 TP0] Decode batch [32761], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.72, #queue-req: 0, 
[2025-10-25 15:55:49 TP0] Decode batch [32801], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:55:50 TP0] Decode batch [32841], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:55:51 TP0] Decode batch [32881], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:55:51 TP0] Decode batch [32921], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:55:52 TP0] Decode batch [32961], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:55:53 TP0] Decode batch [33001], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:55:54 TP0] Decode batch [33041], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:55:55 TP0] Decode batch [33081], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:55:56 TP0] Decode batch [33121], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:55:56 TP0] Decode batch [33161], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:55:57 TP0] Decode batch [33201], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:55:58 TP0] Decode batch [33241], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:55:59 TP0] Decode batch [33281], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:56:00 TP0] Decode batch [33321], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:56:01 TP0] Decode batch [33361], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:56:01 TP0] Decode batch [33401], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:56:02 TP0] Decode batch [33441], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:56:03 TP0] Decode batch [33481], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:56:04 TP0] Decode batch [33521], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:56:04] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:04] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:04] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:04 TP0] Prefill batch [33543], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:56:04] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:04 TP0] Prefill batch [33544], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:56:05 TP0] Decode batch [33563], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.67, #queue-req: 0, 
[2025-10-25 15:56:06 TP0] Decode batch [33603], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:56:07 TP0] Decode batch [33643], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:56:07 TP0] Decode batch [33683], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:56:08 TP0] Decode batch [33723], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:56:09 TP0] Decode batch [33763], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:56:10 TP0] Decode batch [33803], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-25 15:56:11 TP0] Decode batch [33843], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-25 15:56:11 TP0] Decode batch [33883], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-25 15:56:12 TP0] Decode batch [33923], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-25 15:56:13 TP0] Decode batch [33963], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-25 15:56:14 TP0] Decode batch [34003], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-25 15:56:15 TP0] Decode batch [34043], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-25 15:56:16 TP0] Decode batch [34083], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-25 15:56:16 TP0] Decode batch [34123], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-25 15:56:17 TP0] Decode batch [34163], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-25 15:56:18 TP0] Decode batch [34203], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-25 15:56:19 TP0] Decode batch [34243], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-25 15:56:20 TP0] Decode batch [34283], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.11, #queue-req: 0, 
[2025-10-25 15:56:21 TP0] Decode batch [34323], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-25 15:56:21] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:21] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:21 TP0] Prefill batch [34345], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:56:21] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:21] INFO:     127.0.0.1:38788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:21 TP0] Prefill batch [34346], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:56:22 TP0] Decode batch [34365], #running-req: 4, #token: 12873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.50, #queue-req: 0, 
[2025-10-25 15:56:22 TP0] Decode batch [34405], #running-req: 4, #token: 13033, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-25 15:56:23 TP0] Decode batch [34445], #running-req: 4, #token: 13193, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:56:24 TP0] Decode batch [34485], #running-req: 4, #token: 13353, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-25 15:56:25 TP0] Decode batch [34525], #running-req: 4, #token: 13513, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-25 15:56:26 TP0] Decode batch [34565], #running-req: 4, #token: 13673, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:56:27 TP0] Decode batch [34605], #running-req: 4, #token: 13833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-25 15:56:27 TP0] Decode batch [34645], #running-req: 4, #token: 13993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:56:28 TP0] Decode batch [34685], #running-req: 4, #token: 14153, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:56:29 TP0] Decode batch [34725], #running-req: 4, #token: 14313, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:56:30 TP0] Decode batch [34765], #running-req: 4, #token: 14473, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:56:31 TP0] Decode batch [34805], #running-req: 4, #token: 14633, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:56:32 TP0] Decode batch [34845], #running-req: 4, #token: 14793, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:56:32 TP0] Decode batch [34885], #running-req: 4, #token: 14953, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:56:33 TP0] Decode batch [34925], #running-req: 4, #token: 15113, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:56:34 TP0] Decode batch [34965], #running-req: 4, #token: 15273, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:56:35 TP0] Decode batch [35005], #running-req: 4, #token: 15433, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:56:36 TP0] Decode batch [35045], #running-req: 4, #token: 15593, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:56:37 TP0] Decode batch [35085], #running-req: 4, #token: 15753, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:56:37 TP0] Decode batch [35125], #running-req: 4, #token: 15913, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:56:38] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:38] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:38] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:38 TP0] Prefill batch [35147], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:56:38] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:38 TP0] Prefill batch [35148], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:56:38 TP0] Decode batch [35167], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.64, #queue-req: 0, 
[2025-10-25 15:56:39 TP0] Decode batch [35207], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-25 15:56:40 TP0] Decode batch [35247], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-25 15:56:41 TP0] Decode batch [35287], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:56:42 TP0] Decode batch [35327], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:56:42 TP0] Decode batch [35367], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:56:43 TP0] Decode batch [35407], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:56:44 TP0] Decode batch [35447], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:56:45 TP0] Decode batch [35487], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:56:46 TP0] Decode batch [35527], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:56:47 TP0] Decode batch [35567], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-25 15:56:47 TP0] Decode batch [35607], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:56:48 TP0] Decode batch [35647], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:56:49 TP0] Decode batch [35687], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:56:50 TP0] Decode batch [35727], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:56:51 TP0] Decode batch [35767], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:56:52 TP0] Decode batch [35807], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:56:52 TP0] Decode batch [35847], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:56:53 TP0] Decode batch [35887], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:56:54 TP0] Decode batch [35927], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:56:54] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:54] INFO:     127.0.0.1:33844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:54] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:54 TP0] Prefill batch [35949], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:56:54] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:56:55 TP0] Prefill batch [35950], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:56:55 TP0] Decode batch [35969], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.60, #queue-req: 0, 
[2025-10-25 15:56:56 TP0] Decode batch [36009], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:56:57 TP0] Decode batch [36049], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:56:58 TP0] Decode batch [36089], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:56:58 TP0] Decode batch [36129], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:56:59 TP0] Decode batch [36169], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:57:00 TP0] Decode batch [36209], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:57:01 TP0] Decode batch [36249], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:57:02 TP0] Decode batch [36289], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:57:02 TP0] Decode batch [36329], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:57:03 TP0] Decode batch [36369], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:57:04 TP0] Decode batch [36409], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:57:05 TP0] Decode batch [36449], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-25 15:57:06 TP0] Decode batch [36489], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-25 15:57:07 TP0] Decode batch [36529], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-25 15:57:07 TP0] Decode batch [36569], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:57:08 TP0] Decode batch [36609], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:57:09 TP0] Decode batch [36649], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-25 15:57:10 TP0] Decode batch [36689], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-25 15:57:11 TP0] Decode batch [36729], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-25 15:57:11] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:11] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:11] INFO:     127.0.0.1:44124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:11 TP0] Prefill batch [36751], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:57:11] INFO:     127.0.0.1:44134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:11 TP0] Prefill batch [36752], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:57:12 TP0] Decode batch [36771], #running-req: 4, #token: 12873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.51, #queue-req: 0, 
[2025-10-25 15:57:13 TP0] Decode batch [36811], #running-req: 4, #token: 13033, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-25 15:57:13 TP0] Decode batch [36851], #running-req: 4, #token: 13193, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:57:14 TP0] Decode batch [36891], #running-req: 4, #token: 13353, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:57:15 TP0] Decode batch [36931], #running-req: 4, #token: 13513, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-25 15:57:16 TP0] Decode batch [36971], #running-req: 4, #token: 13673, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-25 15:57:17 TP0] Decode batch [37011], #running-req: 4, #token: 13833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:57:18 TP0] Decode batch [37051], #running-req: 4, #token: 13993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:57:18 TP0] Decode batch [37091], #running-req: 4, #token: 14153, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:57:19 TP0] Decode batch [37131], #running-req: 4, #token: 14313, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:57:20 TP0] Decode batch [37171], #running-req: 4, #token: 14473, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:57:21 TP0] Decode batch [37211], #running-req: 4, #token: 14633, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:57:22 TP0] Decode batch [37251], #running-req: 4, #token: 14793, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:57:22 TP0] Decode batch [37291], #running-req: 4, #token: 14953, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:57:23 TP0] Decode batch [37331], #running-req: 4, #token: 15113, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-25 15:57:24 TP0] Decode batch [37371], #running-req: 4, #token: 15273, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:57:25 TP0] Decode batch [37411], #running-req: 4, #token: 15433, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:57:26 TP0] Decode batch [37451], #running-req: 4, #token: 15593, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:57:27 TP0] Decode batch [37491], #running-req: 4, #token: 15753, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-25 15:57:27 TP0] Decode batch [37531], #running-req: 4, #token: 15913, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:57:28] INFO:     127.0.0.1:35586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:28] INFO:     127.0.0.1:35590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:28] INFO:     127.0.0.1:35602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:28 TP0] Prefill batch [37553], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:57:28] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:28 TP0] Prefill batch [37554], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:57:28 TP0] Decode batch [37573], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.91, #queue-req: 0, 
[2025-10-25 15:57:29 TP0] Decode batch [37613], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-25 15:57:30 TP0] Decode batch [37653], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-25 15:57:31 TP0] Decode batch [37693], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-25 15:57:32 TP0] Decode batch [37733], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:57:33 TP0] Decode batch [37773], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-25 15:57:33 TP0] Decode batch [37813], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-25 15:57:34 TP0] Decode batch [37853], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:57:35 TP0] Decode batch [37893], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-25 15:57:36 TP0] Decode batch [37933], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:57:37 TP0] Decode batch [37973], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:57:38 TP0] Decode batch [38013], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:57:38 TP0] Decode batch [38053], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:57:39 TP0] Decode batch [38093], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:57:40 TP0] Decode batch [38133], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:57:41 TP0] Decode batch [38173], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:57:42 TP0] Decode batch [38213], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:57:43 TP0] Decode batch [38253], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:57:43 TP0] Decode batch [38293], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:57:44 TP0] Decode batch [38333], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:57:45] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:45] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:45] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:45 TP0] Prefill batch [38355], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:57:45] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:57:45 TP0] Prefill batch [38356], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:57:45 TP0] Decode batch [38375], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.34, #queue-req: 0, 
[2025-10-25 15:57:46 TP0] Decode batch [38415], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:57:47 TP0] Decode batch [38455], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:57:48 TP0] Decode batch [38495], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:57:48 TP0] Decode batch [38535], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:57:49 TP0] Decode batch [38575], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-25 15:57:50 TP0] Decode batch [38615], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-25 15:57:51 TP0] Decode batch [38655], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:57:52 TP0] Decode batch [38695], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:57:53 TP0] Decode batch [38735], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 191.78, #queue-req: 0, 
[2025-10-25 15:57:53 TP0] Decode batch [38775], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-25 15:57:54 TP0] Decode batch [38815], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:57:55 TP0] Decode batch [38855], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:57:56 TP0] Decode batch [38895], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-25 15:57:57 TP0] Decode batch [38935], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-25 15:57:58 TP0] Decode batch [38975], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:57:58 TP0] Decode batch [39015], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-25 15:57:59 TP0] Decode batch [39055], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:58:00 TP0] Decode batch [39095], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:58:01 TP0] Decode batch [39135], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-25 15:58:01] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:01] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:01] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:01 TP0] Prefill batch [39157], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:58:01] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:01 TP0] Prefill batch [39158], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:58:02 TP0] Decode batch [39177], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.47, #queue-req: 0, 
[2025-10-25 15:58:03 TP0] Decode batch [39217], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:58:04 TP0] Decode batch [39257], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-25 15:58:04 TP0] Decode batch [39297], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:58:05 TP0] Decode batch [39337], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:58:06 TP0] Decode batch [39377], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-25 15:58:07 TP0] Decode batch [39417], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:58:08 TP0] Decode batch [39457], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-25 15:58:09 TP0] Decode batch [39497], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-25 15:58:09 TP0] Decode batch [39537], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-25 15:58:10 TP0] Decode batch [39577], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-25 15:58:11 TP0] Decode batch [39617], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-25 15:58:12 TP0] Decode batch [39657], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-25 15:58:13 TP0] Decode batch [39697], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-25 15:58:13 TP0] Decode batch [39737], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-25 15:58:14 TP0] Decode batch [39777], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-25 15:58:15 TP0] Decode batch [39817], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-25 15:58:16 TP0] Decode batch [39857], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-25 15:58:17 TP0] Decode batch [39897], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-25 15:58:18 TP0] Decode batch [39937], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-25 15:58:18] INFO:     127.0.0.1:36992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:18] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:18] INFO:     127.0.0.1:37020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:18 TP0] Prefill batch [39959], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:58:18] INFO:     127.0.0.1:37026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:18 TP0] Prefill batch [39960], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:58:19 TP0] Decode batch [39979], #running-req: 4, #token: 12873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.12, #queue-req: 0, 
[2025-10-25 15:58:19 TP0] Decode batch [40019], #running-req: 4, #token: 13033, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-25 15:58:20 TP0] Decode batch [40059], #running-req: 4, #token: 13193, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:58:21 TP0] Decode batch [40099], #running-req: 4, #token: 13353, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:58:22 TP0] Decode batch [40139], #running-req: 4, #token: 13513, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:58:23 TP0] Decode batch [40179], #running-req: 4, #token: 13673, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:58:24 TP0] Decode batch [40219], #running-req: 4, #token: 13833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:58:24 TP0] Decode batch [40259], #running-req: 4, #token: 13993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:58:25 TP0] Decode batch [40299], #running-req: 4, #token: 14153, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:58:26 TP0] Decode batch [40339], #running-req: 4, #token: 14313, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-25 15:58:27 TP0] Decode batch [40379], #running-req: 4, #token: 14473, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:58:28 TP0] Decode batch [40419], #running-req: 4, #token: 14633, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:58:29 TP0] Decode batch [40459], #running-req: 4, #token: 14793, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:58:29 TP0] Decode batch [40499], #running-req: 4, #token: 14953, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-25 15:58:30 TP0] Decode batch [40539], #running-req: 4, #token: 15113, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:58:31 TP0] Decode batch [40579], #running-req: 4, #token: 15273, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-25 15:58:32 TP0] Decode batch [40619], #running-req: 4, #token: 15433, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:58:33 TP0] Decode batch [40659], #running-req: 4, #token: 15593, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-25 15:58:34 TP0] Decode batch [40699], #running-req: 4, #token: 15753, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-25 15:58:34 TP0] Decode batch [40739], #running-req: 4, #token: 15913, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-25 15:58:35] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:35] INFO:     127.0.0.1:40210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:35 TP0] Prefill batch [40761], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:58:35] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:35] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:35 TP0] Prefill batch [40762], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:58:35 TP0] Decode batch [40781], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.58, #queue-req: 0, 
[2025-10-25 15:58:36 TP0] Decode batch [40821], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:58:37 TP0] Decode batch [40861], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:58:38 TP0] Decode batch [40901], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-25 15:58:39 TP0] Decode batch [40941], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-25 15:58:39 TP0] Decode batch [40981], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-25 15:58:40 TP0] Decode batch [41021], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-25 15:58:41 TP0] Decode batch [41061], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:58:42 TP0] Decode batch [41101], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-25 15:58:43 TP0] Decode batch [41141], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-25 15:58:44 TP0] Decode batch [41181], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-25 15:58:44 TP0] Decode batch [41221], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-25 15:58:45 TP0] Decode batch [41261], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-25 15:58:46 TP0] Decode batch [41301], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-25 15:58:47 TP0] Decode batch [41341], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-25 15:58:48 TP0] Decode batch [41381], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-25 15:58:49 TP0] Decode batch [41421], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-25 15:58:49 TP0] Decode batch [41461], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-25 15:58:50 TP0] Decode batch [41501], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-25 15:58:51 TP0] Decode batch [41541], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-25 15:58:51] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:52] INFO:     127.0.0.1:60488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:52] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:52 TP0] Prefill batch [41563], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:58:52] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:58:52 TP0] Prefill batch [41564], #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-25 15:58:52 TP0] Decode batch [41583], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.43, #queue-req: 0, 
[2025-10-25 15:58:53 TP0] Decode batch [41623], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-25 15:58:54 TP0] Decode batch [41663], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-25 15:58:55 TP0] Decode batch [41703], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:58:55 TP0] Decode batch [41743], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-25 15:58:56 TP0] Decode batch [41783], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-25 15:58:57 TP0] Decode batch [41823], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-25 15:58:58 TP0] Decode batch [41863], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-25 15:58:59 TP0] Decode batch [41903], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-25 15:59:00 TP0] Decode batch [41943], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:59:00 TP0] Decode batch [41983], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-25 15:59:01 TP0] Decode batch [42023], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-25 15:59:02 TP0] Decode batch [42063], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-25 15:59:03 TP0] Decode batch [42103], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-25 15:59:04 TP0] Decode batch [42143], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-25 15:59:04 TP0] Decode batch [42183], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-25 15:59:05 TP0] Decode batch [42223], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-25 15:59:06 TP0] Decode batch [42263], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-25 15:59:07 TP0] Decode batch [42303], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-25 15:59:08 TP0] Decode batch [42343], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-25 15:59:08] INFO:     127.0.0.1:44630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:59:08] INFO:     127.0.0.1:44642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:59:08] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:59:08 TP0] Prefill batch [42365], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:59:08] INFO:     127.0.0.1:44658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:59:08 TP0] Prefill batch [42366], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:59:09 TP0] Decode batch [42385], #running-req: 4, #token: 12874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.50, #queue-req: 0, 
[2025-10-25 15:59:10 TP0] Decode batch [42425], #running-req: 4, #token: 13034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-25 15:59:10 TP0] Decode batch [42465], #running-req: 4, #token: 13194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-25 15:59:11 TP0] Decode batch [42505], #running-req: 4, #token: 13354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-25 15:59:12 TP0] Decode batch [42545], #running-req: 4, #token: 13514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-25 15:59:13 TP0] Decode batch [42585], #running-req: 4, #token: 13674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:59:14 TP0] Decode batch [42625], #running-req: 4, #token: 13834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:59:15 TP0] Decode batch [42665], #running-req: 4, #token: 13994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:59:15 TP0] Decode batch [42705], #running-req: 4, #token: 14154, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:59:16 TP0] Decode batch [42745], #running-req: 4, #token: 14314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:59:17 TP0] Decode batch [42785], #running-req: 4, #token: 14474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:59:18 TP0] Decode batch [42825], #running-req: 4, #token: 14634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:59:19 TP0] Decode batch [42865], #running-req: 4, #token: 14794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:59:20 TP0] Decode batch [42905], #running-req: 4, #token: 14954, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:59:20 TP0] Decode batch [42945], #running-req: 4, #token: 15114, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:59:21 TP0] Decode batch [42985], #running-req: 4, #token: 15274, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:59:22 TP0] Decode batch [43025], #running-req: 4, #token: 15434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-25 15:59:23 TP0] Decode batch [43065], #running-req: 4, #token: 15594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:59:24 TP0] Decode batch [43105], #running-req: 4, #token: 15754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:59:24 TP0] Decode batch [43145], #running-req: 4, #token: 15914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-25 15:59:25] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:59:25] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:59:25 TP0] Prefill batch [43167], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 15:59:25] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:59:25] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 15:59:25 TP0] Prefill batch [43168], #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-25 15:59:25 TP0] Decode batch [43187], #running-req: 4, #token: 12872, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.75, #queue-req: 0, 
[2025-10-25 15:59:26 TP0] Decode batch [43227], #running-req: 4, #token: 13032, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-25 15:59:27 TP0] Decode batch [43267], #running-req: 4, #token: 13192, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-25 15:59:28 TP0] Decode batch [43307], #running-req: 4, #token: 13352, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:59:29 TP0] Decode batch [43347], #running-req: 4, #token: 13512, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-25 15:59:30 TP0] Decode batch [43387], #running-req: 4, #token: 13672, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-25 15:59:30 TP0] Decode batch [43427], #running-req: 4, #token: 13832, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-25 15:59:31 TP0] Decode batch [43467], #running-req: 4, #token: 13992, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:59:32 TP0] Decode batch [43507], #running-req: 4, #token: 14152, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-25 15:59:33 TP0] Decode batch [43547], #running-req: 4, #token: 14312, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:59:34 TP0] Decode batch [43587], #running-req: 4, #token: 14472, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:59:35 TP0] Decode batch [43627], #running-req: 4, #token: 14632, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-25 15:59:35 TP0] Decode batch [43667], #running-req: 4, #token: 14792, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-25 15:59:36 TP0] Decode batch [43707], #running-req: 4, #token: 14952, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-25 15:59:37 TP0] Decode batch [43747], #running-req: 4, #token: 15112, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-25 15:59:38 TP0] Decode batch [43787], #running-req: 4, #token: 15272, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:59:39 TP0] Decode batch [43827], #running-req: 4, #token: 15432, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:59:40 TP0] Decode batch [43867], #running-req: 4, #token: 15592, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-25 15:59:40 TP0] Decode batch [43907], #running-req: 4, #token: 15752, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-25 15:59:41 TP0] Decode batch [43947], #running-req: 4, #token: 15912, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-25 15:59:42] INFO:     127.0.0.1:52838 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-25 15:59:58] INFO:     127.0.0.1:42372 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-25 16:00:04] INFO:     127.0.0.1:42382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:00:04 TP0] Prefill batch [43969], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:00:04 TP0] Decode batch [43988], #running-req: 1, #token: 3220, token usage: 0.00, cuda graph: True, gen throughput (token/s): 4.55, #queue-req: 0, 
[2025-10-25 16:00:06] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:00:06 TP0] Prefill batch [44002], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:00:06 TP0] Decode batch [44029], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.07, #queue-req: 0, 
[2025-10-25 16:00:07 TP0] Decode batch [44069], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:08 TP0] Decode batch [44109], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:09 TP0] Decode batch [44149], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:00:10 TP0] Decode batch [44189], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:00:10 TP0] Decode batch [44229], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:00:11 TP0] Decode batch [44269], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:00:12 TP0] Decode batch [44309], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:00:13 TP0] Decode batch [44349], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:00:14 TP0] Decode batch [44389], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:00:15 TP0] Decode batch [44429], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:00:15 TP0] Decode batch [44469], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:00:16 TP0] Decode batch [44509], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:00:17 TP0] Decode batch [44549], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:00:18 TP0] Decode batch [44589], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:00:19 TP0] Decode batch [44629], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:00:19 TP0] Decode batch [44669], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:00:20 TP0] Decode batch [44709], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:00:21 TP0] Decode batch [44749], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:00:22 TP0] Decode batch [44789], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:00:22] INFO:     127.0.0.1:52398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:00:22 TP0] Prefill batch [44803], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:00:23 TP0] Decode batch [44830], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:00:24 TP0] Decode batch [44870], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:00:24 TP0] Decode batch [44910], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:00:25 TP0] Decode batch [44950], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:00:26 TP0] Decode batch [44990], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:00:27 TP0] Decode batch [45030], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:00:28 TP0] Decode batch [45070], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:00:29 TP0] Decode batch [45110], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:00:29 TP0] Decode batch [45150], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:00:30 TP0] Decode batch [45190], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:00:31 TP0] Decode batch [45230], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:00:32 TP0] Decode batch [45270], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:00:33 TP0] Decode batch [45310], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:33 TP0] Decode batch [45350], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:00:34 TP0] Decode batch [45390], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:35 TP0] Decode batch [45430], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:00:36 TP0] Decode batch [45470], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:37 TP0] Decode batch [45510], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:38 TP0] Decode batch [45550], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:00:38 TP0] Decode batch [45590], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:00:39] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:00:39 TP0] Prefill batch [45604], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:00:39 TP0] Decode batch [45631], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:00:40 TP0] Decode batch [45671], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:00:41 TP0] Decode batch [45711], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:00:42 TP0] Decode batch [45751], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:00:43 TP0] Decode batch [45791], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:43 TP0] Decode batch [45831], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:00:44 TP0] Decode batch [45871], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:45 TP0] Decode batch [45911], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:46 TP0] Decode batch [45951], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:47 TP0] Decode batch [45991], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:47 TP0] Decode batch [46031], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:48 TP0] Decode batch [46071], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:49 TP0] Decode batch [46111], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:50 TP0] Decode batch [46151], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:51 TP0] Decode batch [46191], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:52 TP0] Decode batch [46231], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:52 TP0] Decode batch [46271], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:53 TP0] Decode batch [46311], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:54 TP0] Decode batch [46351], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:55 TP0] Decode batch [46391], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:00:55] INFO:     127.0.0.1:41570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:00:55 TP0] Prefill batch [46405], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:00:56 TP0] Decode batch [46432], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:00:57 TP0] Decode batch [46472], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:57 TP0] Decode batch [46512], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:00:58 TP0] Decode batch [46552], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:00:59 TP0] Decode batch [46592], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:01:00 TP0] Decode batch [46632], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:01 TP0] Decode batch [46672], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:01 TP0] Decode batch [46712], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:01:02 TP0] Decode batch [46752], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:03 TP0] Decode batch [46792], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:04 TP0] Decode batch [46832], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:01:05 TP0] Decode batch [46872], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:06 TP0] Decode batch [46912], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:01:06 TP0] Decode batch [46952], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:01:07 TP0] Decode batch [46992], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:01:08 TP0] Decode batch [47032], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:01:09 TP0] Decode batch [47072], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:01:10 TP0] Decode batch [47112], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:01:11 TP0] Decode batch [47152], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.21, #queue-req: 0, 
[2025-10-25 16:01:11 TP0] Decode batch [47192], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:01:12] INFO:     127.0.0.1:41802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:01:12 TP0] Prefill batch [47206], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:01:12 TP0] Decode batch [47233], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-25 16:01:13 TP0] Decode batch [47273], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:01:14 TP0] Decode batch [47313], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:01:15 TP0] Decode batch [47353], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:01:16 TP0] Decode batch [47393], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:01:16 TP0] Decode batch [47433], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:01:17 TP0] Decode batch [47473], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:01:18 TP0] Decode batch [47513], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:19 TP0] Decode batch [47553], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:01:20 TP0] Decode batch [47593], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:01:20 TP0] Decode batch [47633], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:01:21 TP0] Decode batch [47673], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.21, #queue-req: 0, 
[2025-10-25 16:01:22 TP0] Decode batch [47713], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:23 TP0] Decode batch [47753], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:24 TP0] Decode batch [47793], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:01:25 TP0] Decode batch [47833], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:25 TP0] Decode batch [47873], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:26 TP0] Decode batch [47913], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:01:27 TP0] Decode batch [47953], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:28 TP0] Decode batch [47993], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:28] INFO:     127.0.0.1:34670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:01:28 TP0] Prefill batch [48007], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:01:29 TP0] Decode batch [48034], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:01:30 TP0] Decode batch [48074], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:01:30 TP0] Decode batch [48114], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:01:31 TP0] Decode batch [48154], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 47.83, #queue-req: 0, 
[2025-10-25 16:01:32 TP0] Decode batch [48194], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:01:33 TP0] Decode batch [48234], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:34 TP0] Decode batch [48274], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:34 TP0] Decode batch [48314], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:35 TP0] Decode batch [48354], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:36 TP0] Decode batch [48394], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:01:37 TP0] Decode batch [48434], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:01:38 TP0] Decode batch [48474], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:01:39 TP0] Decode batch [48514], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:01:39 TP0] Decode batch [48554], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:40 TP0] Decode batch [48594], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:41 TP0] Decode batch [48634], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.28, #queue-req: 0, 
[2025-10-25 16:01:42 TP0] Decode batch [48674], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:01:43 TP0] Decode batch [48714], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:01:43 TP0] Decode batch [48754], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:44 TP0] Decode batch [48794], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:01:45] INFO:     127.0.0.1:38094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:01:45 TP0] Prefill batch [48808], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:01:45 TP0] Decode batch [48835], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:01:46 TP0] Decode batch [48875], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:01:47 TP0] Decode batch [48915], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:01:48 TP0] Decode batch [48955], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:01:48 TP0] Decode batch [48995], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:01:49 TP0] Decode batch [49035], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:01:50 TP0] Decode batch [49075], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:01:51 TP0] Decode batch [49115], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:01:52 TP0] Decode batch [49155], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:01:53 TP0] Decode batch [49195], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:01:53 TP0] Decode batch [49235], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:01:54 TP0] Decode batch [49275], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:01:55 TP0] Decode batch [49315], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:01:56 TP0] Decode batch [49355], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:57 TP0] Decode batch [49395], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:01:57 TP0] Decode batch [49435], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:01:58 TP0] Decode batch [49475], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:01:59 TP0] Decode batch [49515], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:00 TP0] Decode batch [49555], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:01 TP0] Decode batch [49595], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:01] INFO:     127.0.0.1:39248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:02:01 TP0] Prefill batch [49609], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:02:02 TP0] Decode batch [49636], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:02:02 TP0] Decode batch [49676], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:02:03 TP0] Decode batch [49716], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:02:04 TP0] Decode batch [49756], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:02:05 TP0] Decode batch [49796], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:02:06 TP0] Decode batch [49836], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:02:07 TP0] Decode batch [49876], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:02:07 TP0] Decode batch [49916], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:08 TP0] Decode batch [49956], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:02:09 TP0] Decode batch [49996], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:02:10 TP0] Decode batch [50036], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:11 TP0] Decode batch [50076], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:11 TP0] Decode batch [50116], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:12 TP0] Decode batch [50156], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:13 TP0] Decode batch [50196], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:14 TP0] Decode batch [50236], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:15 TP0] Decode batch [50276], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:16 TP0] Decode batch [50316], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:16 TP0] Decode batch [50356], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:17 TP0] Decode batch [50396], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:17] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:02:17 TP0] Prefill batch [50410], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:02:18 TP0] Decode batch [50437], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:02:19 TP0] Decode batch [50477], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:02:20 TP0] Decode batch [50517], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:21 TP0] Decode batch [50557], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:21 TP0] Decode batch [50597], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:22 TP0] Decode batch [50637], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:23 TP0] Decode batch [50677], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:24 TP0] Decode batch [50717], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:02:25 TP0] Decode batch [50757], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:02:25 TP0] Decode batch [50797], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:02:26 TP0] Decode batch [50837], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:02:27 TP0] Decode batch [50877], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:02:28 TP0] Decode batch [50917], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:02:29 TP0] Decode batch [50957], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:02:30 TP0] Decode batch [50997], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:02:30 TP0] Decode batch [51037], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:02:31 TP0] Decode batch [51077], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:02:32 TP0] Decode batch [51117], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:02:33 TP0] Decode batch [51157], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:02:34 TP0] Decode batch [51197], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:02:34] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:02:34 TP0] Prefill batch [51211], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:02:35 TP0] Decode batch [51238], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-25 16:02:35 TP0] Decode batch [51278], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:02:36 TP0] Decode batch [51318], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:02:37 TP0] Decode batch [51358], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:38 TP0] Decode batch [51398], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:39 TP0] Decode batch [51438], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:39 TP0] Decode batch [51478], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:40 TP0] Decode batch [51518], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:41 TP0] Decode batch [51558], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:42 TP0] Decode batch [51598], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:43 TP0] Decode batch [51638], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:44 TP0] Decode batch [51678], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:44 TP0] Decode batch [51718], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:45 TP0] Decode batch [51758], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:46 TP0] Decode batch [51798], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:47 TP0] Decode batch [51838], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:48 TP0] Decode batch [51878], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:48 TP0] Decode batch [51918], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:02:49 TP0] Decode batch [51958], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:50 TP0] Decode batch [51998], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:02:50] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:02:50 TP0] Prefill batch [52012], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:02:51 TP0] Decode batch [52039], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:02:52 TP0] Decode batch [52079], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:02:53 TP0] Decode batch [52119], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:02:53 TP0] Decode batch [52159], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:02:54 TP0] Decode batch [52199], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:55 TP0] Decode batch [52239], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:02:56 TP0] Decode batch [52279], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:02:57 TP0] Decode batch [52319], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:02:58 TP0] Decode batch [52359], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:02:58 TP0] Decode batch [52399], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:02:59 TP0] Decode batch [52439], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:00 TP0] Decode batch [52479], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:01 TP0] Decode batch [52519], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:02 TP0] Decode batch [52559], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:03:03 TP0] Decode batch [52599], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:03:03 TP0] Decode batch [52639], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:04 TP0] Decode batch [52679], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:03:05 TP0] Decode batch [52719], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:03:06 TP0] Decode batch [52759], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:03:07 TP0] Decode batch [52799], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:03:07] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:03:07 TP0] Prefill batch [52813], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:03:08 TP0] Decode batch [52840], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-25 16:03:08 TP0] Decode batch [52880], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-25 16:03:09 TP0] Decode batch [52920], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-25 16:03:10 TP0] Decode batch [52960], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-25 16:03:11 TP0] Decode batch [53000], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:03:12 TP0] Decode batch [53040], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-25 16:03:12 TP0] Decode batch [53080], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:03:13 TP0] Decode batch [53120], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:03:14 TP0] Decode batch [53160], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:03:15 TP0] Decode batch [53200], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:03:16 TP0] Decode batch [53240], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:03:16 TP0] Decode batch [53280], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:03:17 TP0] Decode batch [53320], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:03:18 TP0] Decode batch [53360], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:03:19 TP0] Decode batch [53400], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:03:20 TP0] Decode batch [53440], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:03:21 TP0] Decode batch [53480], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:03:21 TP0] Decode batch [53520], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:03:22 TP0] Decode batch [53560], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:03:23 TP0] Decode batch [53600], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:03:23] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:03:23 TP0] Prefill batch [53614], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:03:24 TP0] Decode batch [53641], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-25 16:03:25 TP0] Decode batch [53681], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:03:26 TP0] Decode batch [53721], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:03:26 TP0] Decode batch [53761], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:03:27 TP0] Decode batch [53801], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:03:28 TP0] Decode batch [53841], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:03:29 TP0] Decode batch [53881], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:03:30 TP0] Decode batch [53921], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:03:30 TP0] Decode batch [53961], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:03:31 TP0] Decode batch [54001], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:03:32 TP0] Decode batch [54041], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:03:33 TP0] Decode batch [54081], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:03:34 TP0] Decode batch [54121], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:03:35 TP0] Decode batch [54161], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:03:35 TP0] Decode batch [54201], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:03:36 TP0] Decode batch [54241], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:03:37 TP0] Decode batch [54281], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:03:38 TP0] Decode batch [54321], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:03:39 TP0] Decode batch [54361], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:03:39 TP0] Decode batch [54401], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:03:40] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:03:40 TP0] Prefill batch [54415], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:03:40 TP0] Decode batch [54442], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:03:41 TP0] Decode batch [54482], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:03:42 TP0] Decode batch [54522], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:03:43 TP0] Decode batch [54562], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:03:44 TP0] Decode batch [54602], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:03:44 TP0] Decode batch [54642], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:03:45 TP0] Decode batch [54682], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:03:46 TP0] Decode batch [54722], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:03:47 TP0] Decode batch [54762], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:03:48 TP0] Decode batch [54802], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:03:49 TP0] Decode batch [54842], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:49 TP0] Decode batch [54882], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:03:50 TP0] Decode batch [54922], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:03:51 TP0] Decode batch [54962], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:52 TP0] Decode batch [55002], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:53 TP0] Decode batch [55042], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:03:54 TP0] Decode batch [55082], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:54 TP0] Decode batch [55122], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:55 TP0] Decode batch [55162], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:03:56 TP0] Decode batch [55202], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:03:56] INFO:     127.0.0.1:49476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:03:56 TP0] Prefill batch [55216], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:03:57 TP0] Decode batch [55243], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:03:58 TP0] Decode batch [55283], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:03:59 TP0] Decode batch [55323], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:03:59 TP0] Decode batch [55363], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:04:00 TP0] Decode batch [55403], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:01 TP0] Decode batch [55443], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:04:02 TP0] Decode batch [55483], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:04:03 TP0] Decode batch [55523], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:03 TP0] Decode batch [55563], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:04 TP0] Decode batch [55603], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:04:05 TP0] Decode batch [55643], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:06 TP0] Decode batch [55683], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:07 TP0] Decode batch [55723], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:04:08 TP0] Decode batch [55763], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:08 TP0] Decode batch [55803], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:09 TP0] Decode batch [55843], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:10 TP0] Decode batch [55883], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:11 TP0] Decode batch [55923], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:04:12 TP0] Decode batch [55963], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:04:12 TP0] Decode batch [56003], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:13] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:04:13 TP0] Prefill batch [56017], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:04:13 TP0] Decode batch [56044], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:04:14 TP0] Decode batch [56084], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:04:15 TP0] Decode batch [56124], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:04:16 TP0] Decode batch [56164], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:04:17 TP0] Decode batch [56204], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:04:17 TP0] Decode batch [56244], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:04:18 TP0] Decode batch [56284], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:04:19 TP0] Decode batch [56324], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:04:20 TP0] Decode batch [56364], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:04:21 TP0] Decode batch [56404], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:04:22 TP0] Decode batch [56444], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:04:22 TP0] Decode batch [56484], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:04:23 TP0] Decode batch [56524], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:04:24 TP0] Decode batch [56564], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:04:25 TP0] Decode batch [56604], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:04:26 TP0] Decode batch [56644], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.29, #queue-req: 0, 
[2025-10-25 16:04:26 TP0] Decode batch [56684], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:04:27 TP0] Decode batch [56724], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:04:28 TP0] Decode batch [56764], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:04:29 TP0] Decode batch [56804], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:04:29] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:04:29 TP0] Prefill batch [56818], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:04:30 TP0] Decode batch [56845], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-25 16:04:31 TP0] Decode batch [56885], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:04:31 TP0] Decode batch [56925], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:04:32 TP0] Decode batch [56965], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:04:33 TP0] Decode batch [57005], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:04:34 TP0] Decode batch [57045], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:04:35 TP0] Decode batch [57085], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:04:36 TP0] Decode batch [57125], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:04:36 TP0] Decode batch [57165], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:04:37 TP0] Decode batch [57205], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:04:38 TP0] Decode batch [57245], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:04:39 TP0] Decode batch [57285], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:04:40 TP0] Decode batch [57325], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:04:40 TP0] Decode batch [57365], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:04:41 TP0] Decode batch [57405], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:04:42 TP0] Decode batch [57445], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:04:43 TP0] Decode batch [57485], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:04:44 TP0] Decode batch [57525], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:04:45 TP0] Decode batch [57565], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:04:45 TP0] Decode batch [57605], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:04:46] INFO:     127.0.0.1:40632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:04:46 TP0] Prefill batch [57619], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:04:46 TP0] Decode batch [57646], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-25 16:04:47 TP0] Decode batch [57686], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:04:48 TP0] Decode batch [57726], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:04:49 TP0] Decode batch [57766], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:04:50 TP0] Decode batch [57806], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:04:50 TP0] Decode batch [57846], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:04:51 TP0] Decode batch [57886], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:04:52 TP0] Decode batch [57926], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:04:53 TP0] Decode batch [57966], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:04:54 TP0] Decode batch [58006], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:04:54 TP0] Decode batch [58046], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:04:55 TP0] Decode batch [58086], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:56 TP0] Decode batch [58126], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:04:57 TP0] Decode batch [58166], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:04:58 TP0] Decode batch [58206], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:04:59 TP0] Decode batch [58246], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-25 16:04:59 TP0] Decode batch [58286], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:05:00 TP0] Decode batch [58326], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:05:01 TP0] Decode batch [58366], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:05:02 TP0] Decode batch [58406], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:05:02] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:05:02 TP0] Prefill batch [58420], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:05:03 TP0] Decode batch [58447], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:05:04 TP0] Decode batch [58487], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:05:04 TP0] Decode batch [58527], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:05:05 TP0] Decode batch [58567], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:05:06 TP0] Decode batch [58607], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:05:07 TP0] Decode batch [58647], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:05:08 TP0] Decode batch [58687], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:08 TP0] Decode batch [58727], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:09 TP0] Decode batch [58767], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:10 TP0] Decode batch [58807], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:05:11 TP0] Decode batch [58847], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:12 TP0] Decode batch [58887], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:05:13 TP0] Decode batch [58927], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:05:13 TP0] Decode batch [58967], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:05:14 TP0] Decode batch [59007], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:05:15 TP0] Decode batch [59047], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:05:16 TP0] Decode batch [59087], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:05:17 TP0] Decode batch [59127], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:05:17 TP0] Decode batch [59167], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:05:18 TP0] Decode batch [59207], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:05:19] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:05:19 TP0] Prefill batch [59221], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:05:19 TP0] Decode batch [59248], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-25 16:05:20 TP0] Decode batch [59288], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:05:21 TP0] Decode batch [59328], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:05:22 TP0] Decode batch [59368], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:05:22 TP0] Decode batch [59408], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:05:23 TP0] Decode batch [59448], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:05:24 TP0] Decode batch [59488], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:05:25 TP0] Decode batch [59528], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:05:26 TP0] Decode batch [59568], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:05:27 TP0] Decode batch [59608], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:05:27 TP0] Decode batch [59648], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:05:28 TP0] Decode batch [59688], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:05:29 TP0] Decode batch [59728], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:05:30 TP0] Decode batch [59768], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:05:31 TP0] Decode batch [59808], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:05:31 TP0] Decode batch [59848], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:05:32 TP0] Decode batch [59888], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:05:33 TP0] Decode batch [59928], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:05:34 TP0] Decode batch [59968], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:05:35 TP0] Decode batch [60008], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:05:35] INFO:     127.0.0.1:41864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:05:35 TP0] Prefill batch [60022], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:05:36 TP0] Decode batch [60049], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:05:36 TP0] Decode batch [60089], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:05:37 TP0] Decode batch [60129], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:05:38 TP0] Decode batch [60169], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:05:39 TP0] Decode batch [60209], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:05:40 TP0] Decode batch [60249], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:05:41 TP0] Decode batch [60289], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:05:41 TP0] Decode batch [60329], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:05:42 TP0] Decode batch [60369], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:05:43 TP0] Decode batch [60409], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:05:44 TP0] Decode batch [60449], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:05:45 TP0] Decode batch [60489], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:05:45 TP0] Decode batch [60529], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:05:46 TP0] Decode batch [60569], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:05:47 TP0] Decode batch [60609], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:48 TP0] Decode batch [60649], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:49 TP0] Decode batch [60689], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:50 TP0] Decode batch [60729], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:50 TP0] Decode batch [60769], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:05:51 TP0] Decode batch [60809], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:05:51] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:05:51 TP0] Prefill batch [60823], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:05:52 TP0] Decode batch [60850], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:05:53 TP0] Decode batch [60890], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:05:54 TP0] Decode batch [60930], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:05:55 TP0] Decode batch [60970], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:55 TP0] Decode batch [61010], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:56 TP0] Decode batch [61050], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:57 TP0] Decode batch [61090], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:05:58 TP0] Decode batch [61130], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:05:59 TP0] Decode batch [61170], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:05:59 TP0] Decode batch [61210], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:06:00 TP0] Decode batch [61250], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:01 TP0] Decode batch [61290], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:02 TP0] Decode batch [61330], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:06:03 TP0] Decode batch [61370], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:04 TP0] Decode batch [61410], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:04 TP0] Decode batch [61450], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:05 TP0] Decode batch [61490], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:06:06 TP0] Decode batch [61530], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:06:07 TP0] Decode batch [61570], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:06:08 TP0] Decode batch [61610], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:06:08] INFO:     127.0.0.1:43798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:06:08 TP0] Prefill batch [61624], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:06:09 TP0] Decode batch [61651], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-25 16:06:09 TP0] Decode batch [61691], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:06:10 TP0] Decode batch [61731], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:06:11 TP0] Decode batch [61771], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:06:12 TP0] Decode batch [61811], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:06:13 TP0] Decode batch [61851], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:06:13 TP0] Decode batch [61891], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:06:14 TP0] Decode batch [61931], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:06:15 TP0] Decode batch [61971], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:06:16 TP0] Decode batch [62011], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:06:17 TP0] Decode batch [62051], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:06:18 TP0] Decode batch [62091], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:06:18 TP0] Decode batch [62131], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:06:19 TP0] Decode batch [62171], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:06:20 TP0] Decode batch [62211], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:21 TP0] Decode batch [62251], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:22 TP0] Decode batch [62291], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:22 TP0] Decode batch [62331], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:06:23 TP0] Decode batch [62371], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:24 TP0] Decode batch [62411], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:06:24] INFO:     127.0.0.1:42712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:06:24 TP0] Prefill batch [62425], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:06:25 TP0] Decode batch [62452], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-25 16:06:26 TP0] Decode batch [62492], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:06:27 TP0] Decode batch [62532], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:06:27 TP0] Decode batch [62572], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:06:28 TP0] Decode batch [62612], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:29 TP0] Decode batch [62652], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:30 TP0] Decode batch [62692], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:31 TP0] Decode batch [62732], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:32 TP0] Decode batch [62772], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:32 TP0] Decode batch [62812], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:06:33 TP0] Decode batch [62852], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:06:34 TP0] Decode batch [62892], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:06:35 TP0] Decode batch [62932], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:06:36 TP0] Decode batch [62972], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:06:36 TP0] Decode batch [63012], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:06:37 TP0] Decode batch [63052], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:06:38 TP0] Decode batch [63092], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:06:39 TP0] Decode batch [63132], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:06:40 TP0] Decode batch [63172], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:06:41 TP0] Decode batch [63212], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:06:41] INFO:     127.0.0.1:49782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:06:41 TP0] Prefill batch [63226], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:06:41 TP0] Decode batch [63253], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:06:42 TP0] Decode batch [63293], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:06:43 TP0] Decode batch [63333], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:06:44 TP0] Decode batch [63373], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:06:45 TP0] Decode batch [63413], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:06:46 TP0] Decode batch [63453], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:06:46 TP0] Decode batch [63493], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:06:47 TP0] Decode batch [63533], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:06:48 TP0] Decode batch [63573], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:49 TP0] Decode batch [63613], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:06:50 TP0] Decode batch [63653], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:06:50 TP0] Decode batch [63693], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:51 TP0] Decode batch [63733], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:06:52 TP0] Decode batch [63773], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:53 TP0] Decode batch [63813], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:06:54 TP0] Decode batch [63853], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:55 TP0] Decode batch [63893], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:55 TP0] Decode batch [63933], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:56 TP0] Decode batch [63973], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:06:57 TP0] Decode batch [64013], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:06:57] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:06:57 TP0] Prefill batch [64027], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:06:58 TP0] Decode batch [64054], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-25 16:06:59 TP0] Decode batch [64094], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:07:00 TP0] Decode batch [64134], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:07:00 TP0] Decode batch [64174], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:01 TP0] Decode batch [64214], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:02 TP0] Decode batch [64254], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:03 TP0] Decode batch [64294], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:04 TP0] Decode batch [64334], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:07:04 TP0] Decode batch [64374], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:05 TP0] Decode batch [64414], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:06 TP0] Decode batch [64454], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:07:07 TP0] Decode batch [64494], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:08 TP0] Decode batch [64534], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:07:09 TP0] Decode batch [64574], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:07:09 TP0] Decode batch [64614], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:07:10 TP0] Decode batch [64654], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:07:11 TP0] Decode batch [64694], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:07:12 TP0] Decode batch [64734], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:07:13 TP0] Decode batch [64774], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:07:13 TP0] Decode batch [64814], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:07:14] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:07:14 TP0] Prefill batch [64828], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:07:14 TP0] Decode batch [64855], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:07:15 TP0] Decode batch [64895], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:07:16 TP0] Decode batch [64935], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:07:17 TP0] Decode batch [64975], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:07:18 TP0] Decode batch [65015], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:07:18 TP0] Decode batch [65055], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:07:19 TP0] Decode batch [65095], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:07:20 TP0] Decode batch [65135], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:07:21 TP0] Decode batch [65175], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:07:22 TP0] Decode batch [65215], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:07:23 TP0] Decode batch [65255], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:07:23 TP0] Decode batch [65295], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:24 TP0] Decode batch [65335], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:07:25 TP0] Decode batch [65375], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:26 TP0] Decode batch [65415], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:27 TP0] Decode batch [65455], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:07:27 TP0] Decode batch [65495], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:28 TP0] Decode batch [65535], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:29 TP0] Decode batch [65575], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:30 TP0] Decode batch [65615], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:30] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:07:30 TP0] Prefill batch [65629], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:07:31 TP0] Decode batch [65656], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-25 16:07:32 TP0] Decode batch [65696], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:07:32 TP0] Decode batch [65736], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:07:33 TP0] Decode batch [65776], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:07:34 TP0] Decode batch [65816], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:07:35 TP0] Decode batch [65856], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:07:36 TP0] Decode batch [65896], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:07:37 TP0] Decode batch [65936], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:07:37 TP0] Decode batch [65976], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:07:38 TP0] Decode batch [66016], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:07:39 TP0] Decode batch [66056], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:07:40 TP0] Decode batch [66096], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:07:41 TP0] Decode batch [66136], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:07:41 TP0] Decode batch [66176], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:07:42 TP0] Decode batch [66216], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:43 TP0] Decode batch [66256], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:07:44 TP0] Decode batch [66296], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:07:45 TP0] Decode batch [66336], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:07:46 TP0] Decode batch [66376], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:46 TP0] Decode batch [66416], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:47] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:07:47 TP0] Prefill batch [66430], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:07:47 TP0] Decode batch [66457], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:07:48 TP0] Decode batch [66497], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:07:49 TP0] Decode batch [66537], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:07:50 TP0] Decode batch [66577], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:07:51 TP0] Decode batch [66617], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:51 TP0] Decode batch [66657], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:52 TP0] Decode batch [66697], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:07:53 TP0] Decode batch [66737], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:07:54 TP0] Decode batch [66777], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:07:55 TP0] Decode batch [66817], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:07:55 TP0] Decode batch [66857], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:07:56 TP0] Decode batch [66897], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:07:57 TP0] Decode batch [66937], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:07:58 TP0] Decode batch [66977], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:07:59 TP0] Decode batch [67017], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:00 TP0] Decode batch [67057], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:08:00 TP0] Decode batch [67097], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:08:01 TP0] Decode batch [67137], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:02 TP0] Decode batch [67177], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:08:03 TP0] Decode batch [67217], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:03] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:08:03 TP0] Prefill batch [67231], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:08:04 TP0] Decode batch [67258], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-25 16:08:05 TP0] Decode batch [67298], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:08:05 TP0] Decode batch [67338], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:08:06 TP0] Decode batch [67378], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:08:07 TP0] Decode batch [67418], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:08:08 TP0] Decode batch [67458], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:08:09 TP0] Decode batch [67498], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:08:09 TP0] Decode batch [67538], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:08:10 TP0] Decode batch [67578], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:08:11 TP0] Decode batch [67618], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:08:12 TP0] Decode batch [67658], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:08:13 TP0] Decode batch [67698], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:08:14 TP0] Decode batch [67738], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:08:14 TP0] Decode batch [67778], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:08:15 TP0] Decode batch [67818], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:08:16 TP0] Decode batch [67858], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:08:17 TP0] Decode batch [67898], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:08:18 TP0] Decode batch [67938], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:08:18 TP0] Decode batch [67978], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:08:19 TP0] Decode batch [68018], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:08:20] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:08:20 TP0] Prefill batch [68032], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:08:20 TP0] Decode batch [68059], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:08:21 TP0] Decode batch [68099], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:08:22 TP0] Decode batch [68139], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:08:23 TP0] Decode batch [68179], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:08:23 TP0] Decode batch [68219], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:08:24 TP0] Decode batch [68259], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:08:25 TP0] Decode batch [68299], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:08:26 TP0] Decode batch [68339], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:08:27 TP0] Decode batch [68379], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:08:28 TP0] Decode batch [68419], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:08:28 TP0] Decode batch [68459], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:08:29 TP0] Decode batch [68499], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:30 TP0] Decode batch [68539], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:08:31 TP0] Decode batch [68579], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:32 TP0] Decode batch [68619], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:08:33 TP0] Decode batch [68659], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:08:33 TP0] Decode batch [68699], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:34 TP0] Decode batch [68739], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:08:35 TP0] Decode batch [68779], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:36 TP0] Decode batch [68819], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:08:36] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:08:36 TP0] Prefill batch [68833], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:08:37 TP0] Decode batch [68860], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:08:38 TP0] Decode batch [68900], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:08:38 TP0] Decode batch [68940], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:08:39 TP0] Decode batch [68980], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:08:40 TP0] Decode batch [69020], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:41 TP0] Decode batch [69060], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:08:42 TP0] Decode batch [69100], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:42 TP0] Decode batch [69140], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:43 TP0] Decode batch [69180], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:08:44 TP0] Decode batch [69220], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:08:45 TP0] Decode batch [69260], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:08:46 TP0] Decode batch [69300], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:08:47 TP0] Decode batch [69340], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:08:47 TP0] Decode batch [69380], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:08:48 TP0] Decode batch [69420], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:08:49 TP0] Decode batch [69460], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:08:50 TP0] Decode batch [69500], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:08:51 TP0] Decode batch [69540], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:08:51 TP0] Decode batch [69580], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:08:52 TP0] Decode batch [69620], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:08:53] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:08:53 TP0] Prefill batch [69634], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:08:53 TP0] Decode batch [69661], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:08:54 TP0] Decode batch [69701], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:08:55 TP0] Decode batch [69741], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:08:56 TP0] Decode batch [69781], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:08:56 TP0] Decode batch [69821], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:08:57 TP0] Decode batch [69861], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:08:58 TP0] Decode batch [69901], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:08:59 TP0] Decode batch [69941], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:09:00 TP0] Decode batch [69981], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:09:01 TP0] Decode batch [70021], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:01 TP0] Decode batch [70061], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:09:02 TP0] Decode batch [70101], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:03 TP0] Decode batch [70141], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:09:04 TP0] Decode batch [70181], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:05 TP0] Decode batch [70221], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:05 TP0] Decode batch [70261], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:06 TP0] Decode batch [70301], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:07 TP0] Decode batch [70341], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:09:08 TP0] Decode batch [70381], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:09 TP0] Decode batch [70421], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:09:09] INFO:     127.0.0.1:60892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:09:09 TP0] Prefill batch [70435], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:09:10 TP0] Decode batch [70462], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:09:10 TP0] Decode batch [70502], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:09:11 TP0] Decode batch [70542], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:09:12 TP0] Decode batch [70582], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:09:13 TP0] Decode batch [70622], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:09:14 TP0] Decode batch [70662], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:15 TP0] Decode batch [70702], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:09:15 TP0] Decode batch [70742], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:16 TP0] Decode batch [70782], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:09:17 TP0] Decode batch [70822], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:18 TP0] Decode batch [70862], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:09:19 TP0] Decode batch [70902], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:19 TP0] Decode batch [70942], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:20 TP0] Decode batch [70982], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:21 TP0] Decode batch [71022], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:22 TP0] Decode batch [71062], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:09:23 TP0] Decode batch [71102], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:09:24 TP0] Decode batch [71142], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:24 TP0] Decode batch [71182], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:09:25 TP0] Decode batch [71222], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:09:25] INFO:     127.0.0.1:45684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:09:25 TP0] Prefill batch [71236], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:09:26 TP0] Decode batch [71263], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:09:27 TP0] Decode batch [71303], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:09:28 TP0] Decode batch [71343], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:09:29 TP0] Decode batch [71383], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:09:29 TP0] Decode batch [71423], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:09:30 TP0] Decode batch [71463], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:09:31 TP0] Decode batch [71503], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:09:32 TP0] Decode batch [71543], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:09:33 TP0] Decode batch [71583], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:33 TP0] Decode batch [71623], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:09:34 TP0] Decode batch [71663], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:35 TP0] Decode batch [71703], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:09:36 TP0] Decode batch [71743], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:37 TP0] Decode batch [71783], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:38 TP0] Decode batch [71823], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:38 TP0] Decode batch [71863], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:39 TP0] Decode batch [71903], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:40 TP0] Decode batch [71943], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:41 TP0] Decode batch [71983], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:09:42 TP0] Decode batch [72023], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:09:42] INFO:     127.0.0.1:46140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:09:42 TP0] Prefill batch [72037], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:09:43 TP0] Decode batch [72064], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-10-25 16:09:43 TP0] Decode batch [72104], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-25 16:09:44 TP0] Decode batch [72144], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-25 16:09:45 TP0] Decode batch [72184], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-25 16:09:46 TP0] Decode batch [72224], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-25 16:09:47 TP0] Decode batch [72264], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-25 16:09:47 TP0] Decode batch [72304], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-25 16:09:48 TP0] Decode batch [72344], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-25 16:09:49 TP0] Decode batch [72384], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:09:50 TP0] Decode batch [72424], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-25 16:09:51 TP0] Decode batch [72464], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:09:52 TP0] Decode batch [72504], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:09:52 TP0] Decode batch [72544], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:09:53 TP0] Decode batch [72584], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:09:54 TP0] Decode batch [72624], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:09:55 TP0] Decode batch [72664], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:09:56 TP0] Decode batch [72704], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:09:56 TP0] Decode batch [72744], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:09:57 TP0] Decode batch [72784], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:09:58 TP0] Decode batch [72824], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:09:58] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:09:58 TP0] Prefill batch [72838], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:09:59 TP0] Decode batch [72865], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:10:00 TP0] Decode batch [72905], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:01 TP0] Decode batch [72945], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:01 TP0] Decode batch [72985], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:10:02 TP0] Decode batch [73025], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:03 TP0] Decode batch [73065], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:04 TP0] Decode batch [73105], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:05 TP0] Decode batch [73145], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:06 TP0] Decode batch [73185], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:10:06 TP0] Decode batch [73225], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:07 TP0] Decode batch [73265], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:10:08 TP0] Decode batch [73305], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:10:09 TP0] Decode batch [73345], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:10:10 TP0] Decode batch [73385], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:10 TP0] Decode batch [73425], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:10:11 TP0] Decode batch [73465], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:12 TP0] Decode batch [73505], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:10:13 TP0] Decode batch [73545], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:14 TP0] Decode batch [73585], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:15 TP0] Decode batch [73625], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:15] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:10:15 TP0] Prefill batch [73639], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:10:15 TP0] Decode batch [73666], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-25 16:10:16 TP0] Decode batch [73706], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:10:17 TP0] Decode batch [73746], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:10:18 TP0] Decode batch [73786], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:10:19 TP0] Decode batch [73826], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:10:20 TP0] Decode batch [73866], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:10:20 TP0] Decode batch [73906], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:10:21 TP0] Decode batch [73946], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:10:22 TP0] Decode batch [73986], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:10:23 TP0] Decode batch [74026], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:10:24 TP0] Decode batch [74066], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:10:24 TP0] Decode batch [74106], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:10:25 TP0] Decode batch [74146], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:10:26 TP0] Decode batch [74186], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:10:27 TP0] Decode batch [74226], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:10:28 TP0] Decode batch [74266], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:10:29 TP0] Decode batch [74306], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:10:29 TP0] Decode batch [74346], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:10:30 TP0] Decode batch [74386], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:10:31 TP0] Decode batch [74426], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:10:31] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:10:31 TP0] Prefill batch [74440], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:10:32 TP0] Decode batch [74467], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:10:33 TP0] Decode batch [74507], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:10:34 TP0] Decode batch [74547], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:10:34 TP0] Decode batch [74587], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:10:35 TP0] Decode batch [74627], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:36 TP0] Decode batch [74667], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:10:37 TP0] Decode batch [74707], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:38 TP0] Decode batch [74747], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:10:38 TP0] Decode batch [74787], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:39 TP0] Decode batch [74827], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:10:40 TP0] Decode batch [74867], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:41 TP0] Decode batch [74907], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:42 TP0] Decode batch [74947], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:43 TP0] Decode batch [74987], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:43 TP0] Decode batch [75027], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:44 TP0] Decode batch [75067], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:45 TP0] Decode batch [75107], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:10:46 TP0] Decode batch [75147], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:10:47 TP0] Decode batch [75187], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:10:48 TP0] Decode batch [75227], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:10:48] INFO:     127.0.0.1:33436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:10:48 TP0] Prefill batch [75241], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:10:48 TP0] Decode batch [75268], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.90, #queue-req: 0, 
[2025-10-25 16:10:49 TP0] Decode batch [75308], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:10:50 TP0] Decode batch [75348], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:10:51 TP0] Decode batch [75388], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:10:52 TP0] Decode batch [75428], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:10:53 TP0] Decode batch [75468], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:10:53 TP0] Decode batch [75508], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:10:54 TP0] Decode batch [75548], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:10:55 TP0] Decode batch [75588], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:10:56 TP0] Decode batch [75628], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:10:57 TP0] Decode batch [75668], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:10:57 TP0] Decode batch [75708], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:10:58 TP0] Decode batch [75748], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:10:59 TP0] Decode batch [75788], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:11:00 TP0] Decode batch [75828], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:11:01 TP0] Decode batch [75868], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:11:02 TP0] Decode batch [75908], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:11:02 TP0] Decode batch [75948], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:11:03 TP0] Decode batch [75988], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:11:04 TP0] Decode batch [76028], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:11:04] INFO:     127.0.0.1:41708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:11:04 TP0] Prefill batch [76042], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:11:05 TP0] Decode batch [76069], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-25 16:11:06 TP0] Decode batch [76109], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:11:07 TP0] Decode batch [76149], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:11:07 TP0] Decode batch [76189], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:11:08 TP0] Decode batch [76229], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:11:09 TP0] Decode batch [76269], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:11:10 TP0] Decode batch [76309], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:11:11 TP0] Decode batch [76349], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:11:11 TP0] Decode batch [76389], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:11:12 TP0] Decode batch [76429], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:11:13 TP0] Decode batch [76469], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:11:14 TP0] Decode batch [76509], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:11:15 TP0] Decode batch [76549], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:11:16 TP0] Decode batch [76589], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:11:16 TP0] Decode batch [76629], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:11:17 TP0] Decode batch [76669], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:18 TP0] Decode batch [76709], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:11:19 TP0] Decode batch [76749], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:20 TP0] Decode batch [76789], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:11:20 TP0] Decode batch [76829], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:21] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:11:21 TP0] Prefill batch [76843], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:11:21 TP0] Decode batch [76870], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.87, #queue-req: 0, 
[2025-10-25 16:11:22 TP0] Decode batch [76910], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:11:23 TP0] Decode batch [76950], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:11:24 TP0] Decode batch [76990], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:11:25 TP0] Decode batch [77030], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:11:25 TP0] Decode batch [77070], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:26 TP0] Decode batch [77110], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:27 TP0] Decode batch [77150], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:28 TP0] Decode batch [77190], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:11:29 TP0] Decode batch [77230], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:30 TP0] Decode batch [77270], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:30 TP0] Decode batch [77310], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:31 TP0] Decode batch [77350], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:11:32 TP0] Decode batch [77390], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:33 TP0] Decode batch [77430], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:34 TP0] Decode batch [77470], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:34 TP0] Decode batch [77510], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:11:35 TP0] Decode batch [77550], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:11:36 TP0] Decode batch [77590], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:11:37 TP0] Decode batch [77630], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:37] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:11:37 TP0] Prefill batch [77644], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:11:38 TP0] Decode batch [77671], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:11:39 TP0] Decode batch [77711], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:11:39 TP0] Decode batch [77751], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:11:40 TP0] Decode batch [77791], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:11:41 TP0] Decode batch [77831], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:11:42 TP0] Decode batch [77871], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:11:43 TP0] Decode batch [77911], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:11:44 TP0] Decode batch [77951], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:11:44 TP0] Decode batch [77991], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:45 TP0] Decode batch [78031], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:11:46 TP0] Decode batch [78071], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:11:47 TP0] Decode batch [78111], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:48 TP0] Decode batch [78151], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:11:48 TP0] Decode batch [78191], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:49 TP0] Decode batch [78231], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:50 TP0] Decode batch [78271], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:11:51 TP0] Decode batch [78311], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:52 TP0] Decode batch [78351], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:11:53 TP0] Decode batch [78391], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:11:53 TP0] Decode batch [78431], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:11:54] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:11:54 TP0] Prefill batch [78445], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:11:54 TP0] Decode batch [78472], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:11:55 TP0] Decode batch [78512], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:11:56 TP0] Decode batch [78552], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:11:57 TP0] Decode batch [78592], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:11:58 TP0] Decode batch [78632], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:11:58 TP0] Decode batch [78672], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:11:59 TP0] Decode batch [78712], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:12:00 TP0] Decode batch [78752], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:12:01 TP0] Decode batch [78792], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:12:02 TP0] Decode batch [78832], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:12:02 TP0] Decode batch [78872], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:12:03 TP0] Decode batch [78912], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:12:04 TP0] Decode batch [78952], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:12:05 TP0] Decode batch [78992], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:12:06 TP0] Decode batch [79032], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:12:07 TP0] Decode batch [79072], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:12:07 TP0] Decode batch [79112], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:12:08 TP0] Decode batch [79152], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:12:09 TP0] Decode batch [79192], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:12:10 TP0] Decode batch [79232], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:12:10] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:12:10 TP0] Prefill batch [79246], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:12:11 TP0] Decode batch [79273], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:12:12 TP0] Decode batch [79313], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:12:12 TP0] Decode batch [79353], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:12:13 TP0] Decode batch [79393], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:12:14 TP0] Decode batch [79433], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:12:15 TP0] Decode batch [79473], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:16 TP0] Decode batch [79513], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:12:16 TP0] Decode batch [79553], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:12:17 TP0] Decode batch [79593], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:12:18 TP0] Decode batch [79633], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:12:19 TP0] Decode batch [79673], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:20 TP0] Decode batch [79713], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:21 TP0] Decode batch [79753], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:21 TP0] Decode batch [79793], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:22 TP0] Decode batch [79833], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:12:23 TP0] Decode batch [79873], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:24 TP0] Decode batch [79913], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:25 TP0] Decode batch [79953], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:25 TP0] Decode batch [79993], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:12:26 TP0] Decode batch [80033], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:12:27] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:12:27 TP0] Prefill batch [80047], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:12:27 TP0] Decode batch [80074], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.84, #queue-req: 0, 
[2025-10-25 16:12:28 TP0] Decode batch [80114], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:12:29 TP0] Decode batch [80154], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:12:30 TP0] Decode batch [80194], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:12:30 TP0] Decode batch [80234], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:12:31 TP0] Decode batch [80274], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:12:32 TP0] Decode batch [80314], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:12:33 TP0] Decode batch [80354], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:12:34 TP0] Decode batch [80394], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:12:35 TP0] Decode batch [80434], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:12:35 TP0] Decode batch [80474], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:12:36 TP0] Decode batch [80514], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:12:37 TP0] Decode batch [80554], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:12:38 TP0] Decode batch [80594], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:12:39 TP0] Decode batch [80634], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:39 TP0] Decode batch [80674], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:40 TP0] Decode batch [80714], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:41 TP0] Decode batch [80754], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:12:42 TP0] Decode batch [80794], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:12:43 TP0] Decode batch [80834], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:12:43] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:12:43 TP0] Prefill batch [80848], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:12:44 TP0] Decode batch [80875], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.88, #queue-req: 0, 
[2025-10-25 16:12:45 TP0] Decode batch [80915], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:12:45 TP0] Decode batch [80955], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:12:46 TP0] Decode batch [80995], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:12:47 TP0] Decode batch [81035], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:12:48 TP0] Decode batch [81075], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:12:49 TP0] Decode batch [81115], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:12:49 TP0] Decode batch [81155], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:12:50 TP0] Decode batch [81195], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:12:51 TP0] Decode batch [81235], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:12:52 TP0] Decode batch [81275], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:12:53 TP0] Decode batch [81315], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:12:54 TP0] Decode batch [81355], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:12:54 TP0] Decode batch [81395], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:12:55 TP0] Decode batch [81435], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:12:56 TP0] Decode batch [81475], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:12:57 TP0] Decode batch [81515], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:12:58 TP0] Decode batch [81555], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:12:58 TP0] Decode batch [81595], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:12:59 TP0] Decode batch [81635], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:13:00] INFO:     127.0.0.1:40272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:13:00 TP0] Prefill batch [81649], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:13:00 TP0] Decode batch [81676], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-25 16:13:01 TP0] Decode batch [81716], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:13:02 TP0] Decode batch [81756], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:13:03 TP0] Decode batch [81796], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:13:03 TP0] Decode batch [81836], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:13:04 TP0] Decode batch [81876], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:13:05 TP0] Decode batch [81916], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:13:06 TP0] Decode batch [81956], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:13:07 TP0] Decode batch [81996], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:08 TP0] Decode batch [82036], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:08 TP0] Decode batch [82076], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:13:09 TP0] Decode batch [82116], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:10 TP0] Decode batch [82156], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-25 16:13:11 TP0] Decode batch [82196], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:12 TP0] Decode batch [82236], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:12 TP0] Decode batch [82276], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:13 TP0] Decode batch [82316], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:14 TP0] Decode batch [82356], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:15 TP0] Decode batch [82396], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:13:16 TP0] Decode batch [82436], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:13:16] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:13:16 TP0] Prefill batch [82450], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:13:17 TP0] Decode batch [82477], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:13:17 TP0] Decode batch [82517], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:13:18 TP0] Decode batch [82557], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:13:19 TP0] Decode batch [82597], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:13:20 TP0] Decode batch [82637], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:13:21 TP0] Decode batch [82677], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:13:22 TP0] Decode batch [82717], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:13:22 TP0] Decode batch [82757], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:23 TP0] Decode batch [82797], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:24 TP0] Decode batch [82837], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:13:25 TP0] Decode batch [82877], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:13:26 TP0] Decode batch [82917], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:26 TP0] Decode batch [82957], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:13:27 TP0] Decode batch [82997], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:13:28 TP0] Decode batch [83037], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:29 TP0] Decode batch [83077], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:13:30 TP0] Decode batch [83117], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:13:31 TP0] Decode batch [83157], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:31 TP0] Decode batch [83197], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:32 TP0] Decode batch [83237], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:32] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:13:32 TP0] Prefill batch [83251], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:13:33 TP0] Decode batch [83278], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:13:34 TP0] Decode batch [83318], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:13:35 TP0] Decode batch [83358], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:13:36 TP0] Decode batch [83398], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:13:36 TP0] Decode batch [83438], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:37 TP0] Decode batch [83478], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:13:38 TP0] Decode batch [83518], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:13:39 TP0] Decode batch [83558], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:13:40 TP0] Decode batch [83598], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:40 TP0] Decode batch [83638], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:13:41 TP0] Decode batch [83678], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:13:42 TP0] Decode batch [83718], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-25 16:13:43 TP0] Decode batch [83758], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:13:44 TP0] Decode batch [83798], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:13:45 TP0] Decode batch [83838], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:13:45 TP0] Decode batch [83878], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:13:46 TP0] Decode batch [83918], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:13:47 TP0] Decode batch [83958], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:13:48 TP0] Decode batch [83998], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:13:49 TP0] Decode batch [84038], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:13:49] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:13:49 TP0] Prefill batch [84052], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:13:50 TP0] Decode batch [84079], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-25 16:13:50 TP0] Decode batch [84119], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:13:51 TP0] Decode batch [84159], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:13:52 TP0] Decode batch [84199], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:13:53 TP0] Decode batch [84239], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:54 TP0] Decode batch [84279], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:13:54 TP0] Decode batch [84319], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:13:55 TP0] Decode batch [84359], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:13:56 TP0] Decode batch [84399], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:57 TP0] Decode batch [84439], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:13:58 TP0] Decode batch [84479], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:13:59 TP0] Decode batch [84519], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:13:59 TP0] Decode batch [84559], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:00 TP0] Decode batch [84599], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:01 TP0] Decode batch [84639], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:02 TP0] Decode batch [84679], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:03 TP0] Decode batch [84719], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:03 TP0] Decode batch [84759], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:04 TP0] Decode batch [84799], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:05 TP0] Decode batch [84839], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:05] INFO:     127.0.0.1:45876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:14:05 TP0] Prefill batch [84853], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:14:06 TP0] Decode batch [84880], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:14:07 TP0] Decode batch [84920], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:14:08 TP0] Decode batch [84960], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:14:08 TP0] Decode batch [85000], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:14:09 TP0] Decode batch [85040], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:14:10 TP0] Decode batch [85080], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:14:11 TP0] Decode batch [85120], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:14:12 TP0] Decode batch [85160], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:14:13 TP0] Decode batch [85200], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:13 TP0] Decode batch [85240], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:14:14 TP0] Decode batch [85280], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:14:15 TP0] Decode batch [85320], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-25 16:14:16 TP0] Decode batch [85360], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:17 TP0] Decode batch [85400], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:14:18 TP0] Decode batch [85440], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:18 TP0] Decode batch [85480], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:19 TP0] Decode batch [85520], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:20 TP0] Decode batch [85560], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:14:21 TP0] Decode batch [85600], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:22 TP0] Decode batch [85640], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:14:22] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:14:22 TP0] Prefill batch [85654], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:14:23 TP0] Decode batch [85681], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:14:23 TP0] Decode batch [85721], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:14:24 TP0] Decode batch [85761], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:14:25 TP0] Decode batch [85801], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:26 TP0] Decode batch [85841], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:14:27 TP0] Decode batch [85881], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:14:27 TP0] Decode batch [85921], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:14:28 TP0] Decode batch [85961], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:29 TP0] Decode batch [86001], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:30 TP0] Decode batch [86041], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:31 TP0] Decode batch [86081], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:32 TP0] Decode batch [86121], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:32 TP0] Decode batch [86161], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:33 TP0] Decode batch [86201], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:34 TP0] Decode batch [86241], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:35 TP0] Decode batch [86281], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:14:36 TP0] Decode batch [86321], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:36 TP0] Decode batch [86361], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:14:37 TP0] Decode batch [86401], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:38 TP0] Decode batch [86441], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:38] INFO:     127.0.0.1:59154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:14:38 TP0] Prefill batch [86455], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:14:39 TP0] Decode batch [86482], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-25 16:14:40 TP0] Decode batch [86522], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:14:41 TP0] Decode batch [86562], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:14:41 TP0] Decode batch [86602], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:42 TP0] Decode batch [86642], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:14:43 TP0] Decode batch [86682], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:44 TP0] Decode batch [86722], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:14:45 TP0] Decode batch [86762], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:46 TP0] Decode batch [86802], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:14:46 TP0] Decode batch [86842], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:14:47 TP0] Decode batch [86882], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:48 TP0] Decode batch [86922], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-25 16:14:49 TP0] Decode batch [86962], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:50 TP0] Decode batch [87002], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:50 TP0] Decode batch [87042], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:51 TP0] Decode batch [87082], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:52 TP0] Decode batch [87122], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:53 TP0] Decode batch [87162], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:14:54 TP0] Decode batch [87202], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:55 TP0] Decode batch [87242], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:14:55] INFO:     127.0.0.1:42898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:14:55 TP0] Prefill batch [87256], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:14:55 TP0] Decode batch [87283], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-25 16:14:56 TP0] Decode batch [87323], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-25 16:14:57 TP0] Decode batch [87363], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-25 16:14:58 TP0] Decode batch [87403], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-25 16:14:59 TP0] Decode batch [87443], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:15:00 TP0] Decode batch [87483], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:15:00 TP0] Decode batch [87523], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:15:01 TP0] Decode batch [87563], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:15:02 TP0] Decode batch [87603], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:15:03 TP0] Decode batch [87643], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:15:04 TP0] Decode batch [87683], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:15:04 TP0] Decode batch [87723], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:15:05 TP0] Decode batch [87763], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:15:06 TP0] Decode batch [87803], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:15:07 TP0] Decode batch [87843], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:15:08 TP0] Decode batch [87883], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:15:09 TP0] Decode batch [87923], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:15:09 TP0] Decode batch [87963], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:15:10 TP0] Decode batch [88003], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:15:11 TP0] Decode batch [88043], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:15:11] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:15:11 TP0] Prefill batch [88057], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:15:12 TP0] Decode batch [88084], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-25 16:15:13 TP0] Decode batch [88124], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:15:14 TP0] Decode batch [88164], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:15:14 TP0] Decode batch [88204], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:15:15 TP0] Decode batch [88244], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:15:16 TP0] Decode batch [88284], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:15:17 TP0] Decode batch [88324], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:15:18 TP0] Decode batch [88364], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:15:18 TP0] Decode batch [88404], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:15:19 TP0] Decode batch [88444], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:15:20 TP0] Decode batch [88484], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:15:21 TP0] Decode batch [88524], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:15:22 TP0] Decode batch [88564], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:15:23 TP0] Decode batch [88604], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:15:23 TP0] Decode batch [88644], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:15:24 TP0] Decode batch [88684], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:15:25 TP0] Decode batch [88724], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:15:26 TP0] Decode batch [88764], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:15:27 TP0] Decode batch [88804], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:15:27 TP0] Decode batch [88844], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:15:28] INFO:     127.0.0.1:37526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:15:28 TP0] Prefill batch [88858], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:15:28 TP0] Decode batch [88885], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-25 16:15:29 TP0] Decode batch [88925], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:15:30 TP0] Decode batch [88965], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:15:31 TP0] Decode batch [89005], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:15:32 TP0] Decode batch [89045], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:15:32 TP0] Decode batch [89085], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:15:33 TP0] Decode batch [89125], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:15:34 TP0] Decode batch [89165], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:15:35 TP0] Decode batch [89205], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:15:36 TP0] Decode batch [89245], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:15:37 TP0] Decode batch [89285], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:15:37 TP0] Decode batch [89325], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:15:38 TP0] Decode batch [89365], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:15:39 TP0] Decode batch [89405], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:15:40 TP0] Decode batch [89445], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:15:41 TP0] Decode batch [89485], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:15:41 TP0] Decode batch [89525], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:15:42 TP0] Decode batch [89565], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:15:43 TP0] Decode batch [89605], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:15:44 TP0] Decode batch [89645], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:15:44] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:15:44 TP0] Prefill batch [89659], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:15:45 TP0] Decode batch [89686], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-25 16:15:46 TP0] Decode batch [89726], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:15:46 TP0] Decode batch [89766], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:15:47 TP0] Decode batch [89806], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:15:48 TP0] Decode batch [89846], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:15:49 TP0] Decode batch [89886], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:15:50 TP0] Decode batch [89926], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:15:51 TP0] Decode batch [89966], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:15:51 TP0] Decode batch [90006], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:15:52 TP0] Decode batch [90046], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:15:53 TP0] Decode batch [90086], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:15:54 TP0] Decode batch [90126], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-25 16:15:55 TP0] Decode batch [90166], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:15:55 TP0] Decode batch [90206], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:15:56 TP0] Decode batch [90246], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:15:57 TP0] Decode batch [90286], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:15:58 TP0] Decode batch [90326], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:15:59 TP0] Decode batch [90366], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:00 TP0] Decode batch [90406], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:00 TP0] Decode batch [90446], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:16:01] INFO:     127.0.0.1:50300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:16:01 TP0] Prefill batch [90460], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:16:01 TP0] Decode batch [90487], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:16:02 TP0] Decode batch [90527], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:16:03 TP0] Decode batch [90567], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:16:04 TP0] Decode batch [90607], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:16:05 TP0] Decode batch [90647], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:16:05 TP0] Decode batch [90687], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:16:06 TP0] Decode batch [90727], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:16:07 TP0] Decode batch [90767], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:16:08 TP0] Decode batch [90807], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:16:09 TP0] Decode batch [90847], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:16:09 TP0] Decode batch [90887], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:16:10 TP0] Decode batch [90927], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:16:11 TP0] Decode batch [90967], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:16:12 TP0] Decode batch [91007], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:13 TP0] Decode batch [91047], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:16:14 TP0] Decode batch [91087], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:14 TP0] Decode batch [91127], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:16:15 TP0] Decode batch [91167], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:16:16 TP0] Decode batch [91207], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:17 TP0] Decode batch [91247], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:17] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:16:17 TP0] Prefill batch [91261], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:16:18 TP0] Decode batch [91288], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-25 16:16:19 TP0] Decode batch [91328], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:16:19 TP0] Decode batch [91368], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:16:20 TP0] Decode batch [91408], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:16:21 TP0] Decode batch [91448], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:16:22 TP0] Decode batch [91488], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:16:23 TP0] Decode batch [91528], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:16:24 TP0] Decode batch [91568], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:16:24 TP0] Decode batch [91608], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:16:25 TP0] Decode batch [91648], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:16:26 TP0] Decode batch [91688], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-25 16:16:27 TP0] Decode batch [91728], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:16:28 TP0] Decode batch [91768], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:16:28 TP0] Decode batch [91808], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:16:29 TP0] Decode batch [91848], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:16:30 TP0] Decode batch [91888], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:16:31 TP0] Decode batch [91928], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:16:32 TP0] Decode batch [91968], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:16:33 TP0] Decode batch [92008], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:16:33 TP0] Decode batch [92048], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:16:34] INFO:     127.0.0.1:49544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:16:34 TP0] Prefill batch [92062], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:16:34 TP0] Decode batch [92089], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:16:35 TP0] Decode batch [92129], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:16:36 TP0] Decode batch [92169], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:16:37 TP0] Decode batch [92209], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:16:38 TP0] Decode batch [92249], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:16:38 TP0] Decode batch [92289], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:16:39 TP0] Decode batch [92329], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:16:40 TP0] Decode batch [92369], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:16:41 TP0] Decode batch [92409], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:16:42 TP0] Decode batch [92449], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:16:42 TP0] Decode batch [92489], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:16:43 TP0] Decode batch [92529], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:16:44 TP0] Decode batch [92569], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:16:45 TP0] Decode batch [92609], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:16:46 TP0] Decode batch [92649], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:16:47 TP0] Decode batch [92689], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:16:47 TP0] Decode batch [92729], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:16:48 TP0] Decode batch [92769], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:16:49 TP0] Decode batch [92809], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:16:50 TP0] Decode batch [92849], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:16:50] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:16:50 TP0] Prefill batch [92863], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:16:51 TP0] Decode batch [92890], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.90, #queue-req: 0, 
[2025-10-25 16:16:52 TP0] Decode batch [92930], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:16:52 TP0] Decode batch [92970], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:53 TP0] Decode batch [93010], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:16:54 TP0] Decode batch [93050], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:55 TP0] Decode batch [93090], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:16:56 TP0] Decode batch [93130], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:56 TP0] Decode batch [93170], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:57 TP0] Decode batch [93210], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:16:58 TP0] Decode batch [93250], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:16:59 TP0] Decode batch [93290], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-25 16:17:00 TP0] Decode batch [93330], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:01 TP0] Decode batch [93370], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:01 TP0] Decode batch [93410], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:02 TP0] Decode batch [93450], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:03 TP0] Decode batch [93490], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:04 TP0] Decode batch [93530], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:05 TP0] Decode batch [93570], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:05 TP0] Decode batch [93610], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:06 TP0] Decode batch [93650], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:07] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:17:07 TP0] Prefill batch [93664], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:17:07 TP0] Decode batch [93691], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.82, #queue-req: 0, 
[2025-10-25 16:17:08 TP0] Decode batch [93731], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:09 TP0] Decode batch [93771], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:10 TP0] Decode batch [93811], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:10 TP0] Decode batch [93851], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:11 TP0] Decode batch [93891], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:12 TP0] Decode batch [93931], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:13 TP0] Decode batch [93971], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:14 TP0] Decode batch [94011], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:15 TP0] Decode batch [94051], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:15 TP0] Decode batch [94091], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:16 TP0] Decode batch [94131], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:17 TP0] Decode batch [94171], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:18 TP0] Decode batch [94211], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:19 TP0] Decode batch [94251], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:20 TP0] Decode batch [94291], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:20 TP0] Decode batch [94331], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:21 TP0] Decode batch [94371], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:22 TP0] Decode batch [94411], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:23 TP0] Decode batch [94451], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:23] INFO:     127.0.0.1:36138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:17:23 TP0] Prefill batch [94465], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:17:24 TP0] Decode batch [94492], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-25 16:17:25 TP0] Decode batch [94532], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:17:25 TP0] Decode batch [94572], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:17:26 TP0] Decode batch [94612], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:17:27 TP0] Decode batch [94652], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:17:28 TP0] Decode batch [94692], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:29 TP0] Decode batch [94732], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:17:29 TP0] Decode batch [94772], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:30 TP0] Decode batch [94812], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:31 TP0] Decode batch [94852], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:32 TP0] Decode batch [94892], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-10-25 16:17:33 TP0] Decode batch [94932], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:17:34 TP0] Decode batch [94972], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:34 TP0] Decode batch [95012], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:17:35 TP0] Decode batch [95052], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:36 TP0] Decode batch [95092], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:37 TP0] Decode batch [95132], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:17:38 TP0] Decode batch [95172], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:38 TP0] Decode batch [95212], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:17:39 TP0] Decode batch [95252], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:17:40] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:17:40 TP0] Prefill batch [95266], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:17:40 TP0] Decode batch [95293], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:17:41 TP0] Decode batch [95333], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:17:42 TP0] Decode batch [95373], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:17:43 TP0] Decode batch [95413], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:17:43 TP0] Decode batch [95453], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:17:44 TP0] Decode batch [95493], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:17:45 TP0] Decode batch [95533], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:17:46 TP0] Decode batch [95573], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:17:47 TP0] Decode batch [95613], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:17:48 TP0] Decode batch [95653], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:17:48 TP0] Decode batch [95693], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:17:49 TP0] Decode batch [95733], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:17:50 TP0] Decode batch [95773], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:17:51 TP0] Decode batch [95813], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:17:52 TP0] Decode batch [95853], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:17:52 TP0] Decode batch [95893], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:17:53 TP0] Decode batch [95933], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:17:54 TP0] Decode batch [95973], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:17:55 TP0] Decode batch [96013], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:17:56 TP0] Decode batch [96053], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:17:56] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:17:56 TP0] Prefill batch [96067], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:17:57 TP0] Decode batch [96094], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:17:57 TP0] Decode batch [96134], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:17:58 TP0] Decode batch [96174], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:17:59 TP0] Decode batch [96214], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:18:00 TP0] Decode batch [96254], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:18:01 TP0] Decode batch [96294], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:18:02 TP0] Decode batch [96334], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:18:02 TP0] Decode batch [96374], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:03 TP0] Decode batch [96414], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:18:04 TP0] Decode batch [96454], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:05 TP0] Decode batch [96494], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:06 TP0] Decode batch [96534], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:18:06 TP0] Decode batch [96574], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:07 TP0] Decode batch [96614], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:08 TP0] Decode batch [96654], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:09 TP0] Decode batch [96694], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:10 TP0] Decode batch [96734], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:11 TP0] Decode batch [96774], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:11 TP0] Decode batch [96814], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:12 TP0] Decode batch [96854], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:12] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:18:12 TP0] Prefill batch [96868], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:18:13 TP0] Decode batch [96895], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:18:14 TP0] Decode batch [96935], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:15 TP0] Decode batch [96975], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:16 TP0] Decode batch [97015], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:16 TP0] Decode batch [97055], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:17 TP0] Decode batch [97095], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:18 TP0] Decode batch [97135], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:19 TP0] Decode batch [97175], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:20 TP0] Decode batch [97215], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:20 TP0] Decode batch [97255], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:21 TP0] Decode batch [97295], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:18:22 TP0] Decode batch [97335], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:23 TP0] Decode batch [97375], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:24 TP0] Decode batch [97415], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:18:25 TP0] Decode batch [97455], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:18:25 TP0] Decode batch [97495], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:26 TP0] Decode batch [97535], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:18:27 TP0] Decode batch [97575], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:18:28 TP0] Decode batch [97615], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:18:29 TP0] Decode batch [97655], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:18:29] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:18:29 TP0] Prefill batch [97669], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:18:30 TP0] Decode batch [97696], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:18:30 TP0] Decode batch [97736], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:18:31 TP0] Decode batch [97776], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:18:32 TP0] Decode batch [97816], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:33 TP0] Decode batch [97856], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:34 TP0] Decode batch [97896], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:35 TP0] Decode batch [97936], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:35 TP0] Decode batch [97976], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:36 TP0] Decode batch [98016], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:37 TP0] Decode batch [98056], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:38 TP0] Decode batch [98096], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:39 TP0] Decode batch [98136], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:39 TP0] Decode batch [98176], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:40 TP0] Decode batch [98216], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:18:41 TP0] Decode batch [98256], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:42 TP0] Decode batch [98296], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:43 TP0] Decode batch [98336], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:44 TP0] Decode batch [98376], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:18:44 TP0] Decode batch [98416], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:45 TP0] Decode batch [98456], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:18:45] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:18:45 TP0] Prefill batch [98470], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:18:46 TP0] Decode batch [98497], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:18:47 TP0] Decode batch [98537], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:18:48 TP0] Decode batch [98577], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:18:49 TP0] Decode batch [98617], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:18:49 TP0] Decode batch [98657], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:18:50 TP0] Decode batch [98697], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:18:51 TP0] Decode batch [98737], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:52 TP0] Decode batch [98777], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:53 TP0] Decode batch [98817], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:53 TP0] Decode batch [98857], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:18:54 TP0] Decode batch [98897], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:55 TP0] Decode batch [98937], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:56 TP0] Decode batch [98977], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:18:57 TP0] Decode batch [99017], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:58 TP0] Decode batch [99057], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:18:58 TP0] Decode batch [99097], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:18:59 TP0] Decode batch [99137], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:00 TP0] Decode batch [99177], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:01 TP0] Decode batch [99217], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:02 TP0] Decode batch [99257], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:02] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:19:02 TP0] Prefill batch [99271], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:19:03 TP0] Decode batch [99298], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-25 16:19:03 TP0] Decode batch [99338], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:19:04 TP0] Decode batch [99378], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:19:05 TP0] Decode batch [99418], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:19:06 TP0] Decode batch [99458], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:19:07 TP0] Decode batch [99498], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:19:07 TP0] Decode batch [99538], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:19:08 TP0] Decode batch [99578], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:09 TP0] Decode batch [99618], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:19:10 TP0] Decode batch [99658], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:11 TP0] Decode batch [99698], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:12 TP0] Decode batch [99738], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:12 TP0] Decode batch [99778], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:13 TP0] Decode batch [99818], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:14 TP0] Decode batch [99858], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:15 TP0] Decode batch [99898], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:16 TP0] Decode batch [99938], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:16 TP0] Decode batch [99978], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:17 TP0] Decode batch [100018], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:18 TP0] Decode batch [100058], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:18] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:19:18 TP0] Prefill batch [100072], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:19:19 TP0] Decode batch [100099], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:19:20 TP0] Decode batch [100139], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:19:21 TP0] Decode batch [100179], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:19:21 TP0] Decode batch [100219], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:19:22 TP0] Decode batch [100259], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:23 TP0] Decode batch [100299], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:24 TP0] Decode batch [100339], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:25 TP0] Decode batch [100379], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:26 TP0] Decode batch [100419], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:26 TP0] Decode batch [100459], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:27 TP0] Decode batch [100499], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:28 TP0] Decode batch [100539], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:29 TP0] Decode batch [100579], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:30 TP0] Decode batch [100619], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:30 TP0] Decode batch [100659], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:31 TP0] Decode batch [100699], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:32 TP0] Decode batch [100739], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:33 TP0] Decode batch [100779], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:34 TP0] Decode batch [100819], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:35 TP0] Decode batch [100859], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:35] INFO:     127.0.0.1:47002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:19:35 TP0] Prefill batch [100873], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:19:35 TP0] Decode batch [100900], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:19:36 TP0] Decode batch [100940], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:37 TP0] Decode batch [100980], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:38 TP0] Decode batch [101020], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:39 TP0] Decode batch [101060], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:40 TP0] Decode batch [101100], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:40 TP0] Decode batch [101140], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:41 TP0] Decode batch [101180], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:42 TP0] Decode batch [101220], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:43 TP0] Decode batch [101260], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:44 TP0] Decode batch [101300], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:44 TP0] Decode batch [101340], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:45 TP0] Decode batch [101380], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:46 TP0] Decode batch [101420], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:47 TP0] Decode batch [101460], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:19:48 TP0] Decode batch [101500], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:49 TP0] Decode batch [101540], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:49 TP0] Decode batch [101580], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:19:50 TP0] Decode batch [101620], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:51 TP0] Decode batch [101660], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:19:51] INFO:     127.0.0.1:38328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:19:51 TP0] Prefill batch [101674], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:19:52 TP0] Decode batch [101701], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:19:53 TP0] Decode batch [101741], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:54 TP0] Decode batch [101781], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:19:54 TP0] Decode batch [101821], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:55 TP0] Decode batch [101861], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:19:56 TP0] Decode batch [101901], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:57 TP0] Decode batch [101941], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:58 TP0] Decode batch [101981], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:58 TP0] Decode batch [102021], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:19:59 TP0] Decode batch [102061], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:00 TP0] Decode batch [102101], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:01 TP0] Decode batch [102141], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:02 TP0] Decode batch [102181], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:03 TP0] Decode batch [102221], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:20:03 TP0] Decode batch [102261], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:04 TP0] Decode batch [102301], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:05 TP0] Decode batch [102341], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:20:06 TP0] Decode batch [102381], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:20:07 TP0] Decode batch [102421], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:08 TP0] Decode batch [102461], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:20:08] INFO:     127.0.0.1:41906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:20:08 TP0] Prefill batch [102475], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:20:08 TP0] Decode batch [102502], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:20:09 TP0] Decode batch [102542], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:10 TP0] Decode batch [102582], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:11 TP0] Decode batch [102622], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:12 TP0] Decode batch [102662], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:13 TP0] Decode batch [102702], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:13 TP0] Decode batch [102742], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:14 TP0] Decode batch [102782], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:15 TP0] Decode batch [102822], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:16 TP0] Decode batch [102862], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:17 TP0] Decode batch [102902], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:20:17 TP0] Decode batch [102942], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:18 TP0] Decode batch [102982], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:20:19 TP0] Decode batch [103022], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:20 TP0] Decode batch [103062], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:21 TP0] Decode batch [103102], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:22 TP0] Decode batch [103142], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:20:22 TP0] Decode batch [103182], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:20:23 TP0] Decode batch [103222], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:20:24 TP0] Decode batch [103262], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:20:24] INFO:     127.0.0.1:42130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:20:24 TP0] Prefill batch [103276], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:20:25 TP0] Decode batch [103303], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:20:26 TP0] Decode batch [103343], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:20:27 TP0] Decode batch [103383], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:20:27 TP0] Decode batch [103423], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:20:28 TP0] Decode batch [103463], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:20:29 TP0] Decode batch [103503], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:20:30 TP0] Decode batch [103543], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:20:31 TP0] Decode batch [103583], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:20:31 TP0] Decode batch [103623], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:20:32 TP0] Decode batch [103663], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:33 TP0] Decode batch [103703], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:20:34 TP0] Decode batch [103743], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:35 TP0] Decode batch [103783], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:20:36 TP0] Decode batch [103823], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:36 TP0] Decode batch [103863], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:37 TP0] Decode batch [103903], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:38 TP0] Decode batch [103943], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:39 TP0] Decode batch [103983], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:40 TP0] Decode batch [104023], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:40 TP0] Decode batch [104063], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:41] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:20:41 TP0] Prefill batch [104077], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:20:41 TP0] Decode batch [104104], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:20:42 TP0] Decode batch [104144], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:20:43 TP0] Decode batch [104184], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:44 TP0] Decode batch [104224], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:20:45 TP0] Decode batch [104264], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:45 TP0] Decode batch [104304], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:20:46 TP0] Decode batch [104344], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:47 TP0] Decode batch [104384], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:48 TP0] Decode batch [104424], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.22, #queue-req: 0, 
[2025-10-25 16:20:49 TP0] Decode batch [104464], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:50 TP0] Decode batch [104504], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:50 TP0] Decode batch [104544], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:51 TP0] Decode batch [104584], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:52 TP0] Decode batch [104624], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:53 TP0] Decode batch [104664], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:20:54 TP0] Decode batch [104704], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:54 TP0] Decode batch [104744], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:55 TP0] Decode batch [104784], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:20:56 TP0] Decode batch [104824], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:57 TP0] Decode batch [104864], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:20:57] INFO:     127.0.0.1:41984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:20:57 TP0] Prefill batch [104878], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:20:58 TP0] Decode batch [104905], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-25 16:20:59 TP0] Decode batch [104945], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.24, #queue-req: 0, 
[2025-10-25 16:20:59 TP0] Decode batch [104985], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:21:00 TP0] Decode batch [105025], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:21:01 TP0] Decode batch [105065], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:21:02 TP0] Decode batch [105105], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:21:03 TP0] Decode batch [105145], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:21:04 TP0] Decode batch [105185], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:04 TP0] Decode batch [105225], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:21:05 TP0] Decode batch [105265], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:21:06 TP0] Decode batch [105305], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:21:07 TP0] Decode batch [105345], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:08 TP0] Decode batch [105385], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:21:09 TP0] Decode batch [105425], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.22, #queue-req: 0, 
[2025-10-25 16:21:09 TP0] Decode batch [105465], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:10 TP0] Decode batch [105505], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:21:11 TP0] Decode batch [105545], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:21:12 TP0] Decode batch [105585], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:13 TP0] Decode batch [105625], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:13 TP0] Decode batch [105665], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:14] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:21:14 TP0] Prefill batch [105679], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:21:14 TP0] Decode batch [105706], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-25 16:21:15 TP0] Decode batch [105746], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:21:16 TP0] Decode batch [105786], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:21:17 TP0] Decode batch [105826], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:21:18 TP0] Decode batch [105866], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:18 TP0] Decode batch [105906], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:19 TP0] Decode batch [105946], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:20 TP0] Decode batch [105986], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:21 TP0] Decode batch [106026], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:22 TP0] Decode batch [106066], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:23 TP0] Decode batch [106106], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:21:23 TP0] Decode batch [106146], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:21:24 TP0] Decode batch [106186], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:21:25 TP0] Decode batch [106226], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:21:26 TP0] Decode batch [106266], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:21:27 TP0] Decode batch [106306], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:21:27 TP0] Decode batch [106346], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:21:28 TP0] Decode batch [106386], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:21:29 TP0] Decode batch [106426], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:21:30 TP0] Decode batch [106466], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:21:30] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:21:30 TP0] Prefill batch [106480], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:21:31 TP0] Decode batch [106507], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-25 16:21:32 TP0] Decode batch [106547], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:21:32 TP0] Decode batch [106587], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:33 TP0] Decode batch [106627], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:34 TP0] Decode batch [106667], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:21:35 TP0] Decode batch [106707], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:21:36 TP0] Decode batch [106747], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:37 TP0] Decode batch [106787], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:37 TP0] Decode batch [106827], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:38 TP0] Decode batch [106867], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:21:39 TP0] Decode batch [106907], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:21:40 TP0] Decode batch [106947], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:41 TP0] Decode batch [106987], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:21:41 TP0] Decode batch [107027], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:21:42 TP0] Decode batch [107067], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:43 TP0] Decode batch [107107], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:44 TP0] Decode batch [107147], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:45 TP0] Decode batch [107187], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:46 TP0] Decode batch [107227], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:21:46 TP0] Decode batch [107267], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:47] INFO:     127.0.0.1:32878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:21:47 TP0] Prefill batch [107281], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:21:47 TP0] Decode batch [107308], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.96, #queue-req: 0, 
[2025-10-25 16:21:48 TP0] Decode batch [107348], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:49 TP0] Decode batch [107388], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:21:50 TP0] Decode batch [107428], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:51 TP0] Decode batch [107468], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:21:51 TP0] Decode batch [107508], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:21:52 TP0] Decode batch [107548], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:53 TP0] Decode batch [107588], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:21:54 TP0] Decode batch [107628], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:21:55 TP0] Decode batch [107668], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:56 TP0] Decode batch [107708], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:21:56 TP0] Decode batch [107748], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:21:57 TP0] Decode batch [107788], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:21:58 TP0] Decode batch [107828], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:21:59 TP0] Decode batch [107868], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:22:00 TP0] Decode batch [107908], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:22:00 TP0] Decode batch [107948], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:22:01 TP0] Decode batch [107988], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:22:02 TP0] Decode batch [108028], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:22:03 TP0] Decode batch [108068], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:22:03] INFO:     127.0.0.1:46030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:22:03 TP0] Prefill batch [108082], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:22:04 TP0] Decode batch [108109], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-25 16:22:05 TP0] Decode batch [108149], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:05 TP0] Decode batch [108189], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:06 TP0] Decode batch [108229], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:22:07 TP0] Decode batch [108269], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:08 TP0] Decode batch [108309], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:09 TP0] Decode batch [108349], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:10 TP0] Decode batch [108389], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:10 TP0] Decode batch [108429], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:11 TP0] Decode batch [108469], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:12 TP0] Decode batch [108509], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:22:13 TP0] Decode batch [108549], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:14 TP0] Decode batch [108589], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:22:14 TP0] Decode batch [108629], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:22:15 TP0] Decode batch [108669], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:16 TP0] Decode batch [108709], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:22:17 TP0] Decode batch [108749], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:22:18 TP0] Decode batch [108789], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:22:19 TP0] Decode batch [108829], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:22:19 TP0] Decode batch [108869], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:22:20] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:22:20 TP0] Prefill batch [108883], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:22:20 TP0] Decode batch [108910], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-25 16:22:21 TP0] Decode batch [108950], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:22:22 TP0] Decode batch [108990], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:22:23 TP0] Decode batch [109030], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:22:24 TP0] Decode batch [109070], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:24 TP0] Decode batch [109110], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:25 TP0] Decode batch [109150], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:22:26 TP0] Decode batch [109190], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:27 TP0] Decode batch [109230], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:28 TP0] Decode batch [109270], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:28 TP0] Decode batch [109310], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:29 TP0] Decode batch [109350], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:30 TP0] Decode batch [109390], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:31 TP0] Decode batch [109430], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:32 TP0] Decode batch [109470], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:33 TP0] Decode batch [109510], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:33 TP0] Decode batch [109550], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:22:34 TP0] Decode batch [109590], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:35 TP0] Decode batch [109630], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:22:36 TP0] Decode batch [109670], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:36] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:22:36 TP0] Prefill batch [109684], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:22:37 TP0] Decode batch [109711], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:22:38 TP0] Decode batch [109751], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:22:38 TP0] Decode batch [109791], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:22:39 TP0] Decode batch [109831], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:22:40 TP0] Decode batch [109871], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:22:41 TP0] Decode batch [109911], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:22:42 TP0] Decode batch [109951], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:22:42 TP0] Decode batch [109991], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:22:43 TP0] Decode batch [110031], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:22:44 TP0] Decode batch [110071], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:22:45 TP0] Decode batch [110111], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:22:46 TP0] Decode batch [110151], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:22:47 TP0] Decode batch [110191], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:22:47 TP0] Decode batch [110231], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:22:48 TP0] Decode batch [110271], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:22:49 TP0] Decode batch [110311], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:22:50 TP0] Decode batch [110351], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:22:51 TP0] Decode batch [110391], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:22:51 TP0] Decode batch [110431], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:22:52 TP0] Decode batch [110471], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:22:53] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:22:53 TP0] Prefill batch [110485], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:22:53 TP0] Decode batch [110512], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-10-25 16:22:54 TP0] Decode batch [110552], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:22:55 TP0] Decode batch [110592], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:22:56 TP0] Decode batch [110632], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:56 TP0] Decode batch [110672], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:57 TP0] Decode batch [110712], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:22:58 TP0] Decode batch [110752], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:22:59 TP0] Decode batch [110792], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:23:00 TP0] Decode batch [110832], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:23:01 TP0] Decode batch [110872], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:23:01 TP0] Decode batch [110912], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:23:02 TP0] Decode batch [110952], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:23:03 TP0] Decode batch [110992], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:23:04 TP0] Decode batch [111032], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:23:05 TP0] Decode batch [111072], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:05 TP0] Decode batch [111112], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:23:06 TP0] Decode batch [111152], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:07 TP0] Decode batch [111192], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:23:08 TP0] Decode batch [111232], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:23:09 TP0] Decode batch [111272], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:23:09] INFO:     127.0.0.1:38046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:23:09 TP0] Prefill batch [111286], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:23:10 TP0] Decode batch [111313], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-25 16:23:10 TP0] Decode batch [111353], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:23:11 TP0] Decode batch [111393], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:23:12 TP0] Decode batch [111433], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:23:13 TP0] Decode batch [111473], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:23:14 TP0] Decode batch [111513], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:23:15 TP0] Decode batch [111553], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:23:15 TP0] Decode batch [111593], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:23:16 TP0] Decode batch [111633], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:23:17 TP0] Decode batch [111673], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:23:18 TP0] Decode batch [111713], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:23:19 TP0] Decode batch [111753], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:23:19 TP0] Decode batch [111793], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:23:20 TP0] Decode batch [111833], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:23:21 TP0] Decode batch [111873], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:23:22 TP0] Decode batch [111913], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:23:23 TP0] Decode batch [111953], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:23:24 TP0] Decode batch [111993], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:23:24 TP0] Decode batch [112033], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:23:25 TP0] Decode batch [112073], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:23:25] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:23:26 TP0] Prefill batch [112087], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:23:26 TP0] Decode batch [112114], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-25 16:23:27 TP0] Decode batch [112154], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:23:28 TP0] Decode batch [112194], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:23:29 TP0] Decode batch [112234], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:23:29 TP0] Decode batch [112274], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:23:30 TP0] Decode batch [112314], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:23:31 TP0] Decode batch [112354], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:23:32 TP0] Decode batch [112394], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:23:33 TP0] Decode batch [112434], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:33 TP0] Decode batch [112474], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:34 TP0] Decode batch [112514], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:35 TP0] Decode batch [112554], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:36 TP0] Decode batch [112594], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:23:37 TP0] Decode batch [112634], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:23:38 TP0] Decode batch [112674], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:38 TP0] Decode batch [112714], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:39 TP0] Decode batch [112754], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:40 TP0] Decode batch [112794], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:23:41 TP0] Decode batch [112834], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:23:42 TP0] Decode batch [112874], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:23:42] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:23:42 TP0] Prefill batch [112888], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:23:43 TP0] Decode batch [112915], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-25 16:23:43 TP0] Decode batch [112955], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:23:44 TP0] Decode batch [112995], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:23:45 TP0] Decode batch [113035], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:23:46 TP0] Decode batch [113075], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:23:47 TP0] Decode batch [113115], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:23:47 TP0] Decode batch [113155], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:23:48 TP0] Decode batch [113195], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:23:49 TP0] Decode batch [113235], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:23:50 TP0] Decode batch [113275], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:23:51 TP0] Decode batch [113315], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:23:52 TP0] Decode batch [113355], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:23:52 TP0] Decode batch [113395], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:23:53 TP0] Decode batch [113435], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:23:54 TP0] Decode batch [113475], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:23:55 TP0] Decode batch [113515], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:23:56 TP0] Decode batch [113555], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:23:56 TP0] Decode batch [113595], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:23:57 TP0] Decode batch [113635], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:23:58 TP0] Decode batch [113675], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:23:58] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:23:58 TP0] Prefill batch [113689], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:23:59 TP0] Decode batch [113716], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-10-25 16:24:00 TP0] Decode batch [113756], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:01 TP0] Decode batch [113796], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:24:01 TP0] Decode batch [113836], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:24:02 TP0] Decode batch [113876], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:24:03 TP0] Decode batch [113916], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:04 TP0] Decode batch [113956], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:05 TP0] Decode batch [113996], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:24:06 TP0] Decode batch [114036], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:06 TP0] Decode batch [114076], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:24:07 TP0] Decode batch [114116], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:24:08 TP0] Decode batch [114156], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:09 TP0] Decode batch [114196], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:24:10 TP0] Decode batch [114236], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:24:10 TP0] Decode batch [114276], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:24:11 TP0] Decode batch [114316], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:24:12 TP0] Decode batch [114356], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:24:13 TP0] Decode batch [114396], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.33, #queue-req: 0, 
[2025-10-25 16:24:14 TP0] Decode batch [114436], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:24:15 TP0] Decode batch [114476], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:24:15] INFO:     127.0.0.1:39634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:24:15 TP0] Prefill batch [114490], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:24:15 TP0] Decode batch [114517], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-25 16:24:16 TP0] Decode batch [114557], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:24:17 TP0] Decode batch [114597], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:24:18 TP0] Decode batch [114637], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:24:19 TP0] Decode batch [114677], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:24:20 TP0] Decode batch [114717], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:24:20 TP0] Decode batch [114757], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:24:21 TP0] Decode batch [114797], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:24:22 TP0] Decode batch [114837], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:24:23 TP0] Decode batch [114877], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:24:24 TP0] Decode batch [114917], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:24:25 TP0] Decode batch [114957], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:24:25 TP0] Decode batch [114997], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:24:26 TP0] Decode batch [115037], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:24:27 TP0] Decode batch [115077], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:24:28 TP0] Decode batch [115117], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:24:29 TP0] Decode batch [115157], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:24:29 TP0] Decode batch [115197], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:24:30 TP0] Decode batch [115237], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:24:31 TP0] Decode batch [115277], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:24:31] INFO:     127.0.0.1:39542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:24:31 TP0] Prefill batch [115291], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:24:32 TP0] Decode batch [115318], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-25 16:24:33 TP0] Decode batch [115358], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:24:34 TP0] Decode batch [115398], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:24:34 TP0] Decode batch [115438], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-25 16:24:35 TP0] Decode batch [115478], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:24:36 TP0] Decode batch [115518], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-25 16:24:37 TP0] Decode batch [115558], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:24:38 TP0] Decode batch [115598], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:24:38 TP0] Decode batch [115638], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:24:39 TP0] Decode batch [115678], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:24:40 TP0] Decode batch [115718], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:24:41 TP0] Decode batch [115758], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-25 16:24:42 TP0] Decode batch [115798], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:24:43 TP0] Decode batch [115838], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:43 TP0] Decode batch [115878], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:44 TP0] Decode batch [115918], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:45 TP0] Decode batch [115958], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:46 TP0] Decode batch [115998], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:24:47 TP0] Decode batch [116038], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:47 TP0] Decode batch [116078], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:24:48] INFO:     127.0.0.1:35636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:24:48 TP0] Prefill batch [116092], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:24:48 TP0] Decode batch [116119], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.97, #queue-req: 0, 
[2025-10-25 16:24:49 TP0] Decode batch [116159], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:24:50 TP0] Decode batch [116199], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:24:51 TP0] Decode batch [116239], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:24:52 TP0] Decode batch [116279], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:24:53 TP0] Decode batch [116319], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:24:53 TP0] Decode batch [116359], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:24:54 TP0] Decode batch [116399], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:24:55 TP0] Decode batch [116439], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:24:56 TP0] Decode batch [116479], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:24:57 TP0] Decode batch [116519], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:24:57 TP0] Decode batch [116559], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:24:58 TP0] Decode batch [116599], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:24:59 TP0] Decode batch [116639], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:25:00 TP0] Decode batch [116679], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:25:01 TP0] Decode batch [116719], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:25:02 TP0] Decode batch [116759], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:25:02 TP0] Decode batch [116799], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:25:03 TP0] Decode batch [116839], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:25:04 TP0] Decode batch [116879], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:25:04] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:25:04 TP0] Prefill batch [116893], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:25:05 TP0] Decode batch [116920], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:25:06 TP0] Decode batch [116960], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:25:07 TP0] Decode batch [117000], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:25:07 TP0] Decode batch [117040], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:25:08 TP0] Decode batch [117080], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:25:09 TP0] Decode batch [117120], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:25:10 TP0] Decode batch [117160], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:25:11 TP0] Decode batch [117200], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:25:11 TP0] Decode batch [117240], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:25:12 TP0] Decode batch [117280], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:25:13 TP0] Decode batch [117320], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:25:14 TP0] Decode batch [117360], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:25:15 TP0] Decode batch [117400], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:25:16 TP0] Decode batch [117440], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:25:16 TP0] Decode batch [117480], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:25:17 TP0] Decode batch [117520], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:25:18 TP0] Decode batch [117560], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:25:19 TP0] Decode batch [117600], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:25:20 TP0] Decode batch [117640], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:25:20 TP0] Decode batch [117680], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:25:21] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:25:21 TP0] Prefill batch [117694], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:25:21 TP0] Decode batch [117721], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:25:22 TP0] Decode batch [117761], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:25:23 TP0] Decode batch [117801], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:25:24 TP0] Decode batch [117841], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:25:25 TP0] Decode batch [117881], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:25 TP0] Decode batch [117921], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:25:26 TP0] Decode batch [117961], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:27 TP0] Decode batch [118001], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:28 TP0] Decode batch [118041], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:29 TP0] Decode batch [118081], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:30 TP0] Decode batch [118121], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:25:30 TP0] Decode batch [118161], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:31 TP0] Decode batch [118201], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:25:32 TP0] Decode batch [118241], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:25:33 TP0] Decode batch [118281], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:25:34 TP0] Decode batch [118321], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:34 TP0] Decode batch [118361], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:25:35 TP0] Decode batch [118401], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:25:36 TP0] Decode batch [118441], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:25:37 TP0] Decode batch [118481], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:25:37] INFO:     127.0.0.1:50794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:25:37 TP0] Prefill batch [118495], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:25:38 TP0] Decode batch [118522], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:25:39 TP0] Decode batch [118562], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:25:39 TP0] Decode batch [118602], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:25:40 TP0] Decode batch [118642], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:25:41 TP0] Decode batch [118682], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:25:42 TP0] Decode batch [118722], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:43 TP0] Decode batch [118762], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:44 TP0] Decode batch [118802], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:44 TP0] Decode batch [118842], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:25:45 TP0] Decode batch [118882], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:46 TP0] Decode batch [118922], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:25:47 TP0] Decode batch [118962], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:25:48 TP0] Decode batch [119002], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:25:48 TP0] Decode batch [119042], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:25:49 TP0] Decode batch [119082], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:25:50 TP0] Decode batch [119122], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:25:51 TP0] Decode batch [119162], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:25:52 TP0] Decode batch [119202], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:25:53 TP0] Decode batch [119242], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:25:53 TP0] Decode batch [119282], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:25:54] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:25:54 TP0] Prefill batch [119296], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:25:54 TP0] Decode batch [119323], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:25:55 TP0] Decode batch [119363], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:25:56 TP0] Decode batch [119403], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:25:57 TP0] Decode batch [119443], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:25:58 TP0] Decode batch [119483], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:25:58 TP0] Decode batch [119523], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:25:59 TP0] Decode batch [119563], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:26:00 TP0] Decode batch [119603], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:26:01 TP0] Decode batch [119643], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:02 TP0] Decode batch [119683], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:26:02 TP0] Decode batch [119723], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:03 TP0] Decode batch [119763], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:04 TP0] Decode batch [119803], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:05 TP0] Decode batch [119843], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:26:06 TP0] Decode batch [119883], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:26:07 TP0] Decode batch [119923], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:07 TP0] Decode batch [119963], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:26:08 TP0] Decode batch [120003], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:26:09 TP0] Decode batch [120043], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:26:10 TP0] Decode batch [120083], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:26:10] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:26:10 TP0] Prefill batch [120097], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:26:11 TP0] Decode batch [120124], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-25 16:26:12 TP0] Decode batch [120164], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:26:12 TP0] Decode batch [120204], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:26:13 TP0] Decode batch [120244], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:26:14 TP0] Decode batch [120284], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:26:15 TP0] Decode batch [120324], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:26:16 TP0] Decode batch [120364], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:26:17 TP0] Decode batch [120404], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:26:17 TP0] Decode batch [120444], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:26:18 TP0] Decode batch [120484], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:19 TP0] Decode batch [120524], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:26:20 TP0] Decode batch [120564], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:21 TP0] Decode batch [120604], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:26:21 TP0] Decode batch [120644], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:26:22 TP0] Decode batch [120684], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:26:23 TP0] Decode batch [120724], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:26:24 TP0] Decode batch [120764], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:26:25 TP0] Decode batch [120804], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:26:26 TP0] Decode batch [120844], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:26:26 TP0] Decode batch [120884], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:26:27] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:26:27 TP0] Prefill batch [120898], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:26:27 TP0] Decode batch [120925], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:26:28 TP0] Decode batch [120965], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:26:29 TP0] Decode batch [121005], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:26:30 TP0] Decode batch [121045], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:26:31 TP0] Decode batch [121085], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:26:31 TP0] Decode batch [121125], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:26:32 TP0] Decode batch [121165], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:26:33 TP0] Decode batch [121205], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:26:34 TP0] Decode batch [121245], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:26:35 TP0] Decode batch [121285], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:26:35 TP0] Decode batch [121325], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:26:36 TP0] Decode batch [121365], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:26:37 TP0] Decode batch [121405], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:26:38 TP0] Decode batch [121445], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:26:39 TP0] Decode batch [121485], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:40 TP0] Decode batch [121525], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:26:40 TP0] Decode batch [121565], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:41 TP0] Decode batch [121605], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:42 TP0] Decode batch [121645], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:26:43 TP0] Decode batch [121685], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:43] INFO:     127.0.0.1:47246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:26:43 TP0] Prefill batch [121699], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:26:44 TP0] Decode batch [121726], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:26:45 TP0] Decode batch [121766], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:26:45 TP0] Decode batch [121806], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:26:46 TP0] Decode batch [121846], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:26:47 TP0] Decode batch [121886], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:26:48 TP0] Decode batch [121926], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:26:49 TP0] Decode batch [121966], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:26:49 TP0] Decode batch [122006], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:26:50 TP0] Decode batch [122046], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:26:51 TP0] Decode batch [122086], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:26:52 TP0] Decode batch [122126], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:26:53 TP0] Decode batch [122166], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:26:54 TP0] Decode batch [122206], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:26:54 TP0] Decode batch [122246], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:26:55 TP0] Decode batch [122286], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:26:56 TP0] Decode batch [122326], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:26:57 TP0] Decode batch [122366], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:26:58 TP0] Decode batch [122406], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:26:58 TP0] Decode batch [122446], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:26:59 TP0] Decode batch [122486], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:27:00] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:27:00 TP0] Prefill batch [122500], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:27:00 TP0] Decode batch [122527], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:27:01 TP0] Decode batch [122567], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:27:02 TP0] Decode batch [122607], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:27:03 TP0] Decode batch [122647], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:27:03 TP0] Decode batch [122687], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:27:04 TP0] Decode batch [122727], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:27:05 TP0] Decode batch [122767], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:27:06 TP0] Decode batch [122807], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:27:07 TP0] Decode batch [122847], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:27:08 TP0] Decode batch [122887], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:27:08 TP0] Decode batch [122927], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:27:09 TP0] Decode batch [122967], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:27:10 TP0] Decode batch [123007], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:27:11 TP0] Decode batch [123047], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:27:12 TP0] Decode batch [123087], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:27:12 TP0] Decode batch [123127], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:27:13 TP0] Decode batch [123167], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:27:14 TP0] Decode batch [123207], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:27:15 TP0] Decode batch [123247], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:27:16 TP0] Decode batch [123287], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:27:16] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:27:16 TP0] Prefill batch [123301], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:27:17 TP0] Decode batch [123328], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-25 16:27:17 TP0] Decode batch [123368], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:27:18 TP0] Decode batch [123408], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:27:19 TP0] Decode batch [123448], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:20 TP0] Decode batch [123488], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:21 TP0] Decode batch [123528], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:27:22 TP0] Decode batch [123568], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:22 TP0] Decode batch [123608], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:23 TP0] Decode batch [123648], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:24 TP0] Decode batch [123688], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:25 TP0] Decode batch [123728], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:27:26 TP0] Decode batch [123768], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:26 TP0] Decode batch [123808], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:27:27 TP0] Decode batch [123848], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:27:28 TP0] Decode batch [123888], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:29 TP0] Decode batch [123928], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:30 TP0] Decode batch [123968], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:31 TP0] Decode batch [124008], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:27:31 TP0] Decode batch [124048], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:27:32 TP0] Decode batch [124088], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:32] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:27:32 TP0] Prefill batch [124102], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:27:33 TP0] Decode batch [124129], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-25 16:27:34 TP0] Decode batch [124169], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:27:35 TP0] Decode batch [124209], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:36 TP0] Decode batch [124249], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:36 TP0] Decode batch [124289], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:37 TP0] Decode batch [124329], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:38 TP0] Decode batch [124369], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:39 TP0] Decode batch [124409], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:40 TP0] Decode batch [124449], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:40 TP0] Decode batch [124489], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:41 TP0] Decode batch [124529], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:27:42 TP0] Decode batch [124569], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:43 TP0] Decode batch [124609], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:44 TP0] Decode batch [124649], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:45 TP0] Decode batch [124689], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:45 TP0] Decode batch [124729], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:27:46 TP0] Decode batch [124769], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:27:47 TP0] Decode batch [124809], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:27:48 TP0] Decode batch [124849], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:27:49 TP0] Decode batch [124889], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:27:49] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:27:49 TP0] Prefill batch [124903], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:27:50 TP0] Decode batch [124930], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.93, #queue-req: 0, 
[2025-10-25 16:27:50 TP0] Decode batch [124970], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:27:51 TP0] Decode batch [125010], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:27:52 TP0] Decode batch [125050], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:53 TP0] Decode batch [125090], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:27:54 TP0] Decode batch [125130], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:54 TP0] Decode batch [125170], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:55 TP0] Decode batch [125210], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:56 TP0] Decode batch [125250], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:57 TP0] Decode batch [125290], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:58 TP0] Decode batch [125330], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:27:59 TP0] Decode batch [125370], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:27:59 TP0] Decode batch [125410], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:28:00 TP0] Decode batch [125450], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:28:01 TP0] Decode batch [125490], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:28:02 TP0] Decode batch [125530], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:28:03 TP0] Decode batch [125570], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:28:03 TP0] Decode batch [125610], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:28:04 TP0] Decode batch [125650], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:28:05 TP0] Decode batch [125690], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:28:05] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:28:05 TP0] Prefill batch [125704], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:28:06 TP0] Decode batch [125731], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:28:07 TP0] Decode batch [125771], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:28:08 TP0] Decode batch [125811], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:28:08 TP0] Decode batch [125851], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:28:09 TP0] Decode batch [125891], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:28:10 TP0] Decode batch [125931], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:28:11 TP0] Decode batch [125971], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:28:12 TP0] Decode batch [126011], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:28:13 TP0] Decode batch [126051], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:28:13 TP0] Decode batch [126091], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:28:14 TP0] Decode batch [126131], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:28:15 TP0] Decode batch [126171], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:28:16 TP0] Decode batch [126211], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:28:17 TP0] Decode batch [126251], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:28:17 TP0] Decode batch [126291], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:28:18 TP0] Decode batch [126331], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:28:19 TP0] Decode batch [126371], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:28:20 TP0] Decode batch [126411], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:28:21 TP0] Decode batch [126451], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:28:22 TP0] Decode batch [126491], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:28:22] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:28:22 TP0] Prefill batch [126505], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:28:22 TP0] Decode batch [126532], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-25 16:28:23 TP0] Decode batch [126572], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:28:24 TP0] Decode batch [126612], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:28:25 TP0] Decode batch [126652], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:28:26 TP0] Decode batch [126692], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:28:27 TP0] Decode batch [126732], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:27 TP0] Decode batch [126772], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:28:28 TP0] Decode batch [126812], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:29 TP0] Decode batch [126852], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:30 TP0] Decode batch [126892], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:31 TP0] Decode batch [126932], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:32 TP0] Decode batch [126972], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:32 TP0] Decode batch [127012], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:28:33 TP0] Decode batch [127052], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:28:34 TP0] Decode batch [127092], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:28:35 TP0] Decode batch [127132], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:36 TP0] Decode batch [127172], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:36 TP0] Decode batch [127212], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:28:37 TP0] Decode batch [127252], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:28:38 TP0] Decode batch [127292], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:38] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:28:38 TP0] Prefill batch [127306], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:28:39 TP0] Decode batch [127333], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-25 16:28:40 TP0] Decode batch [127373], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:41 TP0] Decode batch [127413], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:28:41 TP0] Decode batch [127453], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:28:42 TP0] Decode batch [127493], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:28:43 TP0] Decode batch [127533], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:28:44 TP0] Decode batch [127573], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:28:45 TP0] Decode batch [127613], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:46 TP0] Decode batch [127653], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:28:46 TP0] Decode batch [127693], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:28:47 TP0] Decode batch [127733], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:28:48 TP0] Decode batch [127773], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:28:49 TP0] Decode batch [127813], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:28:50 TP0] Decode batch [127853], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:28:50 TP0] Decode batch [127893], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:28:51 TP0] Decode batch [127933], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:28:52 TP0] Decode batch [127973], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:28:53 TP0] Decode batch [128013], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:28:54 TP0] Decode batch [128053], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:28:55 TP0] Decode batch [128093], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:28:55] INFO:     127.0.0.1:41728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:28:55 TP0] Prefill batch [128107], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:28:55 TP0] Decode batch [128134], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:28:56 TP0] Decode batch [128174], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:28:57 TP0] Decode batch [128214], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:28:58 TP0] Decode batch [128254], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:28:59 TP0] Decode batch [128294], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:00 TP0] Decode batch [128334], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:00 TP0] Decode batch [128374], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:01 TP0] Decode batch [128414], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:02 TP0] Decode batch [128454], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:03 TP0] Decode batch [128494], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:04 TP0] Decode batch [128534], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:04 TP0] Decode batch [128574], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:05 TP0] Decode batch [128614], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:06 TP0] Decode batch [128654], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:07 TP0] Decode batch [128694], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:08 TP0] Decode batch [128734], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:09 TP0] Decode batch [128774], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:09 TP0] Decode batch [128814], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:10 TP0] Decode batch [128854], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:11 TP0] Decode batch [128894], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:11] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:29:11 TP0] Prefill batch [128908], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:29:12 TP0] Decode batch [128935], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-25 16:29:13 TP0] Decode batch [128975], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:14 TP0] Decode batch [129015], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:14 TP0] Decode batch [129055], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:15 TP0] Decode batch [129095], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:16 TP0] Decode batch [129135], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:17 TP0] Decode batch [129175], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:18 TP0] Decode batch [129215], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:19 TP0] Decode batch [129255], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:19 TP0] Decode batch [129295], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:20 TP0] Decode batch [129335], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:21 TP0] Decode batch [129375], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:22 TP0] Decode batch [129415], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:23 TP0] Decode batch [129455], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:29:23 TP0] Decode batch [129495], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:29:24 TP0] Decode batch [129535], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:29:25 TP0] Decode batch [129575], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:29:26 TP0] Decode batch [129615], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:29:27 TP0] Decode batch [129655], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:28 TP0] Decode batch [129695], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:29:28] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:29:28 TP0] Prefill batch [129709], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:29:28 TP0] Decode batch [129736], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-25 16:29:29 TP0] Decode batch [129776], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:30 TP0] Decode batch [129816], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:31 TP0] Decode batch [129856], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:29:32 TP0] Decode batch [129896], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:33 TP0] Decode batch [129936], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:33 TP0] Decode batch [129976], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:34 TP0] Decode batch [130016], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:35 TP0] Decode batch [130056], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:36 TP0] Decode batch [130096], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:37 TP0] Decode batch [130136], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:37 TP0] Decode batch [130176], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:38 TP0] Decode batch [130216], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:39 TP0] Decode batch [130256], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:40 TP0] Decode batch [130296], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:41 TP0] Decode batch [130336], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:42 TP0] Decode batch [130376], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:42 TP0] Decode batch [130416], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:43 TP0] Decode batch [130456], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:44 TP0] Decode batch [130496], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:44] INFO:     127.0.0.1:40488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:29:44 TP0] Prefill batch [130510], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:29:45 TP0] Decode batch [130537], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-25 16:29:46 TP0] Decode batch [130577], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:29:47 TP0] Decode batch [130617], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:29:47 TP0] Decode batch [130657], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:48 TP0] Decode batch [130697], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:29:49 TP0] Decode batch [130737], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:50 TP0] Decode batch [130777], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:29:51 TP0] Decode batch [130817], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:51 TP0] Decode batch [130857], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:52 TP0] Decode batch [130897], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:53 TP0] Decode batch [130937], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:54 TP0] Decode batch [130977], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:29:55 TP0] Decode batch [131017], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:56 TP0] Decode batch [131057], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:56 TP0] Decode batch [131097], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:29:57 TP0] Decode batch [131137], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:29:58 TP0] Decode batch [131177], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:29:59 TP0] Decode batch [131217], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:00 TP0] Decode batch [131257], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:00 TP0] Decode batch [131297], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:01] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:30:01 TP0] Prefill batch [131311], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:30:01 TP0] Decode batch [131338], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-25 16:30:02 TP0] Decode batch [131378], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:30:03 TP0] Decode batch [131418], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:04 TP0] Decode batch [131458], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:05 TP0] Decode batch [131498], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:30:05 TP0] Decode batch [131538], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:30:06 TP0] Decode batch [131578], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:30:07 TP0] Decode batch [131618], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:08 TP0] Decode batch [131658], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:30:09 TP0] Decode batch [131698], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:30:10 TP0] Decode batch [131738], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:30:10 TP0] Decode batch [131778], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:11 TP0] Decode batch [131818], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:30:12 TP0] Decode batch [131858], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:30:13 TP0] Decode batch [131898], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:30:14 TP0] Decode batch [131938], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:30:15 TP0] Decode batch [131978], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:30:15 TP0] Decode batch [132018], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:30:16 TP0] Decode batch [132058], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:30:17 TP0] Decode batch [132098], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:30:17] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:30:17 TP0] Prefill batch [132112], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:30:18 TP0] Decode batch [132139], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-25 16:30:19 TP0] Decode batch [132179], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:30:20 TP0] Decode batch [132219], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:30:20 TP0] Decode batch [132259], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:30:21 TP0] Decode batch [132299], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:30:22 TP0] Decode batch [132339], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:30:23 TP0] Decode batch [132379], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:30:24 TP0] Decode batch [132419], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:30:24 TP0] Decode batch [132459], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:30:25 TP0] Decode batch [132499], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:30:26 TP0] Decode batch [132539], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:30:27 TP0] Decode batch [132579], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:28 TP0] Decode batch [132619], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:30:29 TP0] Decode batch [132659], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:30:29 TP0] Decode batch [132699], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:30:30 TP0] Decode batch [132739], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:31 TP0] Decode batch [132779], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:30:32 TP0] Decode batch [132819], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:33 TP0] Decode batch [132859], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:30:33 TP0] Decode batch [132899], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:34] INFO:     127.0.0.1:44878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:30:34 TP0] Prefill batch [132913], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:30:34 TP0] Decode batch [132940], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-25 16:30:35 TP0] Decode batch [132980], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:30:36 TP0] Decode batch [133020], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:30:37 TP0] Decode batch [133060], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:30:38 TP0] Decode batch [133100], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:30:38 TP0] Decode batch [133140], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:39 TP0] Decode batch [133180], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:40 TP0] Decode batch [133220], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:30:41 TP0] Decode batch [133260], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:30:42 TP0] Decode batch [133300], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:30:43 TP0] Decode batch [133340], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:43 TP0] Decode batch [133380], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:44 TP0] Decode batch [133420], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:30:45 TP0] Decode batch [133460], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:30:46 TP0] Decode batch [133500], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:47 TP0] Decode batch [133540], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:47 TP0] Decode batch [133580], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:30:48 TP0] Decode batch [133620], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:30:49 TP0] Decode batch [133660], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:50 TP0] Decode batch [133700], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:30:50] INFO:     127.0.0.1:43978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:30:50 TP0] Prefill batch [133714], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:30:51 TP0] Decode batch [133741], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-25 16:30:52 TP0] Decode batch [133781], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:30:52 TP0] Decode batch [133821], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:30:53 TP0] Decode batch [133861], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:54 TP0] Decode batch [133901], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:55 TP0] Decode batch [133941], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:56 TP0] Decode batch [133981], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:30:57 TP0] Decode batch [134021], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:30:57 TP0] Decode batch [134061], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:30:58 TP0] Decode batch [134101], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:30:59 TP0] Decode batch [134141], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:31:00 TP0] Decode batch [134181], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:31:01 TP0] Decode batch [134221], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:31:01 TP0] Decode batch [134261], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:31:02 TP0] Decode batch [134301], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:31:03 TP0] Decode batch [134341], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:31:04 TP0] Decode batch [134381], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:31:05 TP0] Decode batch [134421], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:31:06 TP0] Decode batch [134461], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:31:06 TP0] Decode batch [134501], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:31:07] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:31:07 TP0] Prefill batch [134515], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:31:07 TP0] Decode batch [134542], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-25 16:31:08 TP0] Decode batch [134582], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-25 16:31:09 TP0] Decode batch [134622], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-25 16:31:10 TP0] Decode batch [134662], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:31:11 TP0] Decode batch [134702], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:31:11 TP0] Decode batch [134742], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:31:12 TP0] Decode batch [134782], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:31:13 TP0] Decode batch [134822], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:31:14 TP0] Decode batch [134862], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:31:15 TP0] Decode batch [134902], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:31:15 TP0] Decode batch [134942], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:31:16 TP0] Decode batch [134982], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:31:17 TP0] Decode batch [135022], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:31:18 TP0] Decode batch [135062], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:31:19 TP0] Decode batch [135102], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:31:20 TP0] Decode batch [135142], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:31:20 TP0] Decode batch [135182], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:31:21 TP0] Decode batch [135222], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:31:22 TP0] Decode batch [135262], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:31:23 TP0] Decode batch [135302], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:31:23] INFO:     127.0.0.1:42298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:31:23 TP0] Prefill batch [135316], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:31:24 TP0] Decode batch [135343], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-25 16:31:25 TP0] Decode batch [135383], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:31:25 TP0] Decode batch [135423], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:31:26 TP0] Decode batch [135463], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:31:27 TP0] Decode batch [135503], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:31:28 TP0] Decode batch [135543], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:31:29 TP0] Decode batch [135583], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:31:29 TP0] Decode batch [135623], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:31:30 TP0] Decode batch [135663], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:31:31 TP0] Decode batch [135703], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:31:32 TP0] Decode batch [135743], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:31:33 TP0] Decode batch [135783], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-25 16:31:34 TP0] Decode batch [135823], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:31:34 TP0] Decode batch [135863], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-25 16:31:35 TP0] Decode batch [135903], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:31:36 TP0] Decode batch [135943], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:31:37 TP0] Decode batch [135983], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-25 16:31:38 TP0] Decode batch [136023], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:31:39 TP0] Decode batch [136063], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:31:39 TP0] Decode batch [136103], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-25 16:31:40] INFO:     127.0.0.1:37326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:31:40 TP0] Prefill batch [136117], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:31:40 TP0] Decode batch [136144], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-25 16:31:41 TP0] Decode batch [136184], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:31:42 TP0] Decode batch [136224], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:31:43 TP0] Decode batch [136264], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:31:44 TP0] Decode batch [136304], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:31:44 TP0] Decode batch [136344], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:31:45 TP0] Decode batch [136384], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:31:46 TP0] Decode batch [136424], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:31:47 TP0] Decode batch [136464], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:31:48 TP0] Decode batch [136504], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:31:48 TP0] Decode batch [136544], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:31:49 TP0] Decode batch [136584], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:31:50 TP0] Decode batch [136624], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:31:51 TP0] Decode batch [136664], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:31:52 TP0] Decode batch [136704], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:31:53 TP0] Decode batch [136744], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:31:53 TP0] Decode batch [136784], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:31:54 TP0] Decode batch [136824], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.23, #queue-req: 0, 
[2025-10-25 16:31:55 TP0] Decode batch [136864], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:31:56 TP0] Decode batch [136904], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:31:56] INFO:     127.0.0.1:34944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:31:56 TP0] Prefill batch [136918], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:31:57 TP0] Decode batch [136945], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-25 16:31:58 TP0] Decode batch [136985], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:31:58 TP0] Decode batch [137025], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:31:59 TP0] Decode batch [137065], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:32:00 TP0] Decode batch [137105], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:32:01 TP0] Decode batch [137145], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:32:02 TP0] Decode batch [137185], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:02 TP0] Decode batch [137225], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:32:03 TP0] Decode batch [137265], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:04 TP0] Decode batch [137305], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:05 TP0] Decode batch [137345], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:06 TP0] Decode batch [137385], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:07 TP0] Decode batch [137425], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:32:07 TP0] Decode batch [137465], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:08 TP0] Decode batch [137505], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:09 TP0] Decode batch [137545], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:10 TP0] Decode batch [137585], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:32:11 TP0] Decode batch [137625], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:11 TP0] Decode batch [137665], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:12 TP0] Decode batch [137705], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:32:13] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:32:13 TP0] Prefill batch [137719], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:32:13 TP0] Decode batch [137746], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:32:14 TP0] Decode batch [137786], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:32:15 TP0] Decode batch [137826], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:32:16 TP0] Decode batch [137866], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:32:16 TP0] Decode batch [137906], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:32:17 TP0] Decode batch [137946], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:18 TP0] Decode batch [137986], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:32:19 TP0] Decode batch [138026], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:32:20 TP0] Decode batch [138066], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:32:21 TP0] Decode batch [138106], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:21 TP0] Decode batch [138146], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:22 TP0] Decode batch [138186], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:23 TP0] Decode batch [138226], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:32:24 TP0] Decode batch [138266], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:32:25 TP0] Decode batch [138306], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:26 TP0] Decode batch [138346], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:26 TP0] Decode batch [138386], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:27 TP0] Decode batch [138426], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:32:28 TP0] Decode batch [138466], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:32:29 TP0] Decode batch [138506], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:32:29] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:32:29 TP0] Prefill batch [138520], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:32:30 TP0] Decode batch [138547], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-25 16:32:31 TP0] Decode batch [138587], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:32:31 TP0] Decode batch [138627], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:32:32 TP0] Decode batch [138667], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:32:33 TP0] Decode batch [138707], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:32:34 TP0] Decode batch [138747], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:32:35 TP0] Decode batch [138787], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:32:35 TP0] Decode batch [138827], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:32:36 TP0] Decode batch [138867], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:32:37 TP0] Decode batch [138907], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:32:38 TP0] Decode batch [138947], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:32:39 TP0] Decode batch [138987], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:32:40 TP0] Decode batch [139027], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:32:40 TP0] Decode batch [139067], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:32:41 TP0] Decode batch [139107], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:32:42 TP0] Decode batch [139147], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:32:43 TP0] Decode batch [139187], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:32:44 TP0] Decode batch [139227], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:32:44 TP0] Decode batch [139267], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:32:45 TP0] Decode batch [139307], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:32:46] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:32:46 TP0] Prefill batch [139321], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:32:46 TP0] Decode batch [139348], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:32:47 TP0] Decode batch [139388], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:32:48 TP0] Decode batch [139428], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:32:49 TP0] Decode batch [139468], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:32:49 TP0] Decode batch [139508], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:32:50 TP0] Decode batch [139548], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:32:51 TP0] Decode batch [139588], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:32:52 TP0] Decode batch [139628], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:32:53 TP0] Decode batch [139668], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:32:54 TP0] Decode batch [139708], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:32:54 TP0] Decode batch [139748], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:32:55 TP0] Decode batch [139788], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:32:56 TP0] Decode batch [139828], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:32:57 TP0] Decode batch [139868], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:32:58 TP0] Decode batch [139908], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:32:58 TP0] Decode batch [139948], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:32:59 TP0] Decode batch [139988], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:33:00 TP0] Decode batch [140028], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:33:01 TP0] Decode batch [140068], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-25 16:33:02 TP0] Decode batch [140108], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-25 16:33:02] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:33:02 TP0] Prefill batch [140122], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:33:03 TP0] Decode batch [140149], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-25 16:33:03 TP0] Decode batch [140189], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:33:04 TP0] Decode batch [140229], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:33:05 TP0] Decode batch [140269], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:06 TP0] Decode batch [140309], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:33:07 TP0] Decode batch [140349], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:33:08 TP0] Decode batch [140389], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:08 TP0] Decode batch [140429], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:09 TP0] Decode batch [140469], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:33:10 TP0] Decode batch [140509], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:11 TP0] Decode batch [140549], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:33:12 TP0] Decode batch [140589], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:33:12 TP0] Decode batch [140629], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:13 TP0] Decode batch [140669], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:33:14 TP0] Decode batch [140709], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:33:15 TP0] Decode batch [140749], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:16 TP0] Decode batch [140789], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:17 TP0] Decode batch [140829], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:17 TP0] Decode batch [140869], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:18 TP0] Decode batch [140909], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:18] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:33:19 TP0] Prefill batch [140923], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:33:19 TP0] Decode batch [140950], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-25 16:33:20 TP0] Decode batch [140990], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:21 TP0] Decode batch [141030], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:33:22 TP0] Decode batch [141070], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:33:22 TP0] Decode batch [141110], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:33:23 TP0] Decode batch [141150], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:33:24 TP0] Decode batch [141190], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:33:25 TP0] Decode batch [141230], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:26 TP0] Decode batch [141270], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:26 TP0] Decode batch [141310], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:33:27 TP0] Decode batch [141350], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:33:28 TP0] Decode batch [141390], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:33:29 TP0] Decode batch [141430], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:30 TP0] Decode batch [141470], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:33:31 TP0] Decode batch [141510], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:33:31 TP0] Decode batch [141550], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:33:32 TP0] Decode batch [141590], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:33:33 TP0] Decode batch [141630], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:33:34 TP0] Decode batch [141670], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-25 16:33:35 TP0] Decode batch [141710], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-25 16:33:35] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:33:35 TP0] Prefill batch [141724], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:33:36 TP0] Decode batch [141751], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-25 16:33:36 TP0] Decode batch [141791], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:33:37 TP0] Decode batch [141831], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:33:38 TP0] Decode batch [141871], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:33:39 TP0] Decode batch [141911], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:40 TP0] Decode batch [141951], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:41 TP0] Decode batch [141991], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:33:41 TP0] Decode batch [142031], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:42 TP0] Decode batch [142071], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:43 TP0] Decode batch [142111], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:44 TP0] Decode batch [142151], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:45 TP0] Decode batch [142191], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:33:45 TP0] Decode batch [142231], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:46 TP0] Decode batch [142271], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:33:47 TP0] Decode batch [142311], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:33:48 TP0] Decode batch [142351], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:33:49 TP0] Decode batch [142391], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:33:50 TP0] Decode batch [142431], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:33:50 TP0] Decode batch [142471], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:33:51 TP0] Decode batch [142511], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:33:51] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:33:51 TP0] Prefill batch [142525], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:33:52 TP0] Decode batch [142552], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-25 16:33:53 TP0] Decode batch [142592], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-25 16:33:54 TP0] Decode batch [142632], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:33:55 TP0] Decode batch [142672], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:33:55 TP0] Decode batch [142712], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:33:56 TP0] Decode batch [142752], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:33:57 TP0] Decode batch [142792], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:33:58 TP0] Decode batch [142832], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:33:59 TP0] Decode batch [142872], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:33:59 TP0] Decode batch [142912], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:00 TP0] Decode batch [142952], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:01 TP0] Decode batch [142992], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:02 TP0] Decode batch [143032], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:03 TP0] Decode batch [143072], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:04 TP0] Decode batch [143112], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:04 TP0] Decode batch [143152], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:05 TP0] Decode batch [143192], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:06 TP0] Decode batch [143232], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:07 TP0] Decode batch [143272], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:08 TP0] Decode batch [143312], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:34:08] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:34:08 TP0] Prefill batch [143326], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:34:09 TP0] Decode batch [143353], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:34:09 TP0] Decode batch [143393], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:34:10 TP0] Decode batch [143433], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:34:11 TP0] Decode batch [143473], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:12 TP0] Decode batch [143513], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:13 TP0] Decode batch [143553], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:13 TP0] Decode batch [143593], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:14 TP0] Decode batch [143633], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:15 TP0] Decode batch [143673], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:16 TP0] Decode batch [143713], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:17 TP0] Decode batch [143753], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:18 TP0] Decode batch [143793], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:18 TP0] Decode batch [143833], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:19 TP0] Decode batch [143873], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:20 TP0] Decode batch [143913], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:21 TP0] Decode batch [143953], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:34:22 TP0] Decode batch [143993], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:22 TP0] Decode batch [144033], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:34:23 TP0] Decode batch [144073], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:34:24 TP0] Decode batch [144113], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:34:24] INFO:     127.0.0.1:38436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:34:24 TP0] Prefill batch [144127], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:34:25 TP0] Decode batch [144154], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-25 16:34:26 TP0] Decode batch [144194], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:34:27 TP0] Decode batch [144234], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:27 TP0] Decode batch [144274], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:34:28 TP0] Decode batch [144314], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:29 TP0] Decode batch [144354], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:30 TP0] Decode batch [144394], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:31 TP0] Decode batch [144434], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:32 TP0] Decode batch [144474], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:32 TP0] Decode batch [144514], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:34:33 TP0] Decode batch [144554], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:34 TP0] Decode batch [144594], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:34:35 TP0] Decode batch [144634], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:34:36 TP0] Decode batch [144674], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:34:36 TP0] Decode batch [144714], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:34:37 TP0] Decode batch [144754], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:34:38 TP0] Decode batch [144794], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:34:39 TP0] Decode batch [144834], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:34:40 TP0] Decode batch [144874], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:34:41 TP0] Decode batch [144914], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:34:41] INFO:     127.0.0.1:58536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:34:41 TP0] Prefill batch [144928], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:34:41 TP0] Decode batch [144955], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-25 16:34:42 TP0] Decode batch [144995], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-25 16:34:43 TP0] Decode batch [145035], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:34:44 TP0] Decode batch [145075], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:34:45 TP0] Decode batch [145115], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:34:46 TP0] Decode batch [145155], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-25 16:34:46 TP0] Decode batch [145195], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-25 16:34:47 TP0] Decode batch [145235], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:48 TP0] Decode batch [145275], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:34:49 TP0] Decode batch [145315], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:34:50 TP0] Decode batch [145355], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:34:50 TP0] Decode batch [145395], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:34:51 TP0] Decode batch [145435], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:34:52 TP0] Decode batch [145475], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:53 TP0] Decode batch [145515], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:34:54 TP0] Decode batch [145555], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:34:55 TP0] Decode batch [145595], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-25 16:34:55 TP0] Decode batch [145635], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-25 16:34:56 TP0] Decode batch [145675], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:34:57 TP0] Decode batch [145715], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:34:57] INFO:     127.0.0.1:40162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-25 16:34:57 TP0] Prefill batch [145729], #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-25 16:34:58 TP0] Decode batch [145756], #running-req: 1, #token: 3228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-25 16:34:59 TP0] Decode batch [145796], #running-req: 1, #token: 3268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:35:00 TP0] Decode batch [145836], #running-req: 1, #token: 3308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-25 16:35:00 TP0] Decode batch [145876], #running-req: 1, #token: 3348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-25 16:35:01 TP0] Decode batch [145916], #running-req: 1, #token: 3388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:35:02 TP0] Decode batch [145956], #running-req: 1, #token: 3428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:35:03 TP0] Decode batch [145996], #running-req: 1, #token: 3468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:35:04 TP0] Decode batch [146036], #running-req: 1, #token: 3508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:35:04 TP0] Decode batch [146076], #running-req: 1, #token: 3548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-25 16:35:05 TP0] Decode batch [146116], #running-req: 1, #token: 3588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:35:06 TP0] Decode batch [146156], #running-req: 1, #token: 3628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:35:07 TP0] Decode batch [146196], #running-req: 1, #token: 3668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:35:08 TP0] Decode batch [146236], #running-req: 1, #token: 3708, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:35:09 TP0] Decode batch [146276], #running-req: 1, #token: 3748, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:35:09 TP0] Decode batch [146316], #running-req: 1, #token: 3788, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:35:10 TP0] Decode batch [146356], #running-req: 1, #token: 3828, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-25 16:35:11 TP0] Decode batch [146396], #running-req: 1, #token: 3868, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-25 16:35:12 TP0] Decode batch [146436], #running-req: 1, #token: 3908, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:35:13 TP0] Decode batch [146476], #running-req: 1, #token: 3948, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:35:13 TP0] Decode batch [146516], #running-req: 1, #token: 3988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-25 16:35:14] INFO:     127.0.0.1:40084 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-25 16:35:21] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-25 16:35:25] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
