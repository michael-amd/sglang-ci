INFO 11-05 09:50:41 __init__.py:179] Automatically detected platform rocm.
WARNING 11-05 09:50:41 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 09:50:41] WARNING server_args.py:1165: Attention backend not explicitly specified. Use aiter backend by default.
[2025-11-05 09:50:41] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-05 09:50:42] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=608342575, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, decrypted_config_file=None, decrypted_draft_config_file=None)
[2025-11-05 09:50:42] Using default HuggingFace chat template with detected content format: string
INFO 11-05 09:50:51 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 09:50:51] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-05 09:50:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 09:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-05 09:50:52 __init__.py:179] Automatically detected platform rocm.
[2025-11-05 09:50:52 TP0] Process 283 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-11-05 09:50:52 TP7] Process 290 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
INFO 11-05 09:50:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 11-05 09:50:52 __init__.py:179] Automatically detected platform rocm.
INFO 11-05 09:50:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-05 09:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 11-05 09:50:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-11-05 09:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 09:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
INFO 11-05 09:50:52 __init__.py:179] Automatically detected platform rocm.
[2025-11-05 09:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 09:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 11-05 09:50:52 __init__.py:179] Automatically detected platform rocm.
[2025-11-05 09:50:52 TP7] Init torch distributed begin.
[2025-11-05 09:50:52 TP0] Init torch distributed begin.
[2025-11-05 09:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-05 09:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-05 09:50:52 TP4] Process 287 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-11-05 09:50:53 TP1] Process 284 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-11-05 09:50:53 TP2] Process 285 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-11-05 09:50:53 TP6] Process 289 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-11-05 09:50:53 TP5] Process 288 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-05 09:50:53 TP4] Init torch distributed begin.
[2025-11-05 09:50:53 TP1] Init torch distributed begin.
[2025-11-05 09:50:53 TP2] Init torch distributed begin.
[2025-11-05 09:50:53 TP3] Process 286 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-05 09:50:53 TP6] Init torch distributed begin.
[2025-11-05 09:50:53 TP5] Init torch distributed begin.
[2025-11-05 09:50:53 TP3] Init torch distributed begin.
[2025-11-05 09:50:53 TP0] sglang is using nccl==2.21.5
[2025-11-05 09:50:55 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-11-05 09:50:55 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-11-05 09:50:55 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-11-05 09:50:55 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-11-05 09:50:55 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-11-05 09:50:55 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-11-05 09:50:55 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-11-05 09:50:55 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-11-05 09:50:57 TP1] Load weight begin. avail mem=187.19 GB
[2025-11-05 09:50:57 TP0] Load weight begin. avail mem=187.61 GB
[2025-11-05 09:50:57 TP0] Detected fp8 checkpoint.
[2025-11-05 09:50:57 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-11-05 09:50:57 TP5] Load weight begin. avail mem=187.33 GB
[2025-11-05 09:50:57 TP2] Load weight begin. avail mem=187.19 GB
[2025-11-05 09:50:57 TP3] Load weight begin. avail mem=187.20 GB
[2025-11-05 09:50:57 TP6] Load weight begin. avail mem=187.31 GB
[2025-11-05 09:50:57 TP4] Load weight begin. avail mem=187.25 GB
[2025-11-05 09:50:57 TP7] Load weight begin. avail mem=187.32 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<01:17,  2.09it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:57,  2.80it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:48,  3.32it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:01<00:43,  3.61it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:01<00:43,  3.62it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:36,  4.35it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:37,  4.11it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:02<00:35,  4.31it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:02<00:55,  2.75it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:03<00:38,  3.95it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:03<00:33,  4.46it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:03<00:34,  4.39it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:03<00:29,  5.04it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:03<00:16,  8.87it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:03<00:12, 11.75it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:03<00:09, 14.23it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:04<00:08, 16.33it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:04<00:07, 18.76it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:05<00:15,  8.57it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:05<00:13,  9.66it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:05<00:10, 11.78it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:05<00:09, 12.78it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:05<00:08, 14.01it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:05<00:08, 14.86it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:05<00:06, 17.53it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:06<00:08, 13.94it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:06<00:07, 14.97it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:06<00:06, 17.04it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:06<00:06, 15.27it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:06<00:06, 16.13it/s]
Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:06<00:06, 16.82it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:07<00:12,  7.81it/s]
Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:07<00:10,  9.05it/s]
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:07<00:07, 11.87it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:07<00:07, 13.13it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:07<00:07, 12.08it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:08<00:07, 11.64it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:08<00:05, 14.61it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:08<00:05, 14.58it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:08<00:05, 15.74it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:08<00:04, 15.99it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:08<00:05, 12.79it/s]
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:09<00:04, 15.32it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:09<00:04, 16.75it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:09<00:03, 17.00it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:09<00:03, 17.24it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:09<00:03, 17.51it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:09<00:03, 18.95it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:10<00:09,  6.42it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:10<00:07,  7.79it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:10<00:05,  9.21it/s]
Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:10<00:04, 12.24it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:11<00:03, 13.41it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:11<00:04, 11.12it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:11<00:03, 13.85it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:11<00:02, 15.65it/s]
Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:11<00:02, 17.61it/s]
Loading safetensors checkpoint shards:  79% Completed | 128/163 [00:12<00:02, 14.06it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:12<00:02, 12.89it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:12<00:02, 14.74it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:12<00:01, 16.97it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:12<00:01, 17.36it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:12<00:01, 12.65it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:12<00:01, 13.55it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:13<00:01, 15.96it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:13<00:00, 16.58it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:13<00:00, 17.36it/s]
Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:14<00:01,  6.15it/s]
Loading safetensors checkpoint shards:  94% Completed | 154/163 [00:14<00:01,  7.41it/s]
Loading safetensors checkpoint shards:  96% Completed | 156/163 [00:14<00:00,  8.82it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:14<00:00, 11.34it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:14<00:00, 12.69it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00, 12.21it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00, 10.87it/s]

[2025-11-05 09:51:46 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-05 09:51:46 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-05 09:51:46 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.05 GB, mem usage=79.56 GB.
[2025-11-05 09:51:47 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-11-05 09:51:51 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.69 GB, mem usage=79.56 GB.
[2025-11-05 09:51:51 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-11-05 09:51:51 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.76 GB, mem usage=79.56 GB.
[2025-11-05 09:51:52 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-11-05 09:51:52 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-05 09:51:52 TP0] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 09:51:52 TP7] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 09:51:52 TP0] Memory pool end. avail mem=43.78 GB
[2025-11-05 09:51:52 TP7] Memory pool end. avail mem=43.50 GB
[2025-11-05 09:51:52 TP1] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 09:51:52 TP5] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 09:51:52 TP1] Memory pool end. avail mem=43.37 GB
[2025-11-05 09:51:52 TP5] Memory pool end. avail mem=43.51 GB
[2025-11-05 09:51:52 TP6] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 09:51:52 TP4] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 09:51:52 TP6] Memory pool end. avail mem=43.49 GB
[2025-11-05 09:51:52 TP4] Memory pool end. avail mem=43.42 GB
[2025-11-05 09:51:52 TP2] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 09:51:52 TP2] Memory pool end. avail mem=43.36 GB
[2025-11-05 09:51:52 TP3] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-05 09:51:52 TP3] Memory pool end. avail mem=43.37 GB
[2025-11-05 09:51:54 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.58 GB
[2025-11-05 09:51:54 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-11-05 09:51:54 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-11-05 09:51:54 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-11-05 09:51:54 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.17 GB
[2025-11-05 09:51:54 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-11-05 09:51:54 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.30 GB
[2025-11-05 09:51:54 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-11-05 09:51:54 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.22 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.94 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 09:51:57 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 09:51:57 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 09:51:57 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 09:51:57 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 09:51:57 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:57 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:57 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:57 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:57 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:57 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 09:51:57 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 09:51:57 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:57 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:57 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-05 09:51:57 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:57 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 09:51:58 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 09:51:58 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 09:51:58 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 09:51:58 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 09:51:58 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 09:51:58 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 09:51:59 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:51:59 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:51:59 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:51:59 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:51:59 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:51:59 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:51:59 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:51:59 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:51:59 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-05 09:51:59 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:00 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:00 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:00 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:00 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:00 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:00 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.94 GB):   2%|         | 1/52 [00:05<05:05,  5.99s/it]Capturing batches (bs=496 avail_mem=42.27 GB):   2%|         | 1/52 [00:05<05:05,  5.99s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.27 GB):   4%|         | 2/52 [00:07<02:59,  3.60s/it]Capturing batches (bs=480 avail_mem=42.25 GB):   4%|         | 2/52 [00:07<02:59,  3.60s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:02 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.25 GB):   6%|         | 3/52 [00:08<01:45,  2.15s/it]Capturing batches (bs=464 avail_mem=42.25 GB):   6%|         | 3/52 [00:08<01:45,  2.15s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.25 GB):   8%|         | 4/52 [00:08<01:10,  1.46s/it]Capturing batches (bs=448 avail_mem=42.24 GB):   8%|         | 4/52 [00:08<01:10,  1.46s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.24 GB):  10%|         | 5/52 [00:09<00:50,  1.08s/it]Capturing batches (bs=432 avail_mem=42.24 GB):  10%|         | 5/52 [00:09<00:50,  1.08s/it][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:03 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.24 GB):  12%|        | 6/52 [00:09<00:39,  1.17it/s]Capturing batches (bs=416 avail_mem=42.23 GB):  12%|        | 6/52 [00:09<00:39,  1.17it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.23 GB):  13%|        | 7/52 [00:09<00:32,  1.40it/s]Capturing batches (bs=400 avail_mem=42.23 GB):  13%|        | 7/52 [00:09<00:32,  1.40it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:04 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.23 GB):  15%|        | 8/52 [00:10<00:27,  1.62it/s]Capturing batches (bs=384 avail_mem=42.22 GB):  15%|        | 8/52 [00:10<00:27,  1.62it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.22 GB):  17%|        | 9/52 [00:10<00:21,  1.96it/s]Capturing batches (bs=368 avail_mem=42.22 GB):  17%|        | 9/52 [00:10<00:21,  1.96it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.22 GB):  19%|        | 10/52 [00:11<00:20,  2.08it/s]Capturing batches (bs=352 avail_mem=42.22 GB):  19%|        | 10/52 [00:11<00:20,  2.08it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:05 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.22 GB):  21%|        | 11/52 [00:11<00:18,  2.16it/s]Capturing batches (bs=336 avail_mem=42.21 GB):  21%|        | 11/52 [00:11<00:18,  2.16it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.21 GB):  23%|       | 12/52 [00:11<00:17,  2.22it/s]Capturing batches (bs=320 avail_mem=42.21 GB):  23%|       | 12/52 [00:11<00:17,  2.22it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.21 GB):  25%|       | 13/52 [00:12<00:15,  2.53it/s]Capturing batches (bs=304 avail_mem=42.20 GB):  25%|       | 13/52 [00:12<00:15,  2.53it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:06 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.20 GB):  27%|       | 14/52 [00:12<00:15,  2.47it/s]Capturing batches (bs=288 avail_mem=42.20 GB):  27%|       | 14/52 [00:12<00:15,  2.47it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.20 GB):  29%|       | 15/52 [00:12<00:13,  2.75it/s]Capturing batches (bs=272 avail_mem=42.20 GB):  29%|       | 15/52 [00:12<00:13,  2.75it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:07 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.20 GB):  31%|       | 16/52 [00:13<00:13,  2.62it/s]Capturing batches (bs=256 avail_mem=42.19 GB):  31%|       | 16/52 [00:13<00:13,  2.62it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:07 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:07 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:07 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:07 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:07 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:07 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:07 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:07 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:08 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:08 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:08 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:08 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:08 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:08 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:08 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:08 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:08 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.19 GB):  33%|      | 17/52 [00:13<00:13,  2.54it/s]Capturing batches (bs=248 avail_mem=42.18 GB):  33%|      | 17/52 [00:13<00:13,  2.54it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.18 GB):  35%|      | 18/52 [00:14<00:13,  2.50it/s]Capturing batches (bs=240 avail_mem=42.18 GB):  35%|      | 18/52 [00:14<00:13,  2.50it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:08 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.18 GB):  37%|      | 19/52 [00:14<00:13,  2.46it/s]Capturing batches (bs=232 avail_mem=42.17 GB):  37%|      | 19/52 [00:14<00:13,  2.46it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.17 GB):  38%|      | 20/52 [00:15<00:13,  2.44it/s]Capturing batches (bs=224 avail_mem=42.17 GB):  38%|      | 20/52 [00:15<00:13,  2.44it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:09 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.17 GB):  40%|      | 21/52 [00:15<00:12,  2.42it/s]Capturing batches (bs=216 avail_mem=42.16 GB):  40%|      | 21/52 [00:15<00:12,  2.42it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.16 GB):  42%|     | 22/52 [00:15<00:12,  2.41it/s]Capturing batches (bs=208 avail_mem=42.16 GB):  42%|     | 22/52 [00:15<00:12,  2.41it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.16 GB):  44%|     | 23/52 [00:16<00:10,  2.69it/s]Capturing batches (bs=200 avail_mem=42.15 GB):  44%|     | 23/52 [00:16<00:10,  2.69it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:10 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.15 GB):  46%|     | 24/52 [00:16<00:10,  2.60it/s]Capturing batches (bs=192 avail_mem=42.15 GB):  46%|     | 24/52 [00:16<00:10,  2.60it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.15 GB):  48%|     | 25/52 [00:16<00:09,  2.85it/s]Capturing batches (bs=184 avail_mem=42.15 GB):  48%|     | 25/52 [00:16<00:09,  2.85it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.15 GB):  50%|     | 26/52 [00:17<00:08,  3.06it/s]Capturing batches (bs=176 avail_mem=42.14 GB):  50%|     | 26/52 [00:17<00:08,  3.06it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:11 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.14 GB):  52%|    | 27/52 [00:17<00:07,  3.22it/s]Capturing batches (bs=168 avail_mem=42.14 GB):  52%|    | 27/52 [00:17<00:07,  3.22it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.14 GB):  54%|    | 28/52 [00:17<00:08,  2.91it/s]Capturing batches (bs=160 avail_mem=42.14 GB):  54%|    | 28/52 [00:17<00:08,  2.91it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.14 GB):  56%|    | 29/52 [00:18<00:07,  3.12it/s]Capturing batches (bs=152 avail_mem=42.13 GB):  56%|    | 29/52 [00:18<00:07,  3.12it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:12 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.13 GB):  58%|    | 30/52 [00:18<00:07,  2.86it/s]Capturing batches (bs=144 avail_mem=42.13 GB):  58%|    | 30/52 [00:18<00:07,  2.86it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.13 GB):  60%|    | 31/52 [00:18<00:06,  3.07it/s]Capturing batches (bs=136 avail_mem=42.12 GB):  60%|    | 31/52 [00:18<00:06,  3.07it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.12 GB):  62%|   | 32/52 [00:18<00:06,  3.21it/s]Capturing batches (bs=128 avail_mem=42.12 GB):  62%|   | 32/52 [00:18<00:06,  3.21it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:13 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:13 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:13 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:13 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:13 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:13 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:13 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:13 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:13 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:13 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:13 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:13 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:13 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:13 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:13 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:13 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:13 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:13 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:13 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.12 GB):  63%|   | 33/52 [00:19<00:05,  3.32it/s]Capturing batches (bs=120 avail_mem=42.12 GB):  63%|   | 33/52 [00:19<00:05,  3.32it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.12 GB):  65%|   | 34/52 [00:19<00:06,  2.97it/s]Capturing batches (bs=112 avail_mem=42.12 GB):  65%|   | 34/52 [00:19<00:06,  2.97it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.12 GB):  67%|   | 35/52 [00:19<00:05,  3.16it/s]Capturing batches (bs=104 avail_mem=42.11 GB):  67%|   | 35/52 [00:19<00:05,  3.16it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:14 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.11 GB):  69%|   | 36/52 [00:20<00:05,  2.87it/s]Capturing batches (bs=96 avail_mem=42.11 GB):  69%|   | 36/52 [00:20<00:05,  2.87it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.11 GB):  71%|   | 37/52 [00:20<00:04,  3.07it/s]Capturing batches (bs=88 avail_mem=42.10 GB):  71%|   | 37/52 [00:20<00:04,  3.07it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.10 GB):  73%|  | 38/52 [00:21<00:04,  2.81it/s]Capturing batches (bs=80 avail_mem=42.10 GB):  73%|  | 38/52 [00:21<00:04,  2.81it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:15 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.10 GB):  75%|  | 39/52 [00:21<00:04,  3.03it/s]Capturing batches (bs=72 avail_mem=42.10 GB):  75%|  | 39/52 [00:21<00:04,  3.03it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.10 GB):  77%|  | 40/52 [00:21<00:04,  2.80it/s]Capturing batches (bs=64 avail_mem=42.09 GB):  77%|  | 40/52 [00:21<00:04,  2.80it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:16 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:16 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:16 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:16 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:16 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:16 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:16 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 09:52:16 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:16 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:16 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:16 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:16 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:16 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:16 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:16 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 09:52:16 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.09 GB):  79%|  | 41/52 [00:22<00:03,  3.02it/s]Capturing batches (bs=56 avail_mem=42.08 GB):  79%|  | 41/52 [00:22<00:03,  3.02it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:16 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.08 GB):  81%|  | 42/52 [00:22<00:03,  2.80it/s]Capturing batches (bs=48 avail_mem=42.08 GB):  81%|  | 42/52 [00:22<00:03,  2.80it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.08 GB):  83%| | 43/52 [00:22<00:02,  3.01it/s]Capturing batches (bs=40 avail_mem=42.07 GB):  83%| | 43/52 [00:22<00:02,  3.01it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:52:17 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.07 GB):  85%| | 44/52 [00:23<00:02,  2.79it/s]Capturing batches (bs=32 avail_mem=42.07 GB):  85%| | 44/52 [00:23<00:02,  2.79it/s][rank4]:W1105 09:52:23.350000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1105 09:52:23.434000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:52:23.453000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1105 09:52:23.541000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:52:23.565000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1105 09:52:23.676000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:52:23.717000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1105 09:52:23.756000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1105 09:52:23.801000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:52:23.838000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:52:23.879000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1105 09:52:23.933000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:52:23.959000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:52:23.962000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:52:23.970000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1105 09:52:24.043000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1105 09:52:24.095000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1105 09:52:24.176000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:52:24.209000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1105 09:52:24.293000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:52:24.428000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1105 09:52:24.726000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1105 09:52:24.811000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:52:24.945000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1105 09:52:25.281000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:52:25.385000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:52:25.637000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:52:25.673000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:52:25.800000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:52:25.888000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:52:26.148000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:52:26.642000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:52:27.104000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:52:27.179000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:52:27.200000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:52:27.275000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:52:27.277000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:27 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1105 09:52:27.373000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:52:27.387000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:27 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1105 09:52:27.462000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:52:27.489000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:52:27.559000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:52:27.560000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:52:27.564000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:27 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:52:27.622000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:52:27.635000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:52:27.662000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:52:27.697000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:52:27.706000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:27 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1105 09:52:27.733000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:52:27.780000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:27 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:52:27.796000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:52:27.809000 286 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:27 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:52:27.878000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:52:27.907000 283 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:27 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:52:28.087000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:52:28.100000 287 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank7]:W1105 09:52:28.168000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:52:28.199000 284 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank7]:W1105 09:52:28.275000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:52:28.289000 289 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:52:28 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:52:28.361000 288 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank2]:W1105 09:52:28.471000 285 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank7]:W1105 09:52:28.846000 290 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_15 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_5 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2509 seconds and 0.5269 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_5 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_23 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_1 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2272 seconds and 0.5069 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_3 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0065 ms 98.2% 
  triton_bmm_7 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_13 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_9 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_0 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_2 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2137 seconds and 0.4721 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0067 ms 100.0% 
  triton_bmm_15 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_6 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_2 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_3 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2547 seconds and 0.4750 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_15 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_5 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0064 ms 98.1% 
  triton_bmm_19 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2923 seconds and 0.5541 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_5 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_3 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_9 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_2 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2421 seconds and 0.4987 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_3 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 100.0% 
  triton_bmm_13 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_1 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_11 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2640 seconds and 0.4434 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_7 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_13 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0065 ms 98.1% 
  triton_bmm_1 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_9 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2789 seconds and 0.4657 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1105 09:52:39.372000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:52:39.464000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:52:39.618000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:52:39.700000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:52:39.901000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:52:39.912000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:52:39.933000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:52:39.968000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:52:40.158000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:52:40.250000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:52:40.289000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:52:40.424000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:52:40.479000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:52:40.677000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:52:41.192000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:52:41.269000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1105 09:52:53.605000 283 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1105 09:52:55.285000 286 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1105 09:52:55.834000 285 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank1]:W1105 09:52:56.014000 284 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1105 09:53:01.529000 287 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1105 09:53:01.753000 288 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank7]:W1105 09:53:02.387000 290 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1105 09:53:04.532000 289 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 95.6% 
  triton_bmm_41 0.0076 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0081 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0087 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0087 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0091 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0091 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0092 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0098 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 11.0372 seconds and 0.5223 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0074 ms 94.0% 
  triton_bmm_41 0.0076 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0081 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0086 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0088 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0090 ms 77.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0091 ms 76.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0091 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0097 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.5138 seconds and 0.5372 seconds precompiling for 27 choices
[rank0]:W1105 09:53:06.906000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:06.988000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:07.095000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:07 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1105 09:53:07.892000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:07.973000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:08.076000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0070 ms 98.3% 
  triton_bmm_41 0.0075 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0080 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0087 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0088 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0089 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0090 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0094 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.9925 seconds and 0.5211 seconds precompiling for 27 choices
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:08 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0070 ms 96.0% 
  triton_bmm_41 0.0074 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0083 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0089 ms 75.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0089 ms 75.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0091 ms 74.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0094 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.9221 seconds and 0.5155 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 93.4% 
  triton_bmm_41 0.0077 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_33 0.0086 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0086 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0089 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0091 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0092 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0097 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 11.8694 seconds and 0.5240 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 95.6% 
  triton_bmm_41 0.0077 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0080 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0087 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0087 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0092 ms 75.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0092 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0095 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0096 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 13.0677 seconds and 0.5465 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0069 ms 97.7% 
  triton_bmm_41 0.0077 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0078 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_33 0.0085 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0091 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0092 ms 73.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0095 ms 71.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_32 0.0096 ms 70.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.0094 seconds and 0.5261 seconds precompiling for 27 choices
[rank2]:W1105 09:53:09.779000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:09.787000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:09.861000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:09.863000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:09.946000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:09.960000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:09.961000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:10 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:10 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:53:10.023000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:10.124000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:10 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:53:10.354000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:10.430000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:10.529000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:10 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1105 09:53:10.897000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:10.992000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0072 ms 95.6% 
  triton_bmm_41 0.0074 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0080 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0086 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0087 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0087 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0088 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0094 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.9023 seconds and 0.5269 seconds precompiling for 27 choices
[rank3]:W1105 09:53:11.098000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:11 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1105 09:53:12.161000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:12.238000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:12.337000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:12 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:12 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:12 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1105 09:53:13.622000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:13.696000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:13.796000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:13 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1105 09:53:13.881000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:13.955000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:14.056000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:14 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:14 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:14 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:14 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:15 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:15 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:53:15.815000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:15.888000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:15.961000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:15.987000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:16.036000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:16 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1105 09:53:16.139000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:16 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:53:16.271000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:16.348000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:16.442000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:16.450000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:16 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:53:16.517000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:16.619000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:16 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1105 09:53:16.843000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:16.916000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:16 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1105 09:53:17.016000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:17 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1105 09:53:17.290000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:17.350000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:17.525000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:17.582000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:17.758000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:17.814000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:18 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:18 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1105 09:53:18.222000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:18.296000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:18.398000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:18 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:53:19.210000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:19.274000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:19.275000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:19.348000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:19.365000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:19.386000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:19.445000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:19.448000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:19.460000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:19 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:53:19.514000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:19.558000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:19.597000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:19 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:53:19.687000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:19.750000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1105 09:53:19.814000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:19.830000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:20 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:53:20.052000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:20 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:20 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1105 09:53:20.263000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:20.292000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:20.507000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:20 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1105 09:53:20.742000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:21 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:53:21.185000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:21.258000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:21.321000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:21.355000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:21.395000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:21.399000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:21 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:53:21.476000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:21.495000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:21 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:53:21.578000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:21 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1105 09:53:21.631000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1105 09:53:21.872000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:21.913000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:21.991000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:22.098000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:22.120000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:22 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1105 09:53:22.364000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:22.439000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:22 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1105 09:53:22.547000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:22 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1105 09:53:23.918000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:23.992000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:24.089000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:53:24 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0101 ms 100.0% 
  triton_mm_55 0.0321 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0454 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0616 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0681 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0686 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0714 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5441 seconds and 0.3974 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0102 ms 100.0% 
  triton_mm_55 0.0299 ms 34.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0396 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0454 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0491 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0579 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0674 ms 15.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0681 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0686 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4790 seconds and 0.3959 seconds precompiling for 27 choices
[rank1]:W1105 09:53:27.246000 284 torch/_dynamo/variables/builtin.py:1091] [70/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2970>
[rank1]:W1105 09:53:27.279000 284 torch/_dynamo/variables/builtin.py:1091] [71/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2af0>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:53:27 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1105 09:53:27.448000 283 torch/_dynamo/variables/builtin.py:1091] [70/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e68e0>
[rank0]:W1105 09:53:27.480000 283 torch/_dynamo/variables/builtin.py:1091] [71/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e6220>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:53:27 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0101 ms 100.0% 
  triton_mm_55 0.0338 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0398 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0454 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0569 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0687 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0687 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0694 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0718 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5706 seconds and 0.3896 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_55 0.0295 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0451 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0487 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0578 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0677 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0685 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0685 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0704 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4738 seconds and 0.3841 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0289 ms 32.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0397 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0451 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0579 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0681 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0687 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0687 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5780 seconds and 0.3865 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0305 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0414 ms 22.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0452 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0521 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0639 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_74 0.0691 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0711 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_75 0.0714 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5878 seconds and 0.3803 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0100 ms 100.0% 
  triton_mm_55 0.0334 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0396 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0613 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0684 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0688 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0714 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5598 seconds and 0.3881 seconds precompiling for 27 choices
[rank2]:W1105 09:53:29.199000 285 torch/_dynamo/variables/builtin.py:1091] [70/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f69e30>
[rank1]:W1105 09:53:29.230000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:29.231000 285 torch/_dynamo/variables/builtin.py:1091] [71/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f6a5b0>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:53:29 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1105 09:53:29.385000 287 torch/_dynamo/variables/builtin.py:1091] [70/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a550>
[rank4]:W1105 09:53:29.417000 287 torch/_dynamo/variables/builtin.py:1091] [71/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a8e0>
[rank0]:W1105 09:53:29.445000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:29.446000 288 torch/_dynamo/variables/builtin.py:1091] [70/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c61f0>
[rank1]:W1105 09:53:29.466000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1105 09:53:29.478000 288 torch/_dynamo/variables/builtin.py:1091] [71/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c6490>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:53:29 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:53:29 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1105 09:53:29.695000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:29.711000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1105 09:53:29.934000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:29.947000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:29.947000 290 torch/_dynamo/variables/builtin.py:1091] [70/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed1d40>
[rank7]:W1105 09:53:29.979000 290 torch/_dynamo/variables/builtin.py:1091] [71/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed2160>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:53:30 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1105 09:53:30.170000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:30.183000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1105 09:53:30.384000 286 torch/_dynamo/variables/builtin.py:1091] [70/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31ebb0>
[rank3]:W1105 09:53:30.416000 286 torch/_dynamo/variables/builtin.py:1091] [71/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31efd0>
[rank1]:W1105 09:53:30.423000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:30.467000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_55 0.0293 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0414 ms 23.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0451 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0495 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0591 ms 16.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0687 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0688 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0704 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0714 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5994 seconds and 0.3788 seconds precompiling for 27 choices
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:53:30 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1105 09:53:30.707000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:30.715000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:30.743000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:30.943000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:30.955000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:30.974000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:31.182000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:31.191000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:31.211000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:31.398000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:31.422000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:31.428000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:31.447000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:31.452000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:31.475000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:31.634000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:31.659000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:31.665000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:31.690000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:31.711000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:31.735000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:31.870000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:31.899000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:31.904000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:31.920000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:31.927000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:31.947000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:31.954000 289 torch/_dynamo/variables/builtin.py:1091] [70/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8733f4f120>
[rank2]:W1105 09:53:31.971000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:31.986000 289 torch/_dynamo/variables/builtin.py:1091] [71/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8734f10ae0>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:53:32 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1105 09:53:32.110000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1105 09:53:32.134000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:32.140000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:32.155000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:32.166000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:32.183000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:32.203000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1105 09:53:32.346000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:32.370000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:32.376000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:32.387000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:32.407000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:32.435000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:32.475000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:32.587000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:32.606000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:32.610000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:32.618000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:32.647000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:32.666000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:32.712000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:32.846000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:32.850000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:32.855000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:32.883000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:32.902000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:32.945000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:32.951000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:33.086000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:33.090000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:33.123000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:33.138000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:33.146000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:33.183000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:33.189000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:33.326000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:33.330000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:33.363000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:33.374000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:33.382000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:33.423000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:33.429000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:33.503000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:33.574000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:33.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:33.599000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:33.610000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:33.618000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:33.659000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:33.667000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:33.739000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:33.815000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:33.838000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:33.847000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:33.855000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:33.879000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:33.895000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:33.903000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:33.978000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:34.074000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:34.086000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:34.095000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:34.115000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:34.120000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:34.130000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:34.138000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:34.216000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:34.311000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:34.322000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:34.330000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:34.351000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:34.357000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:34.368000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:34.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:34.515000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:34.551000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:34.558000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:34.566000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:34.586000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:34.593000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:34.603000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:34.611000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:34.754000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:34.786000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:34.795000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:34.803000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:34.822000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:34.830000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:34.840000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:34.847000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:34.994000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:35.022000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:35.043000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:35.058000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:35.070000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:35.080000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:35.085000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:35.091000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:35.234000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:35.258000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:35.279000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:35.294000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:35.306000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:35.318000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:35.326000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:35.331000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:35.474000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:35.494000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:35.515000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:35.530000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:35.542000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:35.556000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:35.562000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:35.568000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:35.714000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:35.734000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:35.751000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:35.767000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:35.778000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:35.803000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:35.809000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:35.855000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:35.958000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:35.974000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:35.990000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:36.002000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:36.014000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:36.039000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:36.047000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:36.095000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:36.198000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:36.227000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:36.239000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:36.250000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:36.275000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:36.281000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:36.335000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:36.351000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:36.442000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:36.479000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:36.490000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:36.516000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:36.522000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:36.528000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:36.575000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:36.590000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:36.683000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:36.718000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:36.730000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:36.750000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:36.763000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:36.768000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:36.812000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:36.831000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:36.923000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:36.958000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:36.969000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:36.986000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:37.003000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:37.008000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:37.047000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:37.067000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:37.163000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:37.198000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:37.209000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:37.222000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:37.240000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:37.246000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:37.283000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:37.303000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:37.402000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:37.439000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:37.463000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:37.478000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:37.487000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:37.518000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:37.524000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:37.539000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:37.642000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:37.702000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:37.719000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:37.726000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:37.746000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:37.762000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:37.769000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:37.779000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:37.942000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:37.947000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:37.959000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:37.966000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:37.982000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:38.001000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:38.008000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:38.019000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:38.182000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:38.187000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:38.195000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:38.202000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:38.222000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:38.243000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:38.247000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:38.255000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:38.423000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:38.428000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:38.433000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:38.439000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:38.463000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:38.479000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:38.491000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:38.496000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:38.670000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:38.674000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:38.680000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:38.702000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:38.720000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:38.726000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:38.734000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:38.739000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:38.915000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:38.920000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:38.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:38.942000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:38.959000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:38.975000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:38.980000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:38.986000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:39.155000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:39.160000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:39.166000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:39.178000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:39.195000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:39.212000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:39.219000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:39.225000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:39.399000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:39.403000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:39.409000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:39.415000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:39.450000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:39.460000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:39.466000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:39.503000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:39.638000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:39.646000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:39.652000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:39.658000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:39.690000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:39.699000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:39.706000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:39.744000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:39.892000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:39.896000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:39.901000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:39.934000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:39.943000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:39.951000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:39.988000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:40.000000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:40.135000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:40.142000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:40.174000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:40.185000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:40.191000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:40.202000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:40.227000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:40.239000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:40.374000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:40.383000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:40.415000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:40.426000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:40.432000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:40.443000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:40.463000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:40.475000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:40.614000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:40.623000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:40.651000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:40.666000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:40.675000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:40.683000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:40.703000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:40.714000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:40.859000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:40.863000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:40.886000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:40.902000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:40.919000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:40.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:40.943000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:40.955000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:41.102000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:41.107000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:41.126000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:41.141000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:41.160000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:41.166000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:41.179000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:41.191000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:41.342000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:41.348000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:41.366000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:41.400000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:41.407000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:41.416000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:41.428000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:41.458000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:41.610000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:41.643000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:41.650000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:41.658000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:41.663000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:41.668000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:41.676000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:41.701000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:41.852000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:41.883000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:41.890000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:41.900000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:41.906000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:41.912000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:41.918000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:41.941000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:42.094000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:42.123000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:42.130000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:42.139000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:42.147000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:42.153000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:42.160000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:42.185000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:42.334000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:42.363000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:42.370000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:42.380000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:42.387000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:42.394000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:42.401000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:42.426000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:42.603000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:42.614000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:42.620000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:42.627000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:42.639000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:42.644000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:42.650000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:42.669000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:42.842000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:42.859000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:42.864000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:42.869000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:42.878000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:53:42.887000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:42.893000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:42.910000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:43.082000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:43.099000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:43.106000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:43.118000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:43.123000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:43.131000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:53:43.150000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:43.327000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:43.341000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:43.363000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:43.368000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:43.375000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:43.424000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:43.571000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:43.590000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:43.608000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:43.614000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:43.619000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:43.669000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:43.835000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:43.861000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:43.867000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:43.913000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:43.919000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:43.961000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:44.112000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:44.117000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:44.164000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:44.170000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:44.175000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:44.208000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:44.359000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:53:44.363000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:44.411000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:44.416000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:44.422000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:44.455000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:44.602000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:44.654000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:44.660000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:44.665000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:44.695000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:44.846000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:44.895000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:44.902000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:44.906000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:44.935000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:45.086000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:53:45.137000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:53:45.144000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:45.149000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:53:45.176000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:45.336000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:45.394000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:53:45.638000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:45.659000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:45.908000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:46.178000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:46.428000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:46.679000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:46.932000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:47.170000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:53:47.406000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_81 0.1039 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1144 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1147 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1149 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1164 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1220 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1568 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1570 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6135 seconds and 0.3388 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_81 0.1037 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1106 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1140 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1144 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1158 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1181 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1556 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1576 ms 38.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5845 seconds and 0.3338 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_81 0.1036 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1053 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1116 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1139 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1140 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1154 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1194 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1567 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1567 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6231 seconds and 0.3349 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_81 0.1011 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1023 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1110 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1113 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1115 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1118 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1172 ms 51.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1463 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1466 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5927 seconds and 0.3395 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_81 0.1038 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1057 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1122 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1152 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1153 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1161 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1201 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1568 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1569 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6287 seconds and 0.3384 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0617 ms 100.0% 
  triton_mm_81 0.1041 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1053 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_101 0.1154 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1156 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_80 0.1160 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1164 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1209 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1568 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1570 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6494 seconds and 0.3406 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_81 0.1043 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1063 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1140 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1160 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1160 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1166 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1213 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1572 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1574 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6683 seconds and 0.3368 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_81 0.1039 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1059 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1147 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1173 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_100 0.1175 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1175 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_92 0.1241 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1578 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1581 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6617 seconds and 0.3371 seconds precompiling for 27 choices
Capturing batches (bs=32 avail_mem=42.07 GB):  87%| | 45/52 [02:01<03:28, 29.85s/it]Capturing batches (bs=24 avail_mem=41.46 GB):  87%| | 45/52 [02:01<03:28, 29.85s/it][rank7]:W1105 09:54:00.833000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:00.908000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:01.012000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:01.089000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:01.109000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:01.111000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:01.164000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:01.182000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:01.186000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:01.268000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:01.282000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:01.285000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:01.291000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:01.330000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:01.357000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:01.405000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:01.413000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:01.463000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:01.489000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:01.509000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:01.580000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:01.593000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:01.654000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:01.759000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:02.532000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:02.768000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:02.788000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:02.796000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:02.968000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:03.021000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:03.100000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:03.257000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:04.310000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:04.387000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:04.407000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:04.483000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:04.489000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:04.496000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:04.571000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:04.582000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:04.669000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:04.771000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:04.784000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:04.847000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:04.860000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:04.946000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:04.958000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:04.959000 289 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank3]:W1105 09:54:05.050000 286 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank2]:W1105 09:54:05.134000 285 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank4]:W1105 09:54:05.199000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:05.225000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:05.284000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:05.307000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:05.391000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:05.424000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:05.435000 288 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank7]:W1105 09:54:05.437000 290 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank0]:W1105 09:54:05.542000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:05.626000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:05.733000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:05.877000 287 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank1]:W1105 09:54:05.902000 284 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank0]:W1105 09:54:06.211000 283 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x128, 16x128x512)
  bmm 0.0065 ms 100.0% 
  triton_bmm_115 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_129 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_111 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_113 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_120 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_123 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_126 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3019 seconds and 0.3864 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_117 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_111 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_129 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_115 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 98.1% 
  triton_bmm_110 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_119 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_106 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3530 seconds and 0.3915 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_117 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_126 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_110 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_115 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_124 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_129 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0063 ms 98.1% 
SingleProcess AUTOTUNE benchmarking takes 5.3042 seconds and 0.3911 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_107 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_111 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_106 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_113 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_126 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2690 seconds and 0.3700 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_117 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_129 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_105 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_109 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_120 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_110 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_115 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2874 seconds and 0.3675 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  bmm 0.0065 ms 100.0% 
  triton_bmm_105 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_115 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_120 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_121 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_113 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3037 seconds and 0.3684 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_111 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_117 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_115 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_106 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_109 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2768 seconds and 0.3768 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_115 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_105 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_109 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 98.8% 
  triton_bmm_120 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_123 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3083 seconds and 0.3711 seconds precompiling for 27 choices
[rank5]:W1105 09:54:16.691000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:17.006000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:17.044000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:17.071000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:17.191000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:17.425000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:17.504000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:17.539000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:17.562000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:17.573000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:17.661000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:17.678000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:17.925000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:18.072000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:18.162000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:18.177000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:19.574000 288 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank3]:W1105 09:54:20.103000 286 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank1]:W1105 09:54:20.196000 284 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank7]:W1105 09:54:20.396000 290 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank6]:W1105 09:54:20.819000 289 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank0]:W1105 09:54:21.442000 283 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank4]:W1105 09:54:21.498000 287 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank2]:W1105 09:54:23.505000 285 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 83.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0084 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0085 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 74.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0092 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2251 seconds and 0.5182 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0068 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0079 ms 84.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0086 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0087 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0089 ms 74.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0095 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2433 seconds and 0.5220 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0069 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0080 ms 82.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0086 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_137 0.0086 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0089 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0094 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2157 seconds and 0.5328 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0075 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 83.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_137 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0085 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0091 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0091 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_155 0.0095 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_141 0.0095 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2430 seconds and 0.5169 seconds precompiling for 27 choices
[rank5]:W1105 09:54:26.460000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:26.535000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:26.583000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0068 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 83.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0087 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0093 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2252 seconds and 0.5339 seconds precompiling for 27 choices
[rank3]:W1105 09:54:26.840000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:26.915000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:26.963000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:27.140000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:27.214000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_133 0.0069 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0074 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0080 ms 84.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0085 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_137 0.0087 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0089 ms 75.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0094 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2080 seconds and 0.5334 seconds precompiling for 27 choices
[rank7]:W1105 09:54:27.262000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0072 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0082 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0086 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0087 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0094 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2597 seconds and 0.5217 seconds precompiling for 27 choices
[rank1]:W1105 09:54:27.364000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:27.439000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:27.488000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:27.498000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:27.576000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:27.625000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:28.307000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:28.384000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:28.407000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:28.433000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:28.482000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:28.530000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0067 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0072 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0084 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0087 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0088 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0094 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2022 seconds and 0.5192 seconds precompiling for 27 choices
[rank2]:W1105 09:54:30.758000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:30.834000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:30.882000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:32.352000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:32.382000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:32.427000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:32.458000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:32.530000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:32.563000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:32.895000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:32.970000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:33.073000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:33.511000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:33.561000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:33.586000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:33.637000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:33.687000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:33.741000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:34.228000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:34.304000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:34.407000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:34.427000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:34.501000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:34.602000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:35.722000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:35.815000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:35.962000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:35.998000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:36.051000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:36.202000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:36.238000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:36.287000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:36.479000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:36.508000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:36.572000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:36.646000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:36.747000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:36.759000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:36.855000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:37.002000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:37.099000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:37.339000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:37.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:37.731000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:37.818000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:37.838000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:37.913000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:37.921000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:37.975000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:37.996000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:38.012000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:38.059000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:38.094000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:38.104000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:38.178000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:38.219000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:38.276000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:38.496000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:38.580000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:38.693000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:38.953000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:39.028000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:39.129000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:39.717000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:39.794000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:39.894000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:39.919000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:40.159000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:40.399000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:40.476000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:40.550000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:40.648000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:42.028000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:42.102000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:42.201000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0286 ms 32.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0401 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0485 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0590 ms 16.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0681 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0691 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0695 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5363 seconds and 0.3863 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_159 0.0330 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0403 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0614 ms 15.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0681 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0693 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0709 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5468 seconds and 0.3795 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0308 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0413 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0525 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0659 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0720 ms 13.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0728 ms 12.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0736 ms 12.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.7751 seconds and 0.3839 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0290 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0404 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0528 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0652 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.0695 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0711 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0719 ms 13.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.4840 seconds and 0.3746 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_159 0.0332 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0404 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0488 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_181 0.0681 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0694 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_163 0.0698 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_170 0.0707 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0711 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6650 seconds and 0.3772 seconds precompiling for 27 choices
[rank5]:W1105 09:54:45.626000 288 torch/_dynamo/variables/builtin.py:1091] [70/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c61f0>
[rank5]:W1105 09:54:45.771000 288 torch/_dynamo/variables/builtin.py:1091] [71/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c6490>
[rank3]:W1105 09:54:45.807000 286 torch/_dynamo/variables/builtin.py:1091] [70/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31ebb0>
[rank3]:W1105 09:54:45.830000 286 torch/_dynamo/variables/builtin.py:1091] [71/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31efd0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:54:45 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:54:45 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1105 09:54:45.984000 290 torch/_dynamo/variables/builtin.py:1091] [70/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed1d40>
[rank7]:W1105 09:54:46.008000 290 torch/_dynamo/variables/builtin.py:1091] [71/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed2160>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:54:46 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0288 ms 33.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0402 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0488 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0591 ms 16.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0682 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0688 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0697 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0709 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4944 seconds and 0.3745 seconds precompiling for 27 choices
[rank6]:W1105 09:54:46.533000 289 torch/_dynamo/variables/builtin.py:1091] [70/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8733f4f120>
[rank6]:W1105 09:54:46.557000 289 torch/_dynamo/variables/builtin.py:1091] [71/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8734f10ae0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:54:46 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1105 09:54:46.726000 284 torch/_dynamo/variables/builtin.py:1091] [70/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2970>
[rank1]:W1105 09:54:46.750000 284 torch/_dynamo/variables/builtin.py:1091] [71/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2af0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:54:46 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0291 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0400 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0589 ms 16.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0683 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0688 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0693 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6278 seconds and 0.3940 seconds precompiling for 27 choices
[rank0]:W1105 09:54:47.576000 283 torch/_dynamo/variables/builtin.py:1091] [70/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e68e0>
[rank0]:W1105 09:54:47.599000 283 torch/_dynamo/variables/builtin.py:1091] [71/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e6220>
[rank5]:W1105 09:54:47.634000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:54:47 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1105 09:54:47.835000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:47.863000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:47.883000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:48.079000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:48.106000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:48.127000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:48.319000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:48.350000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:48.364000 287 torch/_dynamo/variables/builtin.py:1091] [70/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a550>
[rank5]:W1105 09:54:48.371000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:48.387000 287 torch/_dynamo/variables/builtin.py:1091] [71/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a8e0>
[rank6]:W1105 09:54:48.411000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:54:48 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1105 09:54:48.559000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:48.594000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:48.600000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:48.615000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0315 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0404 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0602 ms 15.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0680 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0693 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0708 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0710 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5253 seconds and 0.3882 seconds precompiling for 27 choices
[rank6]:W1105 09:54:48.659000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:48.807000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:48.842000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:48.848000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:48.863000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:48.907000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:49.051000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:49.091000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:49.097000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:49.107000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:49.131000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:49.155000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:49.295000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:49.334000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:49.340000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:49.352000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:49.374000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:49.404000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:49.538000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:49.579000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:49.590000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:49.604000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:49.630000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:49.652000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:49.777000 285 torch/_dynamo/variables/builtin.py:1091] [70/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f69e30>
[rank3]:W1105 09:54:49.782000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:49.800000 285 torch/_dynamo/variables/builtin.py:1091] [71/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f6a5b0>
[rank7]:W1105 09:54:49.823000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:49.834000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:49.847000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:54:49 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1105 09:54:49.874000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:49.900000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:49.935000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:50.026000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:50.068000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:50.078000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:50.092000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:50.118000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:50.144000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:50.179000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:50.270000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:50.311000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:50.322000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:50.335000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:50.361000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:50.388000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:50.419000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:50.514000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:50.556000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:50.566000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:50.579000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:50.606000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:50.632000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:50.663000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:50.758000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:50.799000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:50.810000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:50.823000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:50.850000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:50.876000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:50.910000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:51.002000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:51.043000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:51.054000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:51.068000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:51.094000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:51.120000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:51.151000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:51.246000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:51.287000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:51.298000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:51.311000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:51.335000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:51.340000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:51.364000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:51.395000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:51.491000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:51.531000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:51.543000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:51.555000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:51.579000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:51.586000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:51.607000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:51.635000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:51.735000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:51.774000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:51.787000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:51.802000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:51.819000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:51.827000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:51.855000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:51.882000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:51.979000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:52.018000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:52.030000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:52.051000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:52.058000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:52.066000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:52.104000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:52.127000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:52.222000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:52.263000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:52.274000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:52.298000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:52.304000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:52.310000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:52.352000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:52.372000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:52.468000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:52.519000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:52.540000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:52.555000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:52.560000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:52.603000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:52.619000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:52.635000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:52.767000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:52.783000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:52.806000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:52.811000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:52.844000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:52.851000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:52.863000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:52.879000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:53.011000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:53.027000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:53.055000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:53.060000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:53.088000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:53.099000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:53.107000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:53.123000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:53.276000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:53.303000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:53.340000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:53.359000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:53.375000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:53.387000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:53.475000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:53.514000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:53.520000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:53.547000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:53.583000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:53.607000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:53.624000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:53.631000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:53.726000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:53.763000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:53.768000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:53.791000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:53.827000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:53.851000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:53.871000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:53.880000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:53.979000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:54.020000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:54.025000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:54.087000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:54.098000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:54.119000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:54.131000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:54.179000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:54.231000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:54.268000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:54.275000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:54.335000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:54.342000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:54.363000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:54.375000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:54.423000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:54.483000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:54.511000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:54.529000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:54.579000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:54.613000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:54.618000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:54.670000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:54.728000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:54.736000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:54.754000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:54.784000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:54.823000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:54.861000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:54.867000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:54.918000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:54.976000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:54.984000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:55.032000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:55.070000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:55.109000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:55.114000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:55.126000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:55.170000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:55.224000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:55.232000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:55.279000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:55.318000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:55.356000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:55.362000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:55.370000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:55.418000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:55.472000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:55.480000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:55.527000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:55.566000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:55.605000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:55.611000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:55.616000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:55.666000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:55.720000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:55.728000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:55.775000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:55.814000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:55.852000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:55.858000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:55.864000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:55.914000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:55.968000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:55.976000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:56.024000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:56.062000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:56.096000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:56.106000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:56.112000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:56.162000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:56.216000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:56.224000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:56.273000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:56.310000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:56.340000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:56.354000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:56.359000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:56.422000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:56.464000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:56.472000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:56.519000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:56.558000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:56.584000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:56.598000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:56.606000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:56.670000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:56.712000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:56.720000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:56.767000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:56.806000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:56.828000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:56.842000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:56.850000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:56.918000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:56.955000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:56.968000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:57.015000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:57.054000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:57.072000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:57.090000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:57.096000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:57.166000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:57.200000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:57.216000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:57.263000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:57.302000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:57.315000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:57.338000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:57.343000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:57.414000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:57.456000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:57.464000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:57.511000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:57.551000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:57.559000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:57.586000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:57.591000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:57.662000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:57.704000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:57.712000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:57.759000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:57.798000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:57.806000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:57.830000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:57.837000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:57.910000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:57.948000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:57.960000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:58.007000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:58.046000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:58.053000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:58.084000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:58.090000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:58.158000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:58.192000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:58.212000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:58.255000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:58.300000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:58.306000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:58.334000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:58.339000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:58.406000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:58.436000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:58.460000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:58.504000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:58.544000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:58.554000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:58.579000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:58.584000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:58.654000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:58.680000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:58.708000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:58.751000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:58.788000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:58.802000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:58.826000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:58.831000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:58.902000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:58.924000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:58.956000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:58.999000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:59.036000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:59.050000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:59.074000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:59.079000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:59.150000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:59.172000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:59.204000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:59.252000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:59.298000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:59.318000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:59.326000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:59.399000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:59.424000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:59.432000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:59.460000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:59.504000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:59.566000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:59.574000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:59.651000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:59.672000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:59.679000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:59.696000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:59.707000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:54:59.751000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:54:59.815000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:54:59.823000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:54:59.899000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:54:59.919000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:54:59.927000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:54:59.944000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:54:59.959000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:00.003000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:00.064000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:00.155000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:00.172000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:00.179000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:00.196000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:00.219000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:00.311000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:00.399000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:00.407000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:00.419000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:00.424000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:00.431000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:00.443000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:00.467000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:00.555000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:00.656000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:00.660000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:00.676000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:00.683000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:00.690000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:00.696000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:00.711000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:00.800000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:00.911000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:00.923000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:00.935000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:00.941000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:00.963000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:00.975000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:01.047000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:01.067000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:01.166000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:01.171000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:01.187000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:01.192000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:01.216000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:01.223000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:01.292000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:01.315000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:01.423000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:01.443000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:01.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:01.463000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:01.471000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:01.535000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:01.563000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:01.575000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:01.680000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:01.700000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:01.707000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:01.715000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:01.720000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:01.814000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:01.828000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:01.931000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:01.937000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:01.948000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:01.961000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:01.967000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:01.972000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:02.066000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:02.075000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:02.178000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:02.212000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:02.222000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:02.319000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:02.325000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:02.427000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:02.465000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:02.471000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:02.575000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:02.580000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:02.676000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:02.724000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:02.733000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:02.832000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:02.848000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:02.964000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:03.095000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:03.107000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:03.216000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:03.351000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:03.374000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:03.467000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:03.601000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:03.723000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:03.860000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:03.978000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:04.116000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:04.227000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:04.480000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:04.727000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:04.972000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:05.228000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:05.475000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_185 0.0997 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1023 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_205 0.1103 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1108 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_184 0.1110 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1111 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1164 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1457 ms 41.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1459 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6350 seconds and 0.3642 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_185 0.1029 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_205 0.1140 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1148 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_184 0.1151 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1157 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1199 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1559 ms 39.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1562 ms 39.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6211 seconds and 0.3812 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_185 0.1029 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1059 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1130 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1145 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1151 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1163 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1220 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1562 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1563 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6910 seconds and 0.3962 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_185 0.1027 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1057 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1140 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1143 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1149 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1157 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1220 ms 50.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1560 ms 39.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1562 ms 39.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6369 seconds and 0.3492 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_185 0.1025 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1059 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1140 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1147 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1152 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1159 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1216 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1562 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1564 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6417 seconds and 0.3621 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_185 0.1025 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1057 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1102 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1142 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1148 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1154 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1180 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1557 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_202 0.1570 ms 38.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5926 seconds and 0.3629 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_185 0.1024 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1056 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1117 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1154 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.1155 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_204 0.1169 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1199 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1560 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1563 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6074 seconds and 0.3442 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_185 0.1027 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1122 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1154 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.1155 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_204 0.1161 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1236 ms 49.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1563 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1566 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5936 seconds and 0.3447 seconds precompiling for 27 choices
Capturing batches (bs=24 avail_mem=41.46 GB):  88%| | 46/52 [03:19<04:25, 44.28s/it]Capturing batches (bs=16 avail_mem=40.83 GB):  88%| | 46/52 [03:19<04:25, 44.28s/it][rank6]:W1105 09:55:18.558000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:18.608000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:18.635000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:18.670000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:18.674000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:18.682000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:18.743000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:18.745000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:18.749000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:18.787000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:18.808000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:18.845000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:18.850000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:18.855000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:18.883000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:18.921000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:18.945000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:18.987000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:19.020000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:19.027000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:19.126000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:19.234000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:19.310000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:19.416000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:20.277000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:20.312000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:20.372000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:20.389000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:20.505000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:20.564000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:20.641000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:20.953000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:21.933000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:22.009000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:22.119000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:22 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:55:22.354000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:22.375000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:22.378000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:22.429000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:22.451000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:22.455000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:22.491000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:22.527000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:22.551000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:22.555000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:22.567000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:22.580000 286 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:22 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:55:22.594000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:22 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:22 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1105 09:55:22.669000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:22.675000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:22.724000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:22 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1105 09:55:22.738000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:22.784000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:22.802000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:22.818000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:22 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:55:22.902000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:22.918000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:22 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:22 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:55:23.001000 285 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank6]:W1105 09:55:23.031000 289 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank0]:W1105 09:55:23.040000 283 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank1]:W1105 09:55:23.147000 284 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank5]:W1105 09:55:23.256000 288 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank7]:W1105 09:55:23.369000 290 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank4]:W1105 09:55:23.392000 287 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_229 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_218 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_228 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_213 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_215 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_221 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_230 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_212 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_214 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_220 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8390 seconds and 0.1518 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0061 ms 100.0% 
  triton_bmm_226 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_219 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_221 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_231 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_224 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_228 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_214 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_218 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8138 seconds and 0.1552 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_211 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_220 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_230 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_226 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_231 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_209 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_210 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_214 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_223 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7843 seconds and 0.1466 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_221 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_218 0.0066 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_209 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_211 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_212 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_213 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_215 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_219 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_220 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8652 seconds and 0.1551 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_219 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_226 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_225 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_213 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_214 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_215 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_222 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_223 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_224 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7933 seconds and 0.1517 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_211 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_213 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_214 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_219 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_220 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_224 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_227 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_215 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8070 seconds and 0.1576 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_213 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_212 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_215 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0063 ms 99.4% 
  triton_bmm_219 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_210 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_220 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_222 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_211 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8325 seconds and 0.1532 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0065 ms 100.0% 
  triton_bmm_212 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_213 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_218 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_219 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_220 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_223 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_224 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_225 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_226 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8614 seconds and 0.1342 seconds precompiling for 25 choices
[rank0]:W1105 09:55:33.016000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:33.361000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:33.524000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:33.557000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:33.773000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:33.864000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:33.936000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:34.059000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:34.067000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:34.264000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:34.337000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:34.438000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:34.573000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:34.838000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:35.186000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:35.689000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:35.847000 283 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank3]:W1105 09:55:36.201000 286 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank7]:W1105 09:55:36.489000 290 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank2]:W1105 09:55:36.811000 285 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank6]:W1105 09:55:37.134000 289 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank1]:W1105 09:55:38.080000 284 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank5]:W1105 09:55:38.913000 288 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank4]:W1105 09:55:39.652000 287 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_234 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_245 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0073 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0081 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_254 0.0084 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7552 seconds and 0.4020 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_235 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0067 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_245 0.0069 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0071 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0075 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0075 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_238 0.0083 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0083 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_254 0.0085 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8356 seconds and 0.3990 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0078 ms 83.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0078 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0080 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0080 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0086 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8767 seconds and 0.4175 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_245 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0073 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_239 0.0079 ms 82.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0079 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_255 0.0083 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7987 seconds and 0.3944 seconds precompiling for 25 choices
[rank3]:W1105 09:55:42.407000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0067 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0073 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0082 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0082 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0084 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8661 seconds and 0.4035 seconds precompiling for 25 choices
[rank3]:W1105 09:55:42.484000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:42.545000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:42.583000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:42.623000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:42 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:55:42.707000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:42.721000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:42 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:55:42.784000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:42.882000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:42 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1105 09:55:43.385000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_243 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0076 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_239 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_254 0.0084 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8312 seconds and 0.4120 seconds precompiling for 25 choices
[rank2]:W1105 09:55:43.430000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:43.466000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:43.507000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:43.567000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:43.607000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:43 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:43 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_235 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_245 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0081 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_236 0.0083 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8444 seconds and 0.4012 seconds precompiling for 25 choices
[rank1]:W1105 09:55:44.299000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:44.377000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:44.477000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:44 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_235 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_244 0.0068 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0068 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_243 0.0073 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0074 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_238 0.0081 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0081 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_237 0.0084 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8752 seconds and 0.4158 seconds precompiling for 25 choices
[rank5]:W1105 09:55:45.297000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:45.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:45.476000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:45 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1105 09:55:46.358000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:46 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1105 09:55:46.435000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:46.534000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:46 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:47 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:47 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:47 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1105 09:55:47.702000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:47.778000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:47.827000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:47 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:48 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:48 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1105 09:55:48.314000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:48.389000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:48.439000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:48 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:55:48.600000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:48.669000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:48.674000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:48.723000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:48.746000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:48 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1105 09:55:48.796000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:48 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:55:49.754000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:49.828000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:49.876000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:49.928000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:49 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1105 09:55:50.002000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:50 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1105 09:55:50.051000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:50 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:50 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1105 09:55:51.260000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:51.299000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:51.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:51.425000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:51 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:55:51.483000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:51.511000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:51.603000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:51.644000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:51.718000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:51.735000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:51.755000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:51.767000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:51 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1105 09:55:51.852000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:51.999000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:52.099000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:52 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1105 09:55:52.270000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:52 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:52 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1105 09:55:52.523000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:52.640000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:52.779000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:52.817000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:52.887000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:53.067000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:53 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:55:53.148000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:53.237000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:53.312000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:53.315000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:55:53.412000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:53 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:53 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:55:53.629000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:53 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1105 09:55:53.703000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:53.740000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:55:53.802000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:55:53.818000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:53 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1105 09:55:53.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:53 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1105 09:55:54.430000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:54.507000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:55:54.609000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:54.616000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:54 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1105 09:55:54.808000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:54.864000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:54.883000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:54.972000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:55:54.982000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:55 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1105 09:55:55.050000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:55.117000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:55:55.152000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:55 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1105 09:55:55.241000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:55 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1105 09:55:55.497000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:55.760000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:56 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1105 09:55:57.518000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:57.593000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:55:57.691000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:57 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1105 09:55:57.834000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:57.909000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:55:58.007000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 09:55:58 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_268 0.0307 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0309 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0311 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0331 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0463 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0464 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0516 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0520 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0543 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0022 seconds and 0.2187 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_258 0.0325 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_259 0.0330 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0338 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0339 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_266 0.0498 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0499 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0500 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_267 0.0507 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0599 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1138 seconds and 0.2221 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_259 0.0295 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_268 0.0308 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0308 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_258 0.0323 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0463 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0465 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0505 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0508 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0542 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0325 seconds and 0.2182 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_258 0.0311 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_268 0.0313 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_259 0.0316 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0324 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_267 0.0493 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0503 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0507 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0512 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_263 0.0544 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0234 seconds and 0.2309 seconds precompiling for 25 choices
[rank0]:W1105 09:56:00.306000 283 torch/_dynamo/variables/builtin.py:1091] [70/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e68e0>
[rank0]:W1105 09:56:00.533000 283 torch/_dynamo/variables/builtin.py:1091] [71/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e6220>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:56:00 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0098 ms 100.0% 
  triton_mm_268 0.0306 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0306 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0355 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0355 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0467 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0475 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0503 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0504 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_278 0.0623 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0567 seconds and 0.2250 seconds precompiling for 25 choices
[rank3]:W1105 09:56:00.698000 286 torch/_dynamo/variables/builtin.py:1091] [70/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31ebb0>
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0100 ms 100.0% 
  triton_mm_268 0.0308 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0308 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0348 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0356 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0465 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0469 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0514 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0516 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_263 0.0580 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0621 seconds and 0.2178 seconds precompiling for 25 choices
[rank7]:W1105 09:56:00.813000 290 torch/_dynamo/variables/builtin.py:1091] [70/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed1d40>
[rank7]:W1105 09:56:00.836000 290 torch/_dynamo/variables/builtin.py:1091] [71/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed2160>
[rank3]:W1105 09:56:00.945000 286 torch/_dynamo/variables/builtin.py:1091] [71/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31efd0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:56:01 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:56:01 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1105 09:56:01.386000 289 torch/_dynamo/variables/builtin.py:1091] [70/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8733f4f120>
[rank6]:W1105 09:56:01.608000 289 torch/_dynamo/variables/builtin.py:1091] [71/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8734f10ae0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:56:01 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1105 09:56:01.928000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:01.949000 285 torch/_dynamo/variables/builtin.py:1091] [70/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f69e30>
[rank2]:W1105 09:56:01.972000 285 torch/_dynamo/variables/builtin.py:1091] [71/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f6a5b0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:56:02 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1105 09:56:02.113000 284 torch/_dynamo/variables/builtin.py:1091] [70/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2970>
[rank0]:W1105 09:56:02.183000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:02.354000 284 torch/_dynamo/variables/builtin.py:1091] [71/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2af0>
[rank7]:W1105 09:56:02.418000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:56:02 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1105 09:56:02.438000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:02.669000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:02.696000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:02.812000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:02.915000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:02.955000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:03.007000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:03.080000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:03.163000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:03.208000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:03.262000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_269 0.0307 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0307 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_268 0.0307 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0319 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0463 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0463 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0520 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0520 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0541 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0596 seconds and 0.2116 seconds precompiling for 25 choices
[rank3]:W1105 09:56:03.340000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:03.411000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:03.463000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:03.519000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:03.592000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:03.663000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_269 0.0306 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0307 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_259 0.0331 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0338 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0461 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0462 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0507 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0508 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0544 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0862 seconds and 0.2140 seconds precompiling for 25 choices
[rank0]:W1105 09:56:03.715000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:03.752000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:03.775000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:03.844000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:03.916000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:03.967000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:04.004000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:04.032000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:04.096000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:04.168000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:04.219000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:04.224000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:04.255000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:04.292000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:04.347000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:04.420000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:04.474000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:04.480000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:04.503000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:04.544000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:04.599000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:04.668000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:04.730000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:04.736000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:04.750000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:04.796000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:04.851000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:04.860000 288 torch/_dynamo/variables/builtin.py:1091] [70/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c61f0>
[rank5]:W1105 09:56:04.884000 288 torch/_dynamo/variables/builtin.py:1091] [71/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c6490>
[rank7]:W1105 09:56:04.916000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:56:04 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1105 09:56:04.983000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:04.990000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:04.999000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:05.002000 287 torch/_dynamo/variables/builtin.py:1091] [70/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a550>
[rank6]:W1105 09:56:05.048000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:05.108000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:05.165000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:05.239000 287 torch/_dynamo/variables/builtin.py:1091] [71/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a8e0>
[rank2]:W1105 09:56:05.247000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:05.254000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:05.260000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:05.304000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:56:05 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1105 09:56:05.363000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:05.420000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:05.498000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:05.506000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:05.512000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:05.560000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:05.617000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:05.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:05.763000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:05.770000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:05.775000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:05.812000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:05.871000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:05.925000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:06.014000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:06.022000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:06.028000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:06.064000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:06.123000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:06.180000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:06.266000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:06.274000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:06.280000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:06.320000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:06.375000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:06.432000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:06.440000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:06.518000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:06.526000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:06.532000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:06.572000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:06.627000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:06.684000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:06.696000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:06.770000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:06.778000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:06.783000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:06.830000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:06.879000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:06.936000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:06.947000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:07.022000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:07.030000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:07.036000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:07.084000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:07.131000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:07.188000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:07.199000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:07.275000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:07.282000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:07.304000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:07.310000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:07.340000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:07.384000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:07.439000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:07.451000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:07.530000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:07.544000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:07.563000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:07.570000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:07.595000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:07.636000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:07.691000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:07.703000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:07.783000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:07.795000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:07.814000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:07.821000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:07.852000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:07.887000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:07.944000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:07.956000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:08.035000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:08.047000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:08.066000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:08.072000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:08.108000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:08.139000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:08.196000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:08.208000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:08.287000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:08.298000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:08.322000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:08.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:08.360000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:08.391000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:08.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:08.456000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:08.536000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:08.550000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:08.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:08.584000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:08.612000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:08.643000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:08.700000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:08.708000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:08.784000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:08.803000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:08.834000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:08.840000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:08.864000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:08.895000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:08.952000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:08.960000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:09.035000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:09.054000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:09.090000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:09.096000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:09.116000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:09.147000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:09.204000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:09.212000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:09.287000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:09.309000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:09.347000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:09.354000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:09.372000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:09.400000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:09.455000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:09.463000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:09.539000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:09.567000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:09.605000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:09.611000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:09.627000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:09.652000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:09.707000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:09.715000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:09.791000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:09.832000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:09.860000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:09.867000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:09.883000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:09.904000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:09.959000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:09.967000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:10.043000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:10.084000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:10.112000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:10.119000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:10.139000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:10.156000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:10.211000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:10.219000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:10.296000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:10.335000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:10.363000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:10.370000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:10.400000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:10.407000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:10.465000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:10.473000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:10.548000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:10.591000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:10.619000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:10.656000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:10.663000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:10.717000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:10.724000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:10.800000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:10.846000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:10.870000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:10.876000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:10.914000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:10.919000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:10.980000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:11.060000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:11.102000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:11.126000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:11.132000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:11.171000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:11.178000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:11.202000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:11.240000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:11.316000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:11.367000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:11.391000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:11.403000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:11.436000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:11.457000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:11.492000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:11.568000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:11.626000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:11.646000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:11.656000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:11.662000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:11.713000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:11.748000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:11.824000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:11.883000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:11.898000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:11.911000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:11.918000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:11.933000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:11.968000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:12.004000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:12.080000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:12.138000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:12.150000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:12.167000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:12.174000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:12.192000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:12.220000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:12.256000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:12.332000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:12.395000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:12.423000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:12.435000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:12.453000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:12.473000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:12.508000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:12.588000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:12.643000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:12.651000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:12.679000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:12.694000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:12.712000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:12.728000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:12.760000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:12.840000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:12.899000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:12.907000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:12.947000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:12.962000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:12.972000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:12.980000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:13.012000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:13.092000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:13.154000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:13.163000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:13.203000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:13.222000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:13.233000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:13.240000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:13.272000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:13.352000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:13.411000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:13.419000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:13.459000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:13.478000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:13.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:13.507000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:13.532000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:13.608000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:13.671000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:13.719000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:13.743000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:13.756000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:13.764000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:13.784000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:13.860000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:13.923000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:13.928000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:13.979000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:14.003000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:14.012000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:14.020000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:14.036000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:14.112000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:14.182000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:14.188000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:14.235000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:14.262000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:14.270000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:14.281000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:14.372000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:14.439000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:14.447000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:14.491000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:14.512000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:14.520000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:14.528000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:14.544000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:14.628000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:14.694000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:14.703000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:14.747000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:14.768000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:14.780000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:14.785000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:14.800000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:14.880000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:14.955000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:14.960000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:15.003000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:15.028000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:15.035000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:15.054000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:15.062000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:15.132000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:15.211000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:15.219000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:15.259000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:15.284000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:15.290000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:15.316000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:15.321000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:15.384000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:15.467000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:15.479000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:15.515000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:15.540000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:15.547000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:15.572000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:15.578000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:15.636000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:15.723000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:15.735000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:15.771000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:15.797000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:15.804000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:15.829000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:15.838000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:15.979000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:15.991000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:16.027000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:16.060000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:16.067000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:16.093000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:16.098000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:16.112000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:16.235000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:16.250000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:16.283000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:16.317000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:16.324000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:16.348000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:16.358000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:16.368000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:16.491000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:16.511000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:16.539000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:16.573000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:16.580000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:16.604000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:16.618000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:16.626000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:16.747000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:16.767000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:16.795000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:16.829000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:16.836000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:16.860000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:16.880000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:17.014000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:17.035000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:17.063000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:17.092000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:17.116000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:17.132000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:17.271000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:17.291000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:17.319000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:17.348000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:17.373000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:17.385000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:17.535000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:17.551000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:17.609000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:17.637000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:17.653000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:17.795000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:17.811000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:17.872000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:17.916000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:18.051000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:18.071000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:18.129000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:18.173000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:18.311000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:18.331000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:18.397000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:18.437000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:18.592000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:18.668000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:18.700000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:18.850000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:18.926000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:18.956000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:19.181000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:19.212000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:19.444000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:19.475000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:19.696000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:19.735000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:19.960000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:19.991000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:20.215000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:20.242000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:20.466000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:20.499000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:20.714000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:20.756000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:20.963000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:21.007000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:21.254000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:21.506000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:21.769000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_282 0.1037 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1038 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1049 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1050 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1093 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1094 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1128 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1130 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1486 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0942 seconds and 0.1821 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_283 0.1005 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1005 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_292 0.1015 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1015 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1057 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1058 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1090 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1090 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_303 0.1390 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0557 seconds and 0.1889 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_282 0.1037 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1038 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1048 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_301 0.1095 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1096 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_290 0.1134 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1134 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1484 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0095 seconds and 0.1857 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_283 0.1035 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1035 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_292 0.1047 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1047 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1093 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1094 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1131 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1132 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1484 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0739 seconds and 0.1860 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_282 0.1037 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1047 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1048 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1100 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1101 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_291 0.1133 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1136 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_287 0.1484 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0493 seconds and 0.1983 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_282 0.1035 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1035 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_293 0.1045 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1045 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_300 0.1095 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1095 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1125 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1125 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1475 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0669 seconds and 0.1797 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_283 0.1036 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1036 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_293 0.1042 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1043 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1102 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1104 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_290 0.1140 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1140 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1485 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0539 seconds and 0.2058 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_282 0.1036 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1043 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1043 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1100 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1102 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1140 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1144 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_302 0.1487 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0581 seconds and 0.1811 seconds precompiling for 25 choices
Capturing batches (bs=16 avail_mem=40.83 GB):  90%| | 47/52 [04:35<04:28, 53.78s/it]Capturing batches (bs=12 avail_mem=40.25 GB):  90%| | 47/52 [04:35<04:28, 53.78s/it][rank2]:W1105 09:56:34.467000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:34.543000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:34.642000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:34.659000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:34.679000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:34.719000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:34.726000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:34.751000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:34.755000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:34.803000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:34.826000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:34.829000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:34.861000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:34.911000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:34.937000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:35.141000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:35.188000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:35.218000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:35.265000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:35.269000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:35.325000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:35.345000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:35.372000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:35.451000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:36.173000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:36.352000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:36.372000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:36.441000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:36.461000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:36.844000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:36.886000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:36.958000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:38.064000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:38.140000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:38.142000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:38.217000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:38.242000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:38.298000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:38.318000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:38.377000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:38.396000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:38.475000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:38.477000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:38.577000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:38.802000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:38.820000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:38.879000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:38.897000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:38.926000 290 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank4]:W1105 09:56:38.939000 287 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank1]:W1105 09:56:38.979000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:39.001000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:39.011000 285 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank6]:W1105 09:56:39.043000 289 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank3]:W1105 09:56:39.147000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:39.230000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:39.334000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:39.455000 284 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank5]:W1105 09:56:39.490000 288 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank0]:W1105 09:56:39.679000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:39.759000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:39.804000 286 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank0]:W1105 09:56:39.861000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:40.339000 283 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_305 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_310 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_308 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_316 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_319 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_314 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_318 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_322 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8117 seconds and 0.1503 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_327 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_314 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_311 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_308 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_326 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_304 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_317 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 98.1% 
  triton_bmm_316 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8777 seconds and 0.1334 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_318 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_326 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_322 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_324 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_307 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_310 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_311 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_314 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_316 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8025 seconds and 0.1310 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_324 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_307 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_310 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_319 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_320 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_326 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_327 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_325 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8323 seconds and 0.1543 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_322 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_321 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_314 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_319 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_323 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_325 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_320 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_324 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_308 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8347 seconds and 0.1332 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_311 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_324 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_308 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_323 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_305 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_309 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_314 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8597 seconds and 0.1305 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_311 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_316 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_321 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_306 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_315 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_314 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0064 ms 97.5% 
SingleProcess AUTOTUNE benchmarking takes 4.8082 seconds and 0.1324 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_318 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 100.0% 
  triton_bmm_314 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_316 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_317 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_327 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_310 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_326 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_311 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7707 seconds and 0.1299 seconds precompiling for 25 choices
[rank6]:W1105 09:56:49.034000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:49.091000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:49.537000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:49.590000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:49.616000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:49.884000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:50.051000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:50.116000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:50.209000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:50.384000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:50.549000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:50.708000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:50.726000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:56:51.237000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:51.275000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:56:51.791000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:52.452000 284 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank4]:W1105 09:56:52.463000 287 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank6]:W1105 09:56:52.527000 289 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank3]:W1105 09:56:52.881000 286 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank2]:W1105 09:56:53.200000 285 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank7]:W1105 09:56:53.601000 290 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank0]:W1105 09:56:54.170000 283 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank5]:W1105 09:56:55.095000 288 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0082 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0082 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0083 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8549 seconds and 0.3953 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0070 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0071 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0076 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0076 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_335 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0082 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_350 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8521 seconds and 0.4179 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0067 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0067 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0070 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0071 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0080 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0080 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_332 0.0083 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8284 seconds and 0.4114 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0066 ms 99.4% 
  triton_bmm_330 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_341 0.0068 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0069 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0074 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0074 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_335 0.0082 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0082 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_351 0.0083 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7834 seconds and 0.4203 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_331 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_330 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 97.5% 
  triton_bmm_340 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0070 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0070 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_335 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0079 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_351 0.0082 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8404 seconds and 0.4070 seconds precompiling for 25 choices
[rank4]:W1105 09:56:58.690000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:58.747000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:58.767000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:56:58.816000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:58.824000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:58.857000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:56:58.872000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:58.934000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:56:58.983000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_341 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_338 0.0078 ms 83.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0078 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0085 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8889 seconds and 0.4242 seconds precompiling for 25 choices
[rank3]:W1105 09:56:59.178000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:59.255000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:56:59.304000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.2% 
  triton_bmm_340 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0072 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0083 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7769 seconds and 0.4084 seconds precompiling for 25 choices
[rank2]:W1105 09:56:59.533000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:59.610000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:56:59.659000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:59.876000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:56:59.952000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:00.000000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_341 0.0070 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0070 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0075 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0075 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0081 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8461 seconds and 0.3947 seconds precompiling for 25 choices
[rank0]:W1105 09:57:00.611000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:00.688000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:00.748000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:01.473000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:01.552000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:01.602000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:04.120000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:04.196000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:04.306000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:04.737000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:04.816000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:04.923000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:05.354000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:05.431000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:05.521000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:05.536000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:05.604000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:05.714000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:05.826000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:05.903000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:06.007000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:06.331000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:06.409000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:06.514000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:06.619000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:06.695000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:06.799000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:06.814000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:06.891000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:06.996000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:07.062000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:07.319000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:07.572000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:07.684000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:07.940000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:08.196000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:08.292000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:08.499000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:08.551000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:08.764000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:08.808000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:09.024000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:09.199000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:09.225000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:09.300000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:09.400000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:09.467000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:09.724000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:09.844000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:09.921000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:09.959000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:10.008000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:10.024000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:10.224000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:10.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:10.296000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:10.478000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:10.487000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:10.547000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:10.557000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:10.564000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:10.659000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:10.687000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:10.767000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:10.827000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:10.868000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:11.232000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:11.310000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:11.415000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:12.222000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:12.274000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:12.297000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:12.349000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:12.397000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:12.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:14.108000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:14.183000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:14.284000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0099 ms 100.0% 
  triton_mm_355 0.0297 ms 33.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0304 ms 32.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0306 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0311 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0460 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0468 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0516 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0519 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0550 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0379 seconds and 0.2381 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_364 0.0303 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0304 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_355 0.0315 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0334 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0460 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0460 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0512 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0512 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0541 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0211 seconds and 0.2325 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0099 ms 100.0% 
  triton_mm_365 0.0306 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0307 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0348 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0352 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0466 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0467 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0513 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0514 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0635 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1027 seconds and 0.2319 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_355 0.0326 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_364 0.0327 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0332 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_365 0.0332 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_363 0.0466 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0477 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0507 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0509 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_358 0.0546 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0742 seconds and 0.2280 seconds precompiling for 25 choices
[rank1]:W1105 09:57:16.582000 284 torch/_dynamo/variables/builtin.py:1091] [70/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2970>
[rank1]:W1105 09:57:16.605000 284 torch/_dynamo/variables/builtin.py:1091] [71/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2af0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:57:16 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_365 0.0305 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0305 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0342 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0348 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0460 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0460 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_372 0.0504 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0504 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0543 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0708 seconds and 0.2250 seconds precompiling for 25 choices
[rank4]:W1105 09:57:17.114000 287 torch/_dynamo/variables/builtin.py:1091] [70/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a550>
[rank4]:W1105 09:57:17.137000 287 torch/_dynamo/variables/builtin.py:1091] [71/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a8e0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:57:17 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1105 09:57:17.821000 286 torch/_dynamo/variables/builtin.py:1091] [70/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31ebb0>
[rank3]:W1105 09:57:17.844000 286 torch/_dynamo/variables/builtin.py:1091] [71/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31efd0>
[rank6]:W1105 09:57:17.859000 289 torch/_dynamo/variables/builtin.py:1091] [70/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8733f4f120>
[rank6]:W1105 09:57:17.882000 289 torch/_dynamo/variables/builtin.py:1091] [71/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8734f10ae0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:57:17 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:57:17 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0101 ms 100.0% 
  triton_mm_355 0.0299 ms 33.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0305 ms 33.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0305 ms 33.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0320 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0459 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0460 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_372 0.0518 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0519 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0540 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0277 seconds and 0.2268 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_355 0.0325 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0327 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_364 0.0331 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0336 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0505 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0506 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0506 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0509 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0599 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0509 seconds and 0.2274 seconds precompiling for 25 choices
[rank4]:W1105 09:57:18.521000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:18.539000 285 torch/_dynamo/variables/builtin.py:1091] [70/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f69e30>
[rank2]:W1105 09:57:18.562000 285 torch/_dynamo/variables/builtin.py:1091] [71/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f6a5b0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:57:18 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1105 09:57:18.780000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:18.883000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:19.036000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:19.143000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:19.263000 290 torch/_dynamo/variables/builtin.py:1091] [70/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed1d40>
[rank7]:W1105 09:57:19.286000 290 torch/_dynamo/variables/builtin.py:1091] [71/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed2160>
[rank6]:W1105 09:57:19.289000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:19.296000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:57:19 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1105 09:57:19.399000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:19.548000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:19.556000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:19.625000 283 torch/_dynamo/variables/builtin.py:1091] [70/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e68e0>
[rank0]:W1105 09:57:19.648000 283 torch/_dynamo/variables/builtin.py:1091] [71/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e6220>
[rank1]:W1105 09:57:19.660000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:19.700000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:57:19 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1105 09:57:19.811000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:19.823000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:19.915000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_355 0.0302 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_364 0.0305 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0305 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_354 0.0313 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0460 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0460 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0514 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0515 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0540 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0667 seconds and 0.2170 seconds precompiling for 25 choices
[rank3]:W1105 09:57:19.960000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:20.072000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:20.087000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:20.103000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:20.175000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:20.219000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:20.332000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:20.348000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:20.363000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:20.435000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:20.475000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:20.591000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:20.610000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:20.620000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:20.696000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:20.737000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:20.855000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:20.877000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:20.883000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:20.951000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:20.995000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:21.120000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:21.140000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:21.146000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:21.211000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:21.223000 288 torch/_dynamo/variables/builtin.py:1091] [70/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c61f0>
[rank5]:W1105 09:57:21.246000 288 torch/_dynamo/variables/builtin.py:1091] [71/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c6490>
[rank3]:W1105 09:57:21.255000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:21.285000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:57:21 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1105 09:57:21.380000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:21.401000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:21.407000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:21.471000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:21.515000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:21.551000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:21.639000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:21.659000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:21.668000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:21.732000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:21.777000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:21.813000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:21.903000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:21.920000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:21.928000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:21.935000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:21.991000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:22.032000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:22.073000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:22.164000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:22.181000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:22.187000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:22.193000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:22.251000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:22.287000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:22.333000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:22.424000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:22.440000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:22.447000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:22.454000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:22.511000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:22.547000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:22.596000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:22.683000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:22.699000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:22.709000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:22.715000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:22.772000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:22.809000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:22.856000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:22.947000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:22.959000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:22.968000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:22.975000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:23.032000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:23.064000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:23.115000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:23.123000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:23.211000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:23.219000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:23.226000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:23.235000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:23.292000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:23.320000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:23.375000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:23.383000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:23.475000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:23.481000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:23.488000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:23.496000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:23.552000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:23.577000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:23.636000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:23.644000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:23.740000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:23.748000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:23.756000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:23.813000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:23.832000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:23.901000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:23.907000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:24.007000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:24.015000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:24.020000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:24.071000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:24.085000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:24.091000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:24.161000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:24.169000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:24.267000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:24.278000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:24.286000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:24.331000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:24.349000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:24.354000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:24.421000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:24.429000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:24.528000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:24.540000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:24.549000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:24.614000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:24.620000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:24.682000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:24.691000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:24.791000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:24.803000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:24.883000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:24.891000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:24.927000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:24.945000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:24.957000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:25.052000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:25.063000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:25.141000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:25.157000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:25.195000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:25.205000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:25.220000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:25.315000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:25.327000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:25.405000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:25.420000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:25.465000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:25.472000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:25.481000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:25.492000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:25.603000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:25.669000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:25.685000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:25.725000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:25.739000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:25.747000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:25.755000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:25.867000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:25.911000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:25.932000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:25.948000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:25.980000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:26.003000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:26.011000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:26.019000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:26.131000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:26.171000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:26.196000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:26.212000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:26.236000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:26.267000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:26.275000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:26.283000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:26.395000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:26.435000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:26.460000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:26.476000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:26.493000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:26.531000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:26.539000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:26.547000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:26.655000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:26.699000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:26.724000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:26.741000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:26.749000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:26.796000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:26.802000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:26.811000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:26.915000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:26.963000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:26.993000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:27.005000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:27.062000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:27.068000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:27.075000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:27.175000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:27.227000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:27.265000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:27.272000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:27.325000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:27.332000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:27.340000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:27.346000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:27.435000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:27.495000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:27.533000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:27.540000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:27.586000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:27.594000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:27.604000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:27.612000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:27.768000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:27.797000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:27.805000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:27.845000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:27.853000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:27.867000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:27.876000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:28.037000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:28.045000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:28.061000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:28.069000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:28.104000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:28.113000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:28.131000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:28.139000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:28.311000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:28.316000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:28.325000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:28.332000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:28.365000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:28.373000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:28.391000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:28.399000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:28.575000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:28.583000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:28.590000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:28.597000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:28.625000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:28.633000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:28.651000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:28.659000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:28.839000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:28.847000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:28.857000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:28.864000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:28.893000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:28.916000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:28.923000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:29.115000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:29.123000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:29.130000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:29.137000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:29.157000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:29.191000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:29.197000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:29.205000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:29.379000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:29.387000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:29.396000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:29.404000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:29.417000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:29.455000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:29.461000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:29.470000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:29.643000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:29.651000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:29.660000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:29.669000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:29.685000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:29.719000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:29.727000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:29.735000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:29.907000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:29.915000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:29.925000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:29.932000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:29.944000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:29.983000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:29.991000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:30.000000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:30.171000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:30.179000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:30.188000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:30.196000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:30.205000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:30.248000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:30.254000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:30.263000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:30.435000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:30.443000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:30.452000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:30.461000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:30.468000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:30.511000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:30.519000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:30.528000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:30.699000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:30.707000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:30.717000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:30.725000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:30.732000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:30.776000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:30.784000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:30.791000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:30.963000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:30.971000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:30.980000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:30.989000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:30.996000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:31.039000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:31.051000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:31.059000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:31.227000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:31.235000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:31.244000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:31.253000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:31.260000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:31.303000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:31.315000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:31.323000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:31.491000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:31.499000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:31.508000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:31.517000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:31.524000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:31.567000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:31.579000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:31.587000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:31.755000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:31.763000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:31.773000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:31.781000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:31.788000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:31.831000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:31.843000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:31.851000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:32.019000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:32.027000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:32.036000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:32.045000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:32.052000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:32.095000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:32.107000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:32.115000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:32.285000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:32.292000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:32.299000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:32.307000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:32.313000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:32.360000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:32.373000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:32.380000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:32.548000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:32.556000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:32.567000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:32.575000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:32.581000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:32.624000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:32.633000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:32.644000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:32.812000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:32.820000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:32.836000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:32.843000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:32.849000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:32.889000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:32.897000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:32.908000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:33.076000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:33.084000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:33.103000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:33.111000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:33.117000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:33.153000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:33.161000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:33.172000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:33.340000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:33.348000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:33.371000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:33.377000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:33.384000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:33.417000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:33.425000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:33.436000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:33.604000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:33.612000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:33.639000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:33.645000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:33.652000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:33.680000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:33.690000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:33.700000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:33.868000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:33.876000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:33.909000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:33.915000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:33.944000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:33.953000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:33.964000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:34.132000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:34.140000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:34.171000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:34.183000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:34.216000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:34.228000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:34.404000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:34.418000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:34.435000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:34.451000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:34.489000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:34.497000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:34.673000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:34.684000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:34.700000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:34.754000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:34.769000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:34.949000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:34.960000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:34.972000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:35.040000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:35.219000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:35.228000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:35.236000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:35.304000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:35.500000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:35.507000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:35.572000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:35.772000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:35.779000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:35.844000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:36.041000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:36.048000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:36.108000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:36.312000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:36.323000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:36.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:36.583000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:36.639000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:36.839000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:36.903000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:37.096000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:37.168000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:37.439000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:37.703000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:37.968000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:38.239000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_378 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1046 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1091 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1092 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_386 0.1127 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1128 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1450 ms 42.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0340 seconds and 0.2072 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_378 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1047 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1094 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1096 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_386 0.1133 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1133 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1452 ms 42.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0772 seconds and 0.1880 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_378 0.1044 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1093 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1093 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1130 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1453 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0694 seconds and 0.1969 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_378 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_389 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1048 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_396 0.1094 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1096 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1131 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1452 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1398 seconds and 0.2063 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_378 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1044 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1046 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1092 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1092 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1122 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1122 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1443 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0921 seconds and 0.1925 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_379 0.1011 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1011 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1014 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1014 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1062 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1063 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1087 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1087 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1373 ms 44.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0591 seconds and 0.1899 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_378 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1047 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1048 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1098 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1099 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_387 0.1128 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1130 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_383 0.1454 ms 42.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0474 seconds and 0.2044 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_388 0.1041 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1042 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_379 0.1043 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_396 0.1103 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1103 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1137 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1140 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1454 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0205 seconds and 0.1957 seconds precompiling for 25 choices
Capturing batches (bs=12 avail_mem=40.25 GB):  92%|| 48/52 [05:52<04:02, 60.55s/it]Capturing batches (bs=8 avail_mem=39.63 GB):  92%|| 48/52 [05:52<04:02, 60.55s/it] [rank3]:W1105 09:57:50.952000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:51.031000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:51.101000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:51.140000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:51.179000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:51.287000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:51.316000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:51.393000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:51.394000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:51.469000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:51.501000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:51.576000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:51.716000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:51.778000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:51.794000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:51.858000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:51.904000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:51.939000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:51.969000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:51.970000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:52.017000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:52.048000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:52.125000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:52.155000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:52.695000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:52.852000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:53.034000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:53.089000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:53.427000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:53.506000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:53.642000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:53.685000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:54.093000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:54.172000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:54.275000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:54.402000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:54.482000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:57:54.584000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:57:54.746000 286 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank6]:W1105 09:57:55.061000 289 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank5]:W1105 09:57:55.619000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:55.701000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:55.795000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:55.806000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:55.875000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:55.985000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:56.103000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:56.111000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:56.135000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:56.181000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:56.189000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:56.213000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:57:56.283000 288 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank7]:W1105 09:57:56.283000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:57:56.289000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:57:56.314000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:57:56.451000 283 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank4]:W1105 09:57:56.666000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:56.747000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:57:56.760000 290 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank2]:W1105 09:57:56.763000 285 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank1]:W1105 09:57:56.792000 284 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank4]:W1105 09:57:56.861000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:57:57.359000 287 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_416 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_421 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_415 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_414 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_420 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_419 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9655 seconds and 0.1479 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_418 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_412 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_414 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_419 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_421 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_423 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8392 seconds and 0.1539 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_411 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_414 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_410 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_412 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_421 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_420 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8296 seconds and 0.1492 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_405 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 100.0% 
  triton_bmm_407 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_411 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_412 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_404 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_413 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8131 seconds and 0.1502 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_403 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_416 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_411 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.3% 
  triton_bmm_406 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_407 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_412 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7801 seconds and 0.1546 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_410 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_411 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_403 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_404 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_402 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_405 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_407 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_412 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8113 seconds and 0.1483 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_410 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_407 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_413 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_418 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_404 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_412 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8901 seconds and 0.1534 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0065 ms 100.0% 
  triton_bmm_416 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_404 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_405 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_407 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_414 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8812 seconds and 0.1502 seconds precompiling for 25 choices
[rank6]:W1105 09:58:05.274000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:05.379000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:05.779000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:05.892000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:06.531000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:06.782000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:07.037000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:07.085000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:07.285000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:07.588000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:07.638000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:07.650000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:07.777000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:08.035000 289 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank3]:W1105 09:58:08.091000 286 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank7]:W1105 09:58:08.148000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:08.171000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:08.285000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:09.473000 288 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank1]:W1105 09:58:09.602000 284 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank0]:W1105 09:58:09.927000 283 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank2]:W1105 09:58:10.172000 285 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank7]:W1105 09:58:10.284000 290 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank4]:W1105 09:58:11.945000 287 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_427 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_437 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_431 0.0082 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_447 0.0084 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8081 seconds and 0.3989 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_427 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0077 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_431 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0081 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_446 0.0084 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8398 seconds and 0.4137 seconds precompiling for 25 choices
[rank6]:W1105 09:58:14.218000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:14.297000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:14.398000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:14.442000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:14.531000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:14.631000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_427 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0066 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_436 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0071 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_431 0.0079 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0079 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_446 0.0083 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7904 seconds and 0.3960 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_426 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0067 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0067 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0072 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0081 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0083 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8349 seconds and 0.4077 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_427 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_436 0.0069 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0073 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_430 0.0081 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0084 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7975 seconds and 0.4079 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_427 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 96.9% 
  triton_bmm_437 0.0069 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0070 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0075 ms 84.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0075 ms 84.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_430 0.0079 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0079 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0083 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8544 seconds and 0.4064 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.8% 
  triton_bmm_436 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0077 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0077 ms 82.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0078 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0078 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0084 ms 75.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7579 seconds and 0.3936 seconds precompiling for 25 choices
[rank5]:W1105 09:58:15.787000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:15.828000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:15.865000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:15.906000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:15.966000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:16.006000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:16.108000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:16.185000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:16.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:16.528000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:16.538000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:16.607000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:16.626000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:16.708000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:16.726000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_427 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_436 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0073 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_431 0.0082 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0082 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_447 0.0083 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7422 seconds and 0.4128 seconds precompiling for 25 choices
[rank4]:W1105 09:58:18.076000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:18.156000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:18.259000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:20.112000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:20.191000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:20.197000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:20.242000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:20.273000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:20.322000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:20.848000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:20.924000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:20.974000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:21.726000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:21.804000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:21.854000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:21.878000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:21.956000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:21.999000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:22.006000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:22.075000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:22.124000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:22.409000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:22.485000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:22.534000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:23.399000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:23.464000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:23.667000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:23.728000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:23.935000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:23.985000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:24.400000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:24.449000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:24.479000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:24.530000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:24.728000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:24.997000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:25.176000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:25.312000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:25.443000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:25.493000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:25.577000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:25.708000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:25.715000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:25.765000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:25.852000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:25.883000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:25.960000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:25.985000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:26.035000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:26.042000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:26.063000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:26.121000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:26.225000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:26.268000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:27.073000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:27.150000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:27.234000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:27.252000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:27.739000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:27.743000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:27.851000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:28.014000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:28.016000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:28.044000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:28.092000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:28.123000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:28.194000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:28.218000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:28.228000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:28.289000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:28.300000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:28.408000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:30.405000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:30.485000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:30.589000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_460 0.0300 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0301 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_451 0.0333 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0342 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0458 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0458 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0507 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0508 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0552 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0688 seconds and 0.2295 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_451 0.0312 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0322 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_450 0.0322 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_460 0.0327 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_458 0.0459 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0474 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0508 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0509 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_454 0.0596 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0615 seconds and 0.2372 seconds precompiling for 25 choices
[rank3]:W1105 09:58:32.927000 286 torch/_dynamo/variables/builtin.py:1091] [70/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31ebb0>
[rank3]:W1105 09:58:32.950000 286 torch/_dynamo/variables/builtin.py:1091] [71/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31efd0>
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_461 0.0303 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0305 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0346 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0349 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0505 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0509 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0511 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0515 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_470 0.0620 ms 15.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0753 seconds and 0.2278 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:58:33 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1105 09:58:33.226000 289 torch/_dynamo/variables/builtin.py:1091] [70/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8733f4f120>
[rank6]:W1105 09:58:33.249000 289 torch/_dynamo/variables/builtin.py:1091] [71/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8734f10ae0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:58:33 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_451 0.0312 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_460 0.0319 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0321 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_461 0.0321 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_459 0.0489 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0489 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_458 0.0491 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0509 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0608 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1118 seconds and 0.2141 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_451 0.0287 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_460 0.0298 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0300 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_450 0.0316 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0457 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0458 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0509 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0510 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0538 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1085 seconds and 0.2249 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_451 0.0297 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0301 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0301 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0315 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0459 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0460 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0504 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0508 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_454 0.0541 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0691 seconds and 0.2348 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_461 0.0300 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0301 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0346 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0350 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0459 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0459 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0505 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0506 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_470 0.0620 ms 15.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1089 seconds and 0.2138 seconds precompiling for 25 choices
[rank1]:W1105 09:58:34.265000 284 torch/_dynamo/variables/builtin.py:1091] [70/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2970>
[rank1]:W1105 09:58:34.289000 284 torch/_dynamo/variables/builtin.py:1091] [71/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2af0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:58:34 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1105 09:58:34.867000 290 torch/_dynamo/variables/builtin.py:1091] [70/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed1d40>
[rank7]:W1105 09:58:34.890000 290 torch/_dynamo/variables/builtin.py:1091] [71/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed2160>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:58:34 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1105 09:58:35.112000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:35.212000 283 torch/_dynamo/variables/builtin.py:1091] [70/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e68e0>
[rank0]:W1105 09:58:35.236000 283 torch/_dynamo/variables/builtin.py:1091] [71/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e6220>
[rank5]:W1105 09:58:35.253000 288 torch/_dynamo/variables/builtin.py:1091] [70/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c61f0>
[rank2]:W1105 09:58:35.267000 285 torch/_dynamo/variables/builtin.py:1091] [70/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f69e30>
[rank5]:W1105 09:58:35.277000 288 torch/_dynamo/variables/builtin.py:1091] [71/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c6490>
[rank2]:W1105 09:58:35.290000 285 torch/_dynamo/variables/builtin.py:1091] [71/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f6a5b0>
[rank3]:W1105 09:58:35.297000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:58:35 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:58:35 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:58:35 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1105 09:58:35.384000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:35.565000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:35.652000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:35.820000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:35.832000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:35.920000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:36.088000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:36.096000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:36.188000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_451 0.0293 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0298 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0302 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0320 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0457 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0458 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0503 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0504 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0537 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0430 seconds and 0.2275 seconds precompiling for 25 choices
[rank1]:W1105 09:58:36.352000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:36.361000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:36.456000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:36.462000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:36.617000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:36.629000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:36.733000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:36.739000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:36.780000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:36.885000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:36.897000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:36.999000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:37.008000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:37.048000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:37.116000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:37.153000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:37.162000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:37.264000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:37.280000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:37.316000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:37.389000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:37.429000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:37.438000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:37.532000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:37.552000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:37.565000 287 torch/_dynamo/variables/builtin.py:1091] [70/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a550>
[rank0]:W1105 09:58:37.584000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:37.589000 287 torch/_dynamo/variables/builtin.py:1091] [71/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a8e0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:58:37 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1105 09:58:37.659000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:37.697000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:37.706000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:37.800000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:37.824000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:37.856000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:37.928000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:37.950000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:37.965000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:37.974000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:38.068000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:38.096000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:38.128000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:38.199000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:38.217000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:38.228000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:38.241000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:38.336000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:38.368000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:38.398000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:38.471000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:38.481000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:38.493000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:38.505000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:38.604000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:38.640000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:38.664000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:38.743000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:38.751000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:38.760000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:38.769000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:38.872000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:38.911000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:38.932000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:39.017000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:39.024000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:39.032000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:39.041000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:39.140000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:39.146000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:39.184000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:39.200000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:39.285000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:39.297000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:39.305000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:39.317000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:39.408000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:39.416000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:39.456000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:39.472000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:39.549000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:39.568000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:39.575000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:39.590000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:39.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:39.684000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:39.728000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:39.748000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:39.825000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:39.840000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:39.852000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:39.861000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:39.944000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:39.952000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:40.000000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:40.016000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:40.089000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:40.112000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:40.121000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:40.129000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:40.212000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:40.219000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:40.272000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:40.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:40.353000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:40.384000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:40.392000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:40.399000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:40.480000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:40.492000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:40.545000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:40.552000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:40.617000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:40.656000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:40.663000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:40.671000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:40.748000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:40.760000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:40.816000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:40.823000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:40.881000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:40.928000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:40.935000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:40.943000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:41.016000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:41.028000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:41.088000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:41.096000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:41.148000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:41.200000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:41.208000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:41.216000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:41.284000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:41.296000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:41.360000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:41.368000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:41.417000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:41.472000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:41.480000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:41.488000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:41.552000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:41.564000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:41.632000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:41.638000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:41.685000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:41.744000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:41.751000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:41.760000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:41.820000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:41.832000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:41.904000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:41.911000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:41.953000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:42.016000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:42.023000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:42.032000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:42.088000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:42.104000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:42.176000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:42.184000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:42.221000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:42.289000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:42.296000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:42.304000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:42.356000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:42.372000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:42.452000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:42.458000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:42.489000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:42.557000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:42.568000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:42.575000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:42.624000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:42.640000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:42.724000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:42.730000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:42.756000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:42.825000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:42.840000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:42.847000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:42.892000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:42.908000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:42.996000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:43.003000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:43.025000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:43.093000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:43.112000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:43.119000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:43.160000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:43.176000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:43.268000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:43.274000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:43.293000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:43.361000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:43.384000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:43.391000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:43.428000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:43.444000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:43.544000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:43.550000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:43.561000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:43.629000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:43.656000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:43.663000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:43.696000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:43.712000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:43.816000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:43.823000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:43.831000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:43.897000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:43.928000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:43.935000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:43.964000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:43.980000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:44.092000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:44.099000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:44.107000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:44.165000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:44.200000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:44.207000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:44.232000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:44.248000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:44.368000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:44.375000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:44.383000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:44.433000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:44.472000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:44.479000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:44.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:44.520000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:44.640000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:44.653000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:44.660000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:44.701000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:44.744000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:44.751000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:44.768000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:44.792000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:44.913000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:44.921000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:44.936000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:44.969000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:45.016000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:45.023000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:45.036000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:45.064000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:45.184000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:45.192000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:45.212000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:45.237000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:45.288000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:45.295000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:45.304000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:45.336000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:45.457000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:45.464000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:45.488000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:45.504000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:45.564000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:45.571000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:45.580000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:45.608000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:45.729000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:45.736000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:45.764000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:45.773000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:45.836000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:45.844000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:45.851000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:45.880000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:46.000000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:46.008000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:46.041000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:46.048000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:46.109000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:46.116000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:46.124000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:46.152000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:46.272000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:46.280000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:46.309000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:46.324000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:46.380000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:46.388000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:46.396000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:46.424000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:46.543000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:46.550000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:46.575000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:46.601000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:46.656000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:46.664000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:46.673000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:46.697000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:46.819000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:46.825000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:46.847000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:46.877000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:46.928000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:46.937000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:46.945000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:46.965000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:47.092000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:47.099000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:47.119000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:47.149000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:47.201000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:47.209000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:47.218000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:47.233000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:47.364000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:47.372000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:47.392000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:47.421000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:47.481000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:47.490000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:47.501000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:47.641000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:47.650000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:47.673000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:47.696000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:47.752000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:47.759000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:47.768000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:47.918000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:47.929000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:47.950000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:48.034000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:48.042000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:48.052000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:48.196000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:48.211000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:48.224000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:48.309000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:48.321000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:48.330000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:48.411000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:48.433000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:48.467000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:48.487000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:48.500000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:48.581000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:48.594000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:48.602000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:48.687000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:48.709000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:48.739000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:48.759000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:48.776000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:48.854000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:48.865000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:48.875000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:48.964000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:48.986000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:49.024000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:49.064000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:49.142000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:49.151000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:49.244000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:49.270000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:49.300000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:49.340000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:49.422000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:49.430000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:49.515000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:49.521000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:49.554000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:49.572000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:49.581000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:49.616000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:49.710000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:49.795000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:49.801000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:49.838000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:49.845000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:49.858000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:49.892000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:49.990000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:50.077000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:50.085000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:50.116000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:50.132000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:50.152000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:50.170000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:50.264000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:50.361000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:50.368000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:50.397000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:50.404000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:50.428000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:50.449000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:50.540000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:50.581000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:50.637000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:50.644000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:50.676000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:50.683000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:50.704000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:50.725000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:50.816000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:50.865000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:50.921000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:50.928000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:58:50.956000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:50.962000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:50.984000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:58:50.997000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:51.093000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:51.140000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:51.191000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:51.203000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:51.237000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:51.261000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:51.369000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:51.415000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:58:51.473000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:51.480000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:51.509000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:51.537000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:51.641000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:51.691000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:51.760000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:51.783000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:51.815000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:51.972000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:52.047000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:58:52.066000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:52.103000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:52.248000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:52.324000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:52.387000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:52.438000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:52.528000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:58:52.603000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:52.669000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:52.717000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:52.803000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:58:52.946000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:53.002000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:53.096000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:53.281000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:53.372000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:53.556000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:58:53.653000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:53.847000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:54.131000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:54.409000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:54.697000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:58:54.972000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_474 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_485 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1088 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1089 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1126 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1126 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1406 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0737 seconds and 0.1823 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_475 0.1043 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1043 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_484 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_493 0.1088 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1089 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1128 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1128 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1407 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0578 seconds and 0.1857 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_475 0.1042 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1046 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1047 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1090 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1091 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1127 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1130 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1408 ms 43.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1157 seconds and 0.1952 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_474 0.1008 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1009 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_485 0.1011 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1012 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1053 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1053 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1085 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1331 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0915 seconds and 0.1808 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_474 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1044 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1090 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1091 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1122 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1122 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_479 0.1407 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0357 seconds and 0.1924 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_485 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_474 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_484 0.1042 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_492 0.1089 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1094 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1124 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1125 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1405 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0908 seconds and 0.1828 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_474 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1090 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1090 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_483 0.1118 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1120 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_478 0.1396 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0622 seconds and 0.1876 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_485 0.1043 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_474 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1044 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1098 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1098 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1133 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1133 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1408 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0514 seconds and 0.2033 seconds precompiling for 25 choices
Capturing batches (bs=8 avail_mem=39.63 GB):  94%|| 49/52 [07:08<03:16, 65.45s/it]Capturing batches (bs=4 avail_mem=39.02 GB):  94%|| 49/52 [07:08<03:16, 65.45s/it][rank1]:W1105 09:59:07.857000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:07.917000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:07.935000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:07.965000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:07.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:07.995000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:08.008000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:08.043000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:08.044000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:08.070000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:08.085000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:08.103000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:08.133000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:08.150000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:08.179000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:08.192000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:08.211000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:08.320000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:08.601000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:08.681000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:08.698000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:08.777000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:08.791000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:08.887000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:09.570000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:09.620000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:09.719000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:09.726000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:09.731000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:09.854000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:10.314000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:10.415000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:11.425000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:11.441000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:11.504000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:11.520000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:11.555000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:11.571000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:11.682000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:11.760000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:11.810000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:12.280000 284 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank5]:W1105 09:59:12.280000 288 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank3]:W1105 09:59:12.291000 286 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank6]:W1105 09:59:12.305000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:12.322000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:12.396000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:12.412000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:12.458000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:12.471000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:12.764000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:12.843000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:12.893000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:12.941000 289 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank4]:W1105 09:59:12.946000 287 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank0]:W1105 09:59:12.966000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:13.050000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:13.109000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:13.370000 285 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank0]:W1105 09:59:13.584000 283 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank7]:W1105 09:59:13.597000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:13.680000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:13.732000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:14.211000 290 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_512 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_514 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_515 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_503 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_516 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_506 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_513 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_500 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8393 seconds and 0.1614 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_518 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_511 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_512 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_517 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_498 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_514 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.7% 
  triton_bmm_515 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_510 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8765 seconds and 0.1318 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_519 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_508 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_507 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_510 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_501 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8867 seconds and 0.1395 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_513 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_516 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_499 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_500 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_502 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_503 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_510 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_511 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8437 seconds and 0.1480 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_497 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_507 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_503 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_505 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_508 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_514 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8499 seconds and 0.1534 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_507 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_509 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_512 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_506 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_510 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0062 ms 98.7% 
  triton_bmm_499 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_500 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_503 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8526 seconds and 0.1474 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_510 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_511 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.3% 
  triton_bmm_506 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_503 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_518 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8215 seconds and 0.1495 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_517 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_498 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_509 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_514 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_515 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_519 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_516 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_496 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_499 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8977 seconds and 0.1577 seconds precompiling for 25 choices
[rank1]:W1105 09:59:22.663000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:22.693000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:23.160000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:23.196000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:23.231000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:23.312000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:23.352000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:23.439000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:23.733000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:23.833000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:23.873000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:23.945000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:24.216000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:24.631000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:24.727000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:25.137000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:25.425000 286 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank1]:W1105 09:59:25.488000 284 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank4]:W1105 09:59:26.062000 287 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank0]:W1105 09:59:26.270000 283 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank6]:W1105 09:59:26.866000 289 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank2]:W1105 09:59:27.320000 285 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank5]:W1105 09:59:27.929000 288 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank7]:W1105 09:59:28.606000 290 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0066 ms 99.4% 
  triton_bmm_523 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_532 0.0069 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0069 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0073 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0075 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0080 ms 81.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0080 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_543 0.0083 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9056 seconds and 0.3942 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_523 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_533 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0073 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0078 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0081 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0082 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_543 0.0084 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8383 seconds and 0.4216 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_522 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_533 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0069 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_530 0.0072 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0073 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_524 0.0084 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8728 seconds and 0.4276 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_523 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_532 0.0069 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0082 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0082 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8356 seconds and 0.4159 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_523 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_533 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0072 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_542 0.0081 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8588 seconds and 0.4152 seconds precompiling for 25 choices
[rank3]:W1105 09:59:32.265000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:32.301000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:32.344000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:32.378000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:32.393000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:32.428000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:32.506000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_522 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_533 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0072 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0073 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0078 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0078 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_543 0.0083 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7821 seconds and 0.4096 seconds precompiling for 25 choices
[rank0]:W1105 09:59:32.583000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:32.633000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:33.085000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:33.129000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:33.164000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_522 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_532 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0080 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7705 seconds and 0.4174 seconds precompiling for 25 choices
[rank6]:W1105 09:59:33.209000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:33.214000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:33.260000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:33.931000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 100.0% 
  triton_bmm_522 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_533 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_530 0.0077 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0078 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0080 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_542 0.0084 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8859 seconds and 0.4094 seconds precompiling for 25 choices
[rank2]:W1105 09:59:34.008000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:34.057000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:34.110000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:34.189000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:34.239000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:35.010000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:35.089000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:35.141000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:37.947000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:38.023000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:38.035000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:38.113000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:38.128000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:38.219000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:38.574000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:38.652000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:38.756000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:38.775000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:38.833000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:38.854000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:38.914000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:38.961000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:39.024000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:39.096000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:39.173000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:39.278000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:39.870000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:39.947000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:40.050000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:40.102000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:40.179000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:40.284000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:40.895000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:41.164000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:41.413000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:41.425000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:41.689000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:41.724000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:41.936000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:41.956000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:42.000000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:42.188000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:42.216000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:42.282000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:42.469000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:42.487000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:42.737000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:42.751000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:42.822000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:43.020000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:43.092000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:43.296000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:43.377000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:43.399000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:43.477000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:43.579000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:43.624000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:43.701000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:43.768000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:43.803000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:43.819000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:43.898000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:44.002000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:44.046000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:44.146000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:44.229000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:44.318000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:44.361000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:45.429000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:45.505000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:45.507000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:45.509000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:45.584000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:45.586000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:45.610000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:45.686000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:45.689000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:45.997000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:46.081000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:46.194000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_556 0.0296 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0297 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_547 0.0346 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0349 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0450 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0479 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_566 0.0613 ms 15.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0633 seconds and 0.2512 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_556 0.0299 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0299 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_547 0.0340 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0346 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0453 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0455 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0479 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0481 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0578 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1103 seconds and 0.2448 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_557 0.0294 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0295 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0329 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0332 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0446 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0447 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_565 0.0479 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0480 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_550 0.0540 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0959 seconds and 0.2145 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_547 0.0297 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0297 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0299 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0319 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0454 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0475 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0480 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0540 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1194 seconds and 0.2284 seconds precompiling for 25 choices
[rank4]:W1105 09:59:51.012000 287 torch/_dynamo/variables/builtin.py:1091] [70/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a550>
[rank4]:W1105 09:59:51.035000 287 torch/_dynamo/variables/builtin.py:1091] [71/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a8e0>
[rank3]:W1105 09:59:51.122000 286 torch/_dynamo/variables/builtin.py:1091] [70/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31ebb0>
[rank3]:W1105 09:59:51.145000 286 torch/_dynamo/variables/builtin.py:1091] [71/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31efd0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:59:51 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1105 09:59:51.267000 283 torch/_dynamo/variables/builtin.py:1091] [70/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e68e0>
[rank0]:W1105 09:59:51.290000 283 torch/_dynamo/variables/builtin.py:1091] [71/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e6220>
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_547 0.0296 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0296 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0303 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_546 0.0314 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0453 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0454 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0481 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0481 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_551 0.0539 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0989 seconds and 0.2173 seconds precompiling for 25 choices
[rank1]:W1105 09:59:51.325000 284 torch/_dynamo/variables/builtin.py:1091] [70/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2970>
[rank1]:W1105 09:59:51.349000 284 torch/_dynamo/variables/builtin.py:1091] [71/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2af0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:59:51 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_556 0.0298 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0308 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0313 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_546 0.0314 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0454 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0483 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0485 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0492 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_551 0.0540 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0798 seconds and 0.2231 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_547 0.0294 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0295 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0295 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_546 0.0323 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0451 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0451 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0480 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0537 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1116 seconds and 0.2141 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:59:51 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:59:51 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_546 0.0319 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_547 0.0323 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0328 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0330 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_565 0.0485 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0492 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_554 0.0495 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0503 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_550 0.0598 ms 15.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1563 seconds and 0.2284 seconds precompiling for 25 choices
[rank5]:W1105 09:59:52.655000 288 torch/_dynamo/variables/builtin.py:1091] [70/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c61f0>
[rank5]:W1105 09:59:52.678000 288 torch/_dynamo/variables/builtin.py:1091] [71/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c6490>
[rank6]:W1105 09:59:52.707000 289 torch/_dynamo/variables/builtin.py:1091] [70/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8733f4f120>
[rank2]:W1105 09:59:52.723000 285 torch/_dynamo/variables/builtin.py:1091] [70/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f69e30>
[rank6]:W1105 09:59:52.730000 289 torch/_dynamo/variables/builtin.py:1091] [71/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8734f10ae0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:59:52 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1105 09:59:52.746000 285 torch/_dynamo/variables/builtin.py:1091] [71/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f6a5b0>
[rank1]:W1105 09:59:52.760000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:59:52 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1105 09:59:53.040000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:53.264000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:53.317000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:59:53 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1105 09:59:53.548000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:53.566000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:53.594000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:53.716000 290 torch/_dynamo/variables/builtin.py:1091] [70/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed1d40>
[rank7]:W1105 09:59:53.739000 290 torch/_dynamo/variables/builtin.py:1091] [71/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed2160>
[rank4]:W1105 09:59:53.828000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:53.858000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:53.873000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:53.942000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:54.121000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:54.140000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:54.152000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:54.221000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 09:59:54 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1105 09:59:54.406000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:54.421000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:54.428000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:54.500000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:54.693000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:54.701000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:54.708000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:54.716000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:54.777000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:54.821000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:54.948000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:54.976000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:54.985000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:54.992000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:55.000000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:55.053000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:55.101000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:55.230000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:55.261000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:55.269000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:55.278000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:55.287000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:55.340000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:55.384000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:55.511000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:55.541000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:55.548000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:55.562000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:55.568000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:55.620000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:55.672000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:55.789000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:55.821000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:55.828000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:55.842000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:55.848000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:55.862000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:55.900000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:55.948000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:56.069000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:56.101000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:56.108000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:56.127000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:56.136000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:56.145000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:56.180000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:56.232000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:56.349000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:56.381000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:56.387000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:56.407000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:56.417000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:56.425000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:56.460000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:56.508000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:56.629000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:56.661000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:56.667000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:56.687000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:56.697000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:56.705000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:56.740000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:56.784000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:56.909000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:56.938000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:56.944000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:56.968000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:56.978000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:56.986000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:57.020000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:57.064000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:57.189000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:57.213000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:57.220000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:57.247000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:57.258000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:57.267000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:57.300000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:57.340000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:57.469000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:57.489000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:57.496000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:57.523000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:57.537000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:57.545000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:57.576000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:57.616000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:57.749000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:57.765000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:57.772000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:57.799000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:57.818000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:57.826000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:57.852000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:57.892000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:58.030000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:58.042000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:58.048000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:58.075000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:58.098000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:58.106000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:58.128000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:58.168000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:58.314000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:58.322000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:58.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:58.351000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:58.378000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:58.386000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:58.404000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:58.444000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:58.594000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:58.602000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:58.608000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:58.631000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:58.657000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:58.666000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:58.680000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:58.720000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:58.874000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:58.882000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:58.888000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:58.907000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:58.938000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:58.946000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:58.956000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:58.996000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:59.154000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:59.161000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:59.176000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:59.195000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:59.218000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:59.225000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:59.244000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:59.280000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:59.434000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:59.442000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:59.455000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:59.471000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:59.498000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:59.506000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:59.524000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:59.564000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:59.712000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 09:59:59.720000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 09:59:59.733000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 09:59:59.749000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 09:59:59.776000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 09:59:59.783000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 09:59:59.801000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 09:59:59.841000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 09:59:59.996000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:00.002000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:00.010000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:00.024000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:00.060000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:00.067000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:00.077000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:00.113000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:00.276000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:00.285000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:00.292000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:00.301000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:00.336000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:00.348000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:00.356000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:00.385000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:00.560000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:00.567000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:00.574000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:00.582000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:00.612000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:00.628000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:00.636000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:00.657000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:00.844000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:00.851000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:00.859000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:00.866000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:00.888000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:00.908000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:00.916000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:00.929000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:01.124000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:01.131000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:01.140000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:01.147000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:01.164000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:01.188000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:01.197000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:01.206000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:01.408000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:01.415000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:01.422000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:01.430000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:01.440000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:01.470000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:01.477000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:01.487000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:01.692000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:01.699000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:01.707000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:01.714000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:01.722000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:01.748000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:01.756000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:01.766000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:01.977000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:01.984000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:01.991000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:01.999000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:02.006000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:02.028000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:02.036000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:02.046000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:02.254000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:02.269000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:02.275000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:02.283000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:02.290000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:02.308000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:02.316000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:02.326000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:02.533000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:02.545000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:02.560000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:02.567000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:02.573000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:02.588000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:02.596000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:02.606000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:02.809000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:02.821000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:02.844000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:02.850000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:02.858000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:02.868000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:02.876000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:02.886000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:03.089000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:03.097000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:03.128000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:03.134000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:03.142000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:03.151000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:03.159000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:03.169000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:03.365000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:03.373000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:03.404000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:03.416000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:03.424000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:03.433000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:03.442000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:03.450000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:03.641000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:03.649000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:03.680000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:03.696000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:03.704000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:03.713000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:03.722000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:03.731000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:03.917000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:03.925000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:03.956000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:03.976000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:03.984000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:03.996000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:04.004000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:04.014000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:04.193000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:04.204000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:04.236000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:04.256000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:04.264000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:04.276000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:04.293000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:04.303000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:04.470000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:04.485000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:04.524000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:04.536000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:04.544000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:04.556000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:04.575000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:04.583000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:04.749000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:04.765000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:04.804000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:04.816000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:04.824000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:04.836000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:04.853000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:04.863000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:05.029000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:05.045000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:05.084000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:05.096000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:05.104000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:05.116000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:05.133000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:05.143000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:05.309000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:05.325000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:05.364000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:05.376000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:05.388000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:05.396000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:05.413000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:05.423000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:05.590000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:05.605000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:05.644000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:05.656000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:05.672000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:05.680000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:05.693000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:05.703000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:05.870000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:05.885000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:05.924000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:05.936000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:05.956000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:05.964000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:05.975000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:05.983000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:06.150000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:06.165000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:06.208000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:06.225000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:06.245000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:06.255000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:06.263000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:06.272000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:06.429000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:06.445000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:06.496000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:06.512000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:06.533000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:06.540000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:06.551000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:06.559000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:06.709000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:06.725000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:06.776000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:06.796000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:06.814000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:06.824000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:06.832000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:06.841000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:06.989000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:07.005000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:07.056000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:07.076000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:07.105000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:07.112000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:07.123000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:07.131000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:07.269000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:07.285000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:07.336000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:07.360000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:07.386000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:07.396000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:07.404000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:07.413000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:07.549000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:07.565000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:07.616000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:07.644000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:07.665000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:07.681000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:07.689000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:07.697000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:07.829000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:07.845000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:07.896000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:07.928000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:07.945000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:07.957000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:07.972000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:07.981000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:08.109000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:08.125000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:08.176000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:08.212000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:08.225000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:08.237000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:08.256000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:08.265000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:08.389000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:08.405000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:08.456000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:08.496000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:08.506000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:08.515000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:08.541000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:08.552000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:08.684000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:08.736000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:08.780000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:08.788000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:08.797000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:08.828000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:08.837000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:08.965000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:09.016000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:09.064000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:09.072000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:09.081000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:09.113000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:09.122000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:09.261000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:09.296000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:09.348000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:09.362000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:09.372000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:09.405000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:09.582000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:09.637000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:09.664000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:09.672000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:09.698000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:09.865000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:09.925000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:09.958000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:09.986000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:10.157000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:10.222000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:10.248000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:10.282000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:10.442000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:10.509000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:10.530000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:10.569000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:10.724000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:10.801000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:11.008000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:11.285000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:11.577000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_570 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1044 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1045 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1084 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1084 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1121 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1121 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_574 0.1338 ms 45.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1612 seconds and 0.1977 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_581 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_570 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_589 0.1083 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1084 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1117 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1119 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_574 0.1334 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0826 seconds and 0.1907 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_570 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_581 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_571 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_588 0.1083 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1083 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1117 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1117 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1332 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0968 seconds and 0.1893 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_571 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_570 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_581 0.1045 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1045 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1084 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1085 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_579 0.1121 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1121 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_574 0.1335 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0965 seconds and 0.1846 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_571 0.1040 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_570 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_581 0.1042 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1042 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1082 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1083 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1114 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1114 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1323 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0970 seconds and 0.1901 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_580 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_571 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_570 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_588 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1089 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_579 0.1120 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1122 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_575 0.1337 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0854 seconds and 0.2016 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_570 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_580 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_571 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_588 0.1090 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1090 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1119 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1119 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1333 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0273 seconds and 0.1985 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_571 0.1009 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_570 0.1010 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_580 0.1011 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1011 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1052 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1055 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1078 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1079 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1268 ms 47.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0538 seconds and 0.1803 seconds precompiling for 25 choices
Capturing batches (bs=4 avail_mem=39.02 GB):  96%|| 50/52 [08:25<02:17, 68.83s/it]Capturing batches (bs=2 avail_mem=38.42 GB):  96%|| 50/52 [08:25<02:17, 68.83s/it][rank6]:W1105 10:00:24.532000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:24.583000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:24.616000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:24.663000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:24.730000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:24.775000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:24.827000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:24.906000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:24.927000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:25.007000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:25.015000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:25.118000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:25.373000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:25.451000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:25.475000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:25.494000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:25.556000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:25.561000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:25.575000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:25.661000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:25.666000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:25.685000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:25.738000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:25.847000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:26.273000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:26.326000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:26.541000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:26.650000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:27.084000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:27.188000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:27.216000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:27.380000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:27.806000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:27.889000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:27.943000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:28.022000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:28.105000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:28.157000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:28.306000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:28.387000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:28.439000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:28.536000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:28.616000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:28.656000 289 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank7]:W1105 10:00:28.667000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:28.758000 287 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank1]:W1105 10:00:28.772000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:28.858000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:28.913000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:28.926000 288 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank0]:W1105 10:00:28.996000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:29.084000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:29.139000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:29.162000 290 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank1]:W1105 10:00:29.411000 284 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank3]:W1105 10:00:29.488000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:29.527000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:29.574000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:29.610000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:29.627000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:29.646000 283 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank2]:W1105 10:00:29.662000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:30.126000 286 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank2]:W1105 10:00:30.157000 285 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_592 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_596 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_603 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_612 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_602 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_604 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_606 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_607 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8297 seconds and 0.1482 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_603 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_611 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_612 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_609 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_596 0.0066 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_598 0.0066 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_599 0.0066 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_602 0.0066 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_605 0.0066 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8769 seconds and 0.1497 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_613 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_610 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_612 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_604 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_611 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_595 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_597 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_603 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_606 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8886 seconds and 0.1578 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_610 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_611 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_612 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.7% 
  triton_bmm_609 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_596 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_599 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_608 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_614 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_605 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9104 seconds and 0.1505 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_605 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_602 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_604 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_596 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_606 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_595 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_598 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_603 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8444 seconds and 0.1500 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0062 ms 100.0% 
  triton_bmm_598 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_599 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_605 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_596 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_602 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_603 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_604 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_614 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8165 seconds and 0.1375 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_599 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_615 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0063 ms 99.4% 
  triton_bmm_594 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_602 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_598 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_593 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_596 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_613 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_592 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8914 seconds and 0.1354 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0061 ms 100.0% 
  triton_bmm_596 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_595 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_598 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_602 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_609 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_611 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_594 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8799 seconds and 0.1517 seconds precompiling for 25 choices
[rank4]:W1105 10:00:38.935000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:38.956000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:39.336000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:39.439000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:39.463000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:39.841000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:39.958000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:40.034000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:40.338000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:40.434000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:40.467000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:40.489000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:40.545000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:40.856000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:40.946000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:41.002000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:42.425000 284 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank4]:W1105 10:00:42.455000 287 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank6]:W1105 10:00:42.469000 289 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank5]:W1105 10:00:42.917000 288 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank7]:W1105 10:00:43.214000 290 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank0]:W1105 10:00:43.553000 283 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank2]:W1105 10:00:43.952000 285 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank3]:W1105 10:00:44.060000 286 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_618 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_629 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0070 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_623 0.0081 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0081 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_638 0.0082 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8184 seconds and 0.4288 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_619 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 99.4% 
  triton_bmm_618 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_628 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0069 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_623 0.0078 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0079 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_638 0.0081 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8844 seconds and 0.4265 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_619 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_628 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8688 seconds and 0.4215 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_619 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_629 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_623 0.0079 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0079 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_620 0.0083 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8100 seconds and 0.4242 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_628 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0077 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0077 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_623 0.0078 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0078 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_638 0.0084 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8701 seconds and 0.4062 seconds precompiling for 25 choices
[rank4]:W1105 10:00:48.696000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:48.746000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:48.777000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:48.826000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:48.827000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:48.841000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_619 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_629 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0080 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_639 0.0083 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8255 seconds and 0.4066 seconds precompiling for 25 choices
[rank6]:W1105 10:00:48.877000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:48.919000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:48.968000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:49.155000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:49.236000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:49.287000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_619 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.8% 
  triton_bmm_618 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_628 0.0068 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0068 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0078 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0078 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8701 seconds and 0.4207 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.8% 
  triton_bmm_629 0.0068 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0075 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0075 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0080 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0080 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_639 0.0081 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8771 seconds and 0.4073 seconds precompiling for 25 choices
[rank7]:W1105 10:00:49.596000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:49.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:49.728000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:49.806000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:49.897000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:49.948000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:50.341000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:50.421000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:50.447000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:50.472000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:50.527000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:50.577000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:54.467000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:54.544000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:54.632000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:54.651000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:54.704000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:54.713000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:54.781000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:54.822000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:54.886000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:55.209000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:55.287000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:55.364000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:55.394000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:55.442000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:55.546000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:55.658000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:55.738000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:55.846000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:56.059000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:56.137000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:56.199000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:56.244000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:56.278000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:56.384000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:57.634000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:57.911000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:58.041000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:58.133000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:58.187000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:58.322000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:58.330000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:58.413000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:58.580000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:58.601000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:00:58.609000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:00:58.693000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:58.866000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:00:58.877000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:59.037000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:00:59.147000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:59.319000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:59.440000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:59.560000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:00:59.603000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:00:59.724000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:59.730000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:59.809000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:00:59.852000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:00:59.912000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:00.004000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:00.140000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:00.184000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:00.263000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:00.366000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:00.552000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:00.634000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:00.740000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:00.941000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:00.990000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:01.021000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:01.068000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:01.127000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:01.178000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:01.446000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:01.528000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:01.580000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:01.647000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:01.665000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:01.702000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:01.772000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:01.782000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:01.885000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_652 0.0291 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0291 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0346 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0349 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_651 0.0443 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0447 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0447 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0452 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_662 0.0594 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1275 seconds and 0.2260 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_652 0.0295 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0306 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0317 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_653 0.0318 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0465 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_661 0.0466 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0472 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0489 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0546 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1189 seconds and 0.2448 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_653 0.0316 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0317 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0321 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0325 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_660 0.0464 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0473 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0485 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0492 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_646 0.0599 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1621 seconds and 0.2240 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0089 ms 100.0% 
  triton_mm_653 0.0291 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0292 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0295 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0315 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0440 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0443 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0446 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0447 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_646 0.0538 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0873 seconds and 0.2360 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0089 ms 100.0% 
  triton_mm_653 0.0293 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0294 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0295 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0309 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0440 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_660 0.0443 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0445 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0449 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0538 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0979 seconds and 0.2173 seconds precompiling for 25 choices
[rank1]:W1105 10:01:07.036000 284 torch/_dynamo/variables/builtin.py:1091] [70/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2970>
[rank1]:W1105 10:01:07.059000 284 torch/_dynamo/variables/builtin.py:1091] [71/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2af0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:01:07 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1105 10:01:07.288000 289 torch/_dynamo/variables/builtin.py:1091] [70/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8733f4f120>
[rank6]:W1105 10:01:07.311000 289 torch/_dynamo/variables/builtin.py:1091] [71/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8734f10ae0>
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_643 0.0291 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_653 0.0299 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0303 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_642 0.0308 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_651 0.0443 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_661 0.0447 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0448 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0449 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0544 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1106 seconds and 0.2197 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:01:07 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_652 0.0296 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0301 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0337 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0352 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_661 0.0449 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0450 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0455 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0460 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_662 0.0599 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1323 seconds and 0.2210 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_653 0.0293 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0302 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0348 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0350 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_660 0.0451 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0454 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0460 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0464 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_663 0.0589 ms 15.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0700 seconds and 0.2206 seconds precompiling for 25 choices
[rank7]:W1105 10:01:07.995000 290 torch/_dynamo/variables/builtin.py:1091] [70/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed1d40>
[rank7]:W1105 10:01:08.018000 290 torch/_dynamo/variables/builtin.py:1091] [71/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed2160>
[rank5]:W1105 10:01:08.057000 288 torch/_dynamo/variables/builtin.py:1091] [70/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c61f0>
[rank5]:W1105 10:01:08.080000 288 torch/_dynamo/variables/builtin.py:1091] [71/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c6490>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:01:08 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:01:08 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1105 10:01:08.323000 287 torch/_dynamo/variables/builtin.py:1091] [70/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a550>
[rank4]:W1105 10:01:08.346000 287 torch/_dynamo/variables/builtin.py:1091] [71/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a8e0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:01:08 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1105 10:01:08.527000 283 torch/_dynamo/variables/builtin.py:1091] [70/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e68e0>
[rank0]:W1105 10:01:08.550000 283 torch/_dynamo/variables/builtin.py:1091] [71/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e6220>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:01:08 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1105 10:01:08.741000 285 torch/_dynamo/variables/builtin.py:1091] [70/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f69e30>
[rank2]:W1105 10:01:08.764000 285 torch/_dynamo/variables/builtin.py:1091] [71/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f6a5b0>
[rank1]:W1105 10:01:08.765000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:01:08 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1105 10:01:08.833000 286 torch/_dynamo/variables/builtin.py:1091] [70/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31ebb0>
[rank3]:W1105 10:01:08.857000 286 torch/_dynamo/variables/builtin.py:1091] [71/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31efd0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:01:08 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1105 10:01:09.048000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:09.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:09.577000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:09.624000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:09.682000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:09.862000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:09.916000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:09.974000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:10.098000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:10.104000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:10.142000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:10.200000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:10.258000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:10.294000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:10.385000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:10.393000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:10.400000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:10.422000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:10.480000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:10.542000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:10.578000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:10.669000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:10.680000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:10.686000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:10.704000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:10.760000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:10.826000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:10.858000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:10.952000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:10.961000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:10.970000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:10.979000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:10.989000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:11.041000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:11.113000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:11.144000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:11.240000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:11.250000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:11.259000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:11.268000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:11.277000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:11.321000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:11.401000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:11.432000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:11.528000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:11.537000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:11.545000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:11.555000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:11.565000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:11.602000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:11.689000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:11.721000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:11.817000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:11.825000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:11.835000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:11.846000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:11.882000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:11.987000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:12.014000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:12.110000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:12.118000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:12.126000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:12.136000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:12.279000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:12.302000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:12.396000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:12.411000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:12.419000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:12.430000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:12.501000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:12.565000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:12.589000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:12.684000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:12.698000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:12.709000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:12.785000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:12.838000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:12.853000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:12.877000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:12.975000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:12.989000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:13.070000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:13.128000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:13.143000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:13.163000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:13.281000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:13.359000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:13.414000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:13.423000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:13.570000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:13.653000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:13.673000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:13.702000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:13.713000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:13.858000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:13.945000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:13.961000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:13.985000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:13.994000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:14.003000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:14.123000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:14.151000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:14.230000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:14.252000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:14.277000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:14.287000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:14.296000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:14.413000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:14.437000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:14.516000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:14.545000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:14.567000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:14.575000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:14.586000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:14.705000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:14.725000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:14.800000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:14.830000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:14.838000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:14.862000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:14.870000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:14.881000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:14.997000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:15.015000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:15.090000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:15.116000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:15.124000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:15.148000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:15.162000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:15.169000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:15.291000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:15.306000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:15.378000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:15.404000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:15.411000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:15.438000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:15.453000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:15.461000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:15.581000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:15.592000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:15.664000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:15.694000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:15.702000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:15.726000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:15.745000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:15.753000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:15.873000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:15.884000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:15.952000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:15.981000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:15.991000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:16.014000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:16.038000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:16.046000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:16.165000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:16.176000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:16.240000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:16.270000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:16.279000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:16.302000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:16.322000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:16.341000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:16.462000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:16.474000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:16.530000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:16.556000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:16.564000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:16.588000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:16.604000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:16.634000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:16.754000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:16.766000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:16.814000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:16.856000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:16.863000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:16.884000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:16.900000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:16.926000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:17.046000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:17.058000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:17.098000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:17.144000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:17.151000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:17.172000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:17.184000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:17.218000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:17.334000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:17.346000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:17.382000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:17.433000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:17.439000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:17.460000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:17.468000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:17.510000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:17.622000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:17.634000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:17.670000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:17.720000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:17.728000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:17.748000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:17.755000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:17.802000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:17.910000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:17.922000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:17.954000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:18.008000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:18.015000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:18.033000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:18.040000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:18.093000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:18.197000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:18.208000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:18.236000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:18.298000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:18.306000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:18.322000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:18.334000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:18.385000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:18.488000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:18.500000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:18.520000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:18.586000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:18.595000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:18.606000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:18.617000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:18.677000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:18.784000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:18.792000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:18.804000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:18.870000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:18.882000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:18.891000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:18.901000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:18.965000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:19.076000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:19.084000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:19.092000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:19.156000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:19.168000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:19.176000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:19.185000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:19.258000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:19.366000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:19.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:19.383000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:19.444000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:19.455000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:19.464000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:19.472000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:19.545000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:19.653000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:19.660000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:19.668000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:19.734000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:19.750000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:19.759000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:19.767000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:19.833000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:19.940000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:19.948000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:19.955000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:20.021000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:20.034000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:20.046000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:20.055000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:20.125000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:20.229000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:20.236000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:20.243000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:20.310000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:20.319000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:20.329000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:20.339000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:20.417000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:20.516000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:20.524000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:20.531000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:20.598000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:20.607000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:20.615000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:20.626000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:20.709000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:20.809000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:20.816000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:20.823000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:20.882000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:20.892000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:20.903000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:20.912000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:21.009000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:21.100000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:21.112000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:21.118000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:21.165000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:21.178000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:21.190000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:21.199000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:21.301000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:21.392000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:21.398000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:21.408000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:21.454000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:21.464000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:21.475000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:21.485000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:21.593000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:21.680000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:21.686000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:21.696000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:21.742000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:21.751000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:21.762000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:21.771000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:21.885000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:21.968000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:21.975000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:21.984000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:22.038000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:22.050000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:22.060000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:22.070000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:22.179000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:22.258000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:22.266000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:22.275000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:22.324000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:22.332000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:22.344000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:22.353000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:22.470000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:22.546000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:22.554000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:22.563000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:22.613000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:22.620000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:22.633000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:22.640000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:22.762000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:22.834000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:22.842000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:22.851000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:22.900000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:22.908000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:22.921000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:22.928000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:23.054000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:23.122000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:23.130000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:23.139000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:23.189000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:23.196000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:23.207000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:23.216000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:23.346000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:23.410000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:23.418000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:23.427000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:23.476000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:23.484000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:23.495000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:23.504000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:23.638000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:23.698000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:23.706000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:23.715000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:23.764000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:23.772000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:23.783000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:23.792000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:23.930000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:23.986000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:23.994000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:24.003000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:24.053000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:24.060000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:24.071000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:24.080000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:24.222000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:24.274000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:24.282000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:24.291000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:24.340000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:24.348000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:24.361000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:24.368000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:24.515000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:24.562000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:24.571000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:24.579000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:24.629000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:24.636000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:24.647000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:24.656000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:24.807000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:24.850000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:24.859000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:24.867000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:24.920000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:24.927000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:24.935000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:24.944000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:25.102000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:25.138000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:25.147000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:25.155000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:25.208000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:25.216000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:25.224000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:25.233000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:25.398000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:25.430000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:25.438000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:25.446000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:25.496000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:25.504000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:25.512000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:25.521000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:25.690000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:25.718000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:25.726000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:25.735000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:25.784000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:25.799000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:25.807000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:25.982000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:26.009000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:26.017000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:26.027000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:26.072000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:26.088000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:26.096000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:26.274000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:26.298000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:26.307000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:26.315000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:26.360000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:26.376000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:26.384000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:26.567000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:26.586000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:26.602000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:26.653000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:26.665000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:26.676000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:26.870000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:26.898000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:26.944000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:26.956000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:26.968000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:27.163000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:27.187000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:27.244000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:27.254000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:27.546000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:27.837000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_677 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_667 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_685 0.1069 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1069 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1118 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1120 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1296 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1511 seconds and 0.1905 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_667 0.1007 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1007 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_676 0.1007 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1008 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1033 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1035 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1076 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1077 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1232 ms 48.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1201 seconds and 0.1889 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_677 0.1041 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1041 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_667 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_684 0.1068 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1069 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1118 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1119 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1295 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1778 seconds and 0.1795 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0601 ms 100.0% 
  triton_mm_677 0.1045 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1045 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_676 0.1045 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_667 0.1046 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1069 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1069 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1114 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1115 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1290 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1062 seconds and 0.1963 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_677 0.1040 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1068 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1068 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1115 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1115 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1292 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0993 seconds and 0.1833 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_677 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1043 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1043 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_685 0.1068 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1069 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1116 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1116 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1289 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1039 seconds and 0.2047 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_666 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_676 0.1045 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1045 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1070 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1072 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1118 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1118 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1294 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0849 seconds and 0.1964 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0600 ms 100.0% 
  triton_mm_677 0.1039 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1039 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1041 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_676 0.1041 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_684 0.1067 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1068 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1110 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1111 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1280 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1032 seconds and 0.2054 seconds precompiling for 25 choices
Capturing batches (bs=2 avail_mem=38.42 GB):  98%|| 51/52 [09:42<01:11, 71.12s/it]Capturing batches (bs=1 avail_mem=37.82 GB):  98%|| 51/52 [09:42<01:11, 71.12s/it][rank5]:W1105 10:01:40.418000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:40.498000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:40.572000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:40.589000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:40.610000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:40.651000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:40.668000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:40.736000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:40.760000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:40.778000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:40.778000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:40.818000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:40.857000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:40.930000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:40.966000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:41.122000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:41.201000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:41.235000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:41.309000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:41.315000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:41.425000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:41.492000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:41.571000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:41.681000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:42.080000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:42.239000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:42.246000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:42.403000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:42.422000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:42.788000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:42.896000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:43.130000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:43.657000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:43.741000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:43.788000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:43.828000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:43.848000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:43.869000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:43.911000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:43.960000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:43.974000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:44.018000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:44.041000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:44.118000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:44.145000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:44.174000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:44.200000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:44.255000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:44.306000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:44.359000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:44.425000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:44.507000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:44.612000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:44.969000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:45.051000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:45.156000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:49.575000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:49.698000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:49.860000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:49.994000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:50.082000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:50.154000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:50.208000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:50.370000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:50.511000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:50.638000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:50.669000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:50.692000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:50.899000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:51.177000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:51.194000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:51.411000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:53.173000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:53.215000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:53.254000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:53.295000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:53.308000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:53.346000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:53.385000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:53.464000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:53.515000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:53.561000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:53.641000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:53.693000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:53.922000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:54.005000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:54.057000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:54.575000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:54.659000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:54.712000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:55.949000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:56.028000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:56.032000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:01:56.080000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:56.112000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:01:56.164000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:57.702000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:57.781000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:01:57.888000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:58.203000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:58.281000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:58.285000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:58.360000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:01:58.395000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:01:58.467000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:58.575000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:58.655000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:58.739000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:01:58.764000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:58.818000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:01:58.923000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:59.097000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:59.177000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:01:59.285000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:00.365000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:00.437000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:00.515000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:00.620000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:00.653000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:00.920000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:00.941000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:00.999000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:01.105000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:01.257000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:01.480000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:01.516000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:01.556000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:01.772000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:01.808000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:01.817000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:01.844000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:02.065000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:02.096000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:02.105000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:02.329000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:02.390000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:02.408000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:02.427000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:02.511000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:02.722000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:03.014000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:03.090000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:03.376000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:03.468000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:03.541000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:03.546000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:03.585000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:03.610000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:03.619000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:03.648000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:03.672000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:03.690000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:03.724000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:03.794000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:03.877000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:04.090000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:04.168000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:04.169000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:04.272000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:04.429000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:04.508000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:04.612000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:05.225000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:05.305000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:05.409000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:05.576000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:05.656000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:05.761000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:07.509000 286 torch/_dynamo/variables/builtin.py:1091] [70/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31ebb0>
[rank1]:W1105 10:02:07.522000 284 torch/_dynamo/variables/builtin.py:1091] [70/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2970>
[rank3]:W1105 10:02:07.532000 286 torch/_dynamo/variables/builtin.py:1091] [71/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f536f31efd0>
[rank1]:W1105 10:02:07.546000 284 torch/_dynamo/variables/builtin.py:1091] [71/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f121d4b2af0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:07 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:07 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1105 10:02:08.148000 283 torch/_dynamo/variables/builtin.py:1091] [70/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e68e0>
[rank0]:W1105 10:02:08.172000 283 torch/_dynamo/variables/builtin.py:1091] [71/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af70e6220>
[rank5]:W1105 10:02:08.195000 288 torch/_dynamo/variables/builtin.py:1091] [70/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c61f0>
[rank5]:W1105 10:02:08.219000 288 torch/_dynamo/variables/builtin.py:1091] [71/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f6ea07c6490>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:08 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:08 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1105 10:02:08.454000 287 torch/_dynamo/variables/builtin.py:1091] [70/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a550>
[rank4]:W1105 10:02:08.477000 287 torch/_dynamo/variables/builtin.py:1091] [71/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4eb6b7a8e0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:08 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1105 10:02:09.112000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:09.143000 290 torch/_dynamo/variables/builtin.py:1091] [70/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed1d40>
[rank7]:W1105 10:02:09.166000 290 torch/_dynamo/variables/builtin.py:1091] [71/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ef5c1ed2160>
[rank2]:W1105 10:02:09.196000 285 torch/_dynamo/variables/builtin.py:1091] [70/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f69e30>
[rank2]:W1105 10:02:09.220000 285 torch/_dynamo/variables/builtin.py:1091] [71/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9af0f6a5b0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:09 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:09 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1105 10:02:09.412000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:09.420000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:09.582000 289 torch/_dynamo/variables/builtin.py:1091] [70/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8733f4f120>
[rank6]:W1105 10:02:09.605000 289 torch/_dynamo/variables/builtin.py:1091] [71/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8734f10ae0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:09 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1105 10:02:09.700000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:09.712000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:09.731000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:09.988000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:10.000000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:10.024000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:10.114000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:10.280000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:10.296000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:10.316000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:10.414000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:10.526000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:10.572000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:10.589000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:10.608000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:10.710000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:10.785000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:10.818000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:10.864000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:10.880000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:10.900000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:11.006000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:11.066000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:11.080000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:11.110000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:11.156000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:11.172000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:11.192000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:11.203000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:11.302000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:11.361000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:11.374000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:11.401000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:11.450000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:11.466000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:11.485000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:11.497000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:11.597000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:11.652000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:11.666000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:11.692000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:11.742000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:11.758000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:11.777000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:11.793000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:11.893000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:11.944000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:11.958000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:11.984000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:12.033000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:12.050000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:12.069000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:12.089000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:12.189000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:12.240000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:12.250000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:12.276000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:12.322000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:12.342000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:12.361000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:12.385000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:12.485000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:12.533000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:12.542000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:12.572000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:12.614000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:12.634000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:12.653000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:12.681000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:12.783000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:12.827000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:12.834000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:12.870000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:12.904000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:12.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:12.944000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:12.979000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:13.078000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:13.119000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:13.128000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:13.162000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:13.196000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:13.217000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:13.236000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:13.271000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:13.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:13.414000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:13.424000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:13.454000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:13.488000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:13.508000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:13.532000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:13.563000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:13.671000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:13.706000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:13.720000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:13.746000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:13.782000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:13.800000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:13.824000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:13.859000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:13.966000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:13.998000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:14.024000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:14.038000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:14.076000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:14.093000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:14.120000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:14.155000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:14.262000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:14.290000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:14.320000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:14.330000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:14.380000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:14.389000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:14.416000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:14.451000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:14.559000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:14.582000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:14.616000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:14.625000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:14.672000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:14.684000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:14.712000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:14.746000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:14.855000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:14.878000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:14.912000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:14.921000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:14.964000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:14.976000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:15.008000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:15.042000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:15.150000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:15.170000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:15.208000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:15.217000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:15.256000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:15.268000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:15.304000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:15.339000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:15.454000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:15.466000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:15.504000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:15.513000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:15.556000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:15.565000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:15.600000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:15.635000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:15.750000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:15.760000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:15.800000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:15.814000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:15.848000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:15.860000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:15.896000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:15.930000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:16.050000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:16.059000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:16.096000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:16.106000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:16.140000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:16.152000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:16.192000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:16.227000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:16.346000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:16.356000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:16.400000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:16.409000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:16.444000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:16.456000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:16.496000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:16.539000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:16.647000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:16.655000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:16.696000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:16.706000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:16.736000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:16.748000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:16.792000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:16.839000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:16.947000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:16.963000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:16.992000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:17.014000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:17.028000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:17.040000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:17.088000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:17.147000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:17.243000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:17.266000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:17.292000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:17.314000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:17.324000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:17.333000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:17.384000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:17.447000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:17.534000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:17.562000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:17.588000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:17.611000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:17.618000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:17.628000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:17.680000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:17.746000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:17.826000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:17.862000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:17.884000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:17.907000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:17.914000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:17.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:17.976000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:18.047000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:18.118000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:18.158000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:18.180000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:18.202000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:18.210000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:18.220000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:18.272000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:18.347000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:18.410000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:18.462000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:18.476000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:18.497000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:18.507000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:18.518000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:18.570000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:18.645000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:18.705000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:18.760000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:18.774000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:18.796000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:18.805000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:18.816000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:18.865000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:18.947000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:18.999000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:19.062000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:19.073000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:19.094000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:19.113000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:19.165000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:19.251000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:19.295000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:19.362000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:19.373000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:19.390000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:19.413000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:19.464000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:19.561000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:19.589000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:19.657000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:19.675000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:19.685000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:19.711000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:19.861000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:19.885000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:19.906000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:19.959000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:19.978000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:19.988000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:20.163000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:20.178000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:20.209000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:20.258000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:20.281000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:20.290000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:20.463000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:20.475000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:20.508000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:20.583000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:20.602000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:20.778000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:20.785000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:20.810000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:20.823000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:20.902000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:21.086000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:21.126000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:21.138000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:21.206000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:21.409000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:21.417000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:21.427000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:21.438000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:21.506000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:21.690000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:21.717000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:21.724000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:21.733000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:21.744000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:21.805000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:21.901000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:21.986000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:22.021000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:22.029000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:22.037000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:22.049000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:22.102000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:22.197000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:22.206000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:22.286000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:22.325000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:22.332000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:22.346000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:22.357000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:22.410000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:22.493000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:22.505000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:22.602000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:22.628000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:22.635000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:22.654000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:22.666000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:22.706000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:22.793000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:22.805000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:22.906000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:22.932000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:22.939000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:22.950000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:22.962000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:23.006000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:23.093000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:23.109000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:23.206000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:23.236000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:23.243000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:23.253000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:23.264000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:23.305000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:23.393000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:23.413000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:23.510000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:23.540000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:23.547000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:23.557000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:23.568000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:23.605000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:23.693000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:23.717000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:23.810000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:23.844000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:23.851000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:23.861000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:23.874000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:23.905000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:23.993000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:24.021000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:24.121000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:24.154000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:24.163000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:24.174000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:24.185000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:24.204000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:24.295000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:24.327000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:24.432000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:24.459000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:24.468000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:24.480000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:24.489000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:24.504000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:24.595000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:24.631000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:24.736000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:24.762000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:24.772000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:24.780000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:24.789000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:24.804000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:24.895000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:24.935000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:25.040000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:25.065000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:25.073000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:25.083000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:25.093000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:25.105000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:25.193000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:25.241000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:25.342000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:25.372000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:25.379000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:25.389000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:25.400000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:25.410000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:25.493000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:25.549000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:25.646000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:25.683000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:25.693000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:25.700000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:25.709000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:25.718000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:25.795000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:25.855000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:25.961000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:25.990000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:25.999000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:26.007000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:26.016000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:26.024000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:26.093000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:26.158000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:26.278000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:26.293000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:26.300000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:26.318000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:26.331000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:26.340000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:26.389000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:26.465000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:26.582000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:26.600000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:26.607000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1105 10:02:26.616000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:26.630000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:26.640000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:26.691000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:26.775000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:26.885000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:26.907000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:26.916000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1105 10:02:26.928000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:26.937000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:26.991000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:27.083000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:27.190000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:27.214000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:27.222000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1105 10:02:27.241000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:27.289000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:27.389000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:27.490000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:27.518000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:27.535000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:27.589000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:27.697000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:27.794000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:27.823000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1105 10:02:27.843000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:27.895000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:28.003000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:28.097000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1105 10:02:28.131000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:28.191000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:28.302000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1105 10:02:28.396000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1105 10:02:28.486000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:28.602000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1105 10:02:28.906000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
Capturing batches (bs=1 avail_mem=37.82 GB): 100%|| 52/52 [10:39<00:00, 67.13s/it]Capturing batches (bs=1 avail_mem=37.82 GB): 100%|| 52/52 [10:39<00:00, 12.31s/it]
[2025-11-05 10:02:35 TP0] Registering 6396 cuda graph addresses
[2025-11-05 10:02:36 TP7] Capture cuda graph end. Time elapsed: 642.00 s. mem usage=6.17 GB. avail mem=37.12 GB.
[2025-11-05 10:02:36 TP1] Capture cuda graph end. Time elapsed: 642.07 s. mem usage=6.13 GB. avail mem=37.03 GB.
[2025-11-05 10:02:36 TP2] Capture cuda graph end. Time elapsed: 642.08 s. mem usage=6.26 GB. avail mem=36.89 GB.
[2025-11-05 10:02:36 TP3] Capture cuda graph end. Time elapsed: 642.10 s. mem usage=6.16 GB. avail mem=37.01 GB.
[2025-11-05 10:02:36 TP0] Capture cuda graph end. Time elapsed: 642.21 s. mem usage=6.19 GB. avail mem=37.39 GB.
[2025-11-05 10:02:36 TP5] Capture cuda graph end. Time elapsed: 642.13 s. mem usage=6.25 GB. avail mem=37.05 GB.
[2025-11-05 10:02:36 TP4] Capture cuda graph end. Time elapsed: 641.87 s. mem usage=6.22 GB. avail mem=36.99 GB.
[2025-11-05 10:02:36 TP6] Capture cuda graph end. Time elapsed: 642.21 s. mem usage=6.15 GB. avail mem=37.13 GB.
[2025-11-05 10:02:36 TP0] max_total_num_tokens=971639, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=37.39 GB
[2025-11-05 10:02:37] INFO:     Started server process [43]
[2025-11-05 10:02:37] INFO:     Waiting for application startup.
[2025-11-05 10:02:37] INFO:     Application startup complete.
[2025-11-05 10:02:37] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-11-05 10:02:38] INFO:     127.0.0.1:33512 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 10:02:38 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:39 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:39 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:39 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:39 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:40 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:40 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:40 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:40] INFO:     127.0.0.1:33518 - "GET /get_model_info HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:40 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:42] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:42] The server is fired up and ready to roll!
[2025-11-05 10:02:47] INFO:     127.0.0.1:49454 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 10:02:47 TP0] Prefill batch, #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:47 TP0] Prefill batch, #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:47 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP0] Prefill batch, #new-seq: 39, #new-token: 2451, #cached-token: 26013, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:48 TP0] Prefill batch, #new-seq: 145, #new-token: 8820, #cached-token: 97006, token usage: 0.00, #running-req: 40, #queue-req: 0, 
[2025-11-05 10:02:48 TP0] Prefill batch, #new-seq: 135, #new-token: 7855, #cached-token: 90349, token usage: 0.01, #running-req: 185, #queue-req: 0, 
[2025-11-05 10:02:49 TP0] Prefill batch, #new-seq: 275, #new-token: 16318, #cached-token: 184105, token usage: 0.02, #running-req: 320, #queue-req: 89, 
[2025-11-05 10:02:51 TP0] Prefill batch, #new-seq: 275, #new-token: 16346, #cached-token: 184172, token usage: 0.04, #running-req: 595, #queue-req: 135, 
[2025-11-05 10:02:53 TP0] Prefill batch, #new-seq: 154, #new-token: 9582, #cached-token: 103146, token usage: 0.05, #running-req: 870, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-05 10:02:55 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:55 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:02:58] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP0] Prefill batch, #new-seq: 1, #new-token: 69, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:58 TP0] Decode batch, #running-req: 1024, #token: 95398, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1518.82, #queue-req: 294, 
[2025-11-05 10:02:58 TP0] Prefill batch, #new-seq: 1, #new-token: 76, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:58 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59 TP0] Prefill batch, #new-seq: 1, #new-token: 62, #cached-token: 671, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP3] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP1] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP7] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP2] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP5] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP0] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP6] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP4] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59 TP0] Prefill batch, #new-seq: 2, #new-token: 126, #cached-token: 1340, token usage: 0.11, #running-req: 1022, #queue-req: 290, 
[aiter] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP0] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP4] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP6] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP2] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP7] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP5] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP3] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP1] [fused_moe] using default for (126, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59] INFO:     127.0.0.1:52284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:02:59] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP0] Prefill batch, #new-seq: 9, #new-token: 674, #cached-token: 6031, token usage: 0.11, #running-req: 1015, #queue-req: 281, 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP2] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP3] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP1] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP6] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP7] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP5] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP0] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:02:59 TP4] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00] INFO:     127.0.0.1:50300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] Prefill batch, #new-seq: 3, #new-token: 193, #cached-token: 2011, token usage: 0.11, #running-req: 1021, #queue-req: 278, 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] Prefill batch, #new-seq: 8, #new-token: 433, #cached-token: 5365, token usage: 0.11, #running-req: 1016, #queue-req: 270, 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] Prefill batch, #new-seq: 5, #new-token: 286, #cached-token: 3347, token usage: 0.11, #running-req: 1019, #queue-req: 265, 
[aiter] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (286, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] Prefill batch, #new-seq: 7, #new-token: 341, #cached-token: 4689, token usage: 0.11, #running-req: 1017, #queue-req: 258, 
[aiter] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (341, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:00] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] Prefill batch, #new-seq: 4, #new-token: 189, #cached-token: 2681, token usage: 0.11, #running-req: 1020, #queue-req: 254, 
[aiter] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP1] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP2] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP6] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP0] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP5] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP4] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP7] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:00 TP3] [fused_moe] using default for (189, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:53712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP0] Prefill batch, #new-seq: 12, #new-token: 867, #cached-token: 8039, token usage: 0.11, #running-req: 1012, #queue-req: 242, 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP1] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP2] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP3] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP6] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP0] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP7] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP5] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP4] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01 TP0] Prefill batch, #new-seq: 8, #new-token: 436, #cached-token: 5361, token usage: 0.11, #running-req: 1016, #queue-req: 234, 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP1] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP2] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP0] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP6] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP3] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP7] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP5] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP4] [fused_moe] using default for (436, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01 TP0] Prefill batch, #new-seq: 6, #new-token: 421, #cached-token: 4022, token usage: 0.11, #running-req: 1018, #queue-req: 228, 
[aiter] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP1] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP6] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP5] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP0] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP3] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP7] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP2] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP4] [fused_moe] using default for (421, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:53398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01 TP0] Prefill batch, #new-seq: 8, #new-token: 515, #cached-token: 5360, token usage: 0.11, #running-req: 1016, #queue-req: 220, 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP1] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP3] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP2] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP6] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP0] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP7] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP5] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP4] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:01] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:01 TP0] Prefill batch, #new-seq: 10, #new-token: 496, #cached-token: 6695, token usage: 0.12, #running-req: 1014, #queue-req: 210, 
[2025-11-05 10:03:02] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:54766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:55884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02 TP0] Prefill batch, #new-seq: 6, #new-token: 280, #cached-token: 4019, token usage: 0.12, #running-req: 1018, #queue-req: 204, 
[aiter] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP2] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP1] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP6] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP3] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP5] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP7] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP4] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP0] [fused_moe] using default for (280, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:51482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:55926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02 TP0] Prefill batch, #new-seq: 9, #new-token: 566, #cached-token: 6029, token usage: 0.12, #running-req: 1015, #queue-req: 195, 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP1] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP2] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP0] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP3] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP6] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP7] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP5] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP4] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02 TP0] Prefill batch, #new-seq: 4, #new-token: 242, #cached-token: 2680, token usage: 0.12, #running-req: 1020, #queue-req: 191, 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP1] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP2] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP3] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP6] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP7] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP0] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP5] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP4] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02] INFO:     127.0.0.1:49466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP0] Prefill batch, #new-seq: 13, #new-token: 756, #cached-token: 8707, token usage: 0.12, #running-req: 1011, #queue-req: 178, 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP1] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP6] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP2] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP3] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP7] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP0] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP5] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP4] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:52246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:02 TP0] Prefill batch, #new-seq: 6, #new-token: 318, #cached-token: 4020, token usage: 0.12, #running-req: 1018, #queue-req: 172, 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP0] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP1] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP6] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP3] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP7] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP2] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP4] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:02 TP5] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:52514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03 TP0] Prefill batch, #new-seq: 8, #new-token: 377, #cached-token: 5359, token usage: 0.12, #running-req: 1016, #queue-req: 164, 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP1] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP2] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP6] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP0] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP5] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP7] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP3] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP4] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03 TP0] Prefill batch, #new-seq: 6, #new-token: 365, #cached-token: 4020, token usage: 0.12, #running-req: 1018, #queue-req: 158, 
[aiter] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP1] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP2] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP3] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP0] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP6] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP7] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP5] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP4] [fused_moe] using default for (365, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:53862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03 TP0] Prefill batch, #new-seq: 10, #new-token: 557, #cached-token: 6698, token usage: 0.12, #running-req: 1014, #queue-req: 148, 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP1] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP2] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP6] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP7] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP3] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP5] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP0] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP4] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:57268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03 TP0] Prefill batch, #new-seq: 8, #new-token: 570, #cached-token: 5361, token usage: 0.12, #running-req: 1016, #queue-req: 140, 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP1] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP2] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP6] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP5] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP7] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP0] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP3] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03 TP4] [fused_moe] using default for (570, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:03] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:52056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:52982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:03 TP0] Prefill batch, #new-seq: 12, #new-token: 666, #cached-token: 8042, token usage: 0.12, #running-req: 1012, #queue-req: 128, 
[2025-11-05 10:03:04] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:57486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP0] Prefill batch, #new-seq: 11, #new-token: 783, #cached-token: 7370, token usage: 0.12, #running-req: 1013, #queue-req: 117, 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP2] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP1] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP6] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP0] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP7] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP3] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP5] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP4] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:52852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04 TP0] Prefill batch, #new-seq: 8, #new-token: 571, #cached-token: 5361, token usage: 0.12, #running-req: 1016, #queue-req: 109, 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP1] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP2] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP3] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP0] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP6] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP7] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP5] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP4] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04 TP0] Prefill batch, #new-seq: 10, #new-token: 682, #cached-token: 6702, token usage: 0.12, #running-req: 1014, #queue-req: 99, 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP1] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP2] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP6] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP3] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP5] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP0] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP7] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP4] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:50524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP0] Prefill batch, #new-seq: 15, #new-token: 774, #cached-token: 10050, token usage: 0.12, #running-req: 1009, #queue-req: 84, 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP1] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP0] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP2] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP6] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP7] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP3] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP5] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP4] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:51862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:04 TP0] Prefill batch, #new-seq: 5, #new-token: 289, #cached-token: 3351, token usage: 0.12, #running-req: 1019, #queue-req: 79, 
[aiter] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP1] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP2] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP0] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP6] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP5] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP3] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP7] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:04 TP4] [fused_moe] using default for (289, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:50834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:54406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP0] Prefill batch, #new-seq: 17, #new-token: 1139, #cached-token: 11383, token usage: 0.12, #running-req: 1007, #queue-req: 62, 
[2025-11-05 10:03:05] INFO:     127.0.0.1:50222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:53886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:57336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05 TP0] Prefill batch, #new-seq: 15, #new-token: 1046, #cached-token: 10049, token usage: 0.12, #running-req: 1009, #queue-req: 47, 
[2025-11-05 10:03:05] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05 TP0] Prefill batch, #new-seq: 11, #new-token: 616, #cached-token: 7367, token usage: 0.12, #running-req: 1013, #queue-req: 36, 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP1] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP0] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP2] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP6] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP3] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP7] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP5] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP4] [fused_moe] using default for (616, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05] INFO:     127.0.0.1:51758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:54868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP0] Prefill batch, #new-seq: 14, #new-token: 832, #cached-token: 9385, token usage: 0.12, #running-req: 1010, #queue-req: 22, 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP1] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP0] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP6] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP2] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP3] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP7] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP4] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05 TP5] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:05] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:50982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:51932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:53428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:05 TP0] Prefill batch, #new-seq: 15, #new-token: 956, #cached-token: 10045, token usage: 0.13, #running-req: 1009, #queue-req: 7, 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP2] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP1] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP0] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP7] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP3] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP5] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP6] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP4] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP0] Decode batch, #running-req: 1009, #token: 120988, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5540.96, #queue-req: 7, 
[2025-11-05 10:03:06] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:54346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06 TP0] Prefill batch, #new-seq: 7, #new-token: 350, #cached-token: 4695, token usage: 0.13, #running-req: 1013, #queue-req: 0, 
[aiter] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP1] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP2] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP0] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP6] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP7] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP5] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP3] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP4] [fused_moe] using default for (350, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06] INFO:     127.0.0.1:50362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:49690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP1] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP4] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP0] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP2] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP6] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP7] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP5] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP3] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP4] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP0] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP2] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP6] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP3] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP1] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP5] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP7] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:51462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP0] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP4] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP2] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP1] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP6] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP3] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP5] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP7] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:06] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP4] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP0] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP6] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP2] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP1] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP5] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP7] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:06 TP3] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP0] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP4] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP6] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP2] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP5] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP1] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP7] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP3] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:55060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP4] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP6] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP2] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP0] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP5] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP1] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP7] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP3] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP0] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP5] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP2] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP4] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP6] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP1] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP3] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP7] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP2] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP4] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP6] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP0] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP5] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP1] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP3] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP7] [fused_moe] using default for (917, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP2] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP1] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP3] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP0] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP4] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP7] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP6] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP5] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:51666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:52908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP7] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP2] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP6] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP4] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP0] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP3] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP5] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP1] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP2] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP4] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP6] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP3] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP0] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP7] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP1] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP5] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:07] INFO:     127.0.0.1:58294 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP0] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP2] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP4] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP6] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP7] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP3] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP1] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:07 TP5] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:51966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:56134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:49678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:08] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP4] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP5] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP0] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP1] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP2] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP6] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP3] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:08 TP7] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:51844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:51876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:57294 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:51196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:09] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP4] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP2] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP6] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP0] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP5] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP1] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP3] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:09 TP7] [fused_moe] using default for (665, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10] INFO:     127.0.0.1:49792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP4] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP0] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP5] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP2] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP6] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP1] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP3] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP7] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP4] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP2] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP0] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP6] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP5] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP1] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP3] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP7] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP4] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP2] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP6] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP0] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP5] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP1] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP3] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP7] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP4] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP2] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP6] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP0] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP5] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP1] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP3] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP7] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:32898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10 TP0] Decode batch, #running-req: 624, #token: 93435, token usage: 0.10, cuda graph: False, gen throughput (token/s): 7523.57, #queue-req: 0, 
[2025-11-05 10:03:10] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:56362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:60388 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP4] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP2] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP6] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP0] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP5] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP1] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP3] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP7] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP4] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP2] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP6] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP0] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP5] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP1] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP3] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP7] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP4] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP0] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP2] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP5] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP6] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP1] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP3] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP7] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:56058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP4] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP0] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP5] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP2] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP6] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP1] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP3] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP7] [fused_moe] using default for (550, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:10] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP4] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP0] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP5] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP2] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP6] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP1] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP3] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:10 TP7] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP4] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP0] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP5] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP2] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP6] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP1] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP3] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP7] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP4] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP0] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP5] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP2] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP6] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP1] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP3] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP7] [fused_moe] using default for (524, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP4] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP2] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP0] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP6] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP5] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP1] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP3] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11 TP7] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:03:11] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:50164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:57210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:11] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:52390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:52362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:58888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:12] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:32786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:32824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13 TP0] Decode batch, #running-req: 288, #token: 54952, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6144.46, #queue-req: 0, 
[2025-11-05 10:03:13] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:53596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:49534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:32840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:53180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:55984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:51492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:13] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:32866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:54630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:32828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:32850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:52002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:14] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15 TP0] Decode batch, #running-req: 117, #token: 27379, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3993.37, #queue-req: 0, 
[2025-11-05 10:03:15] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:49612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:49544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:15] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:33024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:59606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16 TP0] Decode batch, #running-req: 35, #token: 10291, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1844.39, #queue-req: 0, 
[2025-11-05 10:03:16] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:16] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:60598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17 TP0] Decode batch, #running-req: 12, #token: 4340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 821.52, #queue-req: 0, 
[2025-11-05 10:03:17] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:17] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:18] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:18] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:18] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:18] INFO:     127.0.0.1:59726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:18 TP0] Decode batch, #running-req: 3, #token: 1355, token usage: 0.00, cuda graph: True, gen throughput (token/s): 283.40, #queue-req: 0, 
[2025-11-05 10:03:18] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:19] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:03:32] INFO:     127.0.0.1:58712 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 10:03:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-05 10:03:33 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-05 10:03:33 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-05 10:03:33 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-05 10:03:33 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-05 10:03:33 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-05 10:03:33 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-05 10:03:33 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-05 10:03:33 TP2] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 54.05299978s
[2025-11-05 10:04:27 TP2] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 54.05299978s
[2025-11-05 10:04:27] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 10:04:27 TP0] Prefill batch, #new-seq: 32, #new-token: 32, #cached-token: 23243, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:04:27 TP0] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:04:27 TP6] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:04:27 TP4] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:04:27 TP1] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:04:27 TP5] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:04:27 TP3] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:04:27 TP7] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:04:27 TP2] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:04:27 TP0] Prefill batch, #new-seq: 46, #new-token: 46, #cached-token: 33588, token usage: 0.01, #running-req: 33, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35635, token usage: 0.01, #running-req: 79, #queue-req: 0, 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP6] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP4] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP1] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP2] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP5] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP7] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP0] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP3] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39644, token usage: 0.01, #running-req: 128, #queue-req: 0, 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP6] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP4] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP7] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP5] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP2] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP1] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP3] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP0] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:27 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40540, token usage: 0.02, #running-req: 182, #queue-req: 0, 
[2025-11-05 10:04:28 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 42788, token usage: 0.02, #running-req: 238, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP7] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP4] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP6] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP5] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP1] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP2] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP3] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP0] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 45212, token usage: 0.02, #running-req: 297, #queue-req: 0, 
[2025-11-05 10:04:28 TP0] Prefill batch, #new-seq: 63, #new-token: 63, #cached-token: 45896, token usage: 0.03, #running-req: 359, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP6] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP7] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP4] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP5] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP2] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP1] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP3] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP0] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49540, token usage: 0.03, #running-req: 422, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP0] Prefill batch, #new-seq: 69, #new-token: 69, #cached-token: 49931, token usage: 0.03, #running-req: 490, #queue-req: 0, 
[2025-11-05 10:04:28 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53918, token usage: 0.04, #running-req: 559, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:28 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53858, token usage: 0.04, #running-req: 633, #queue-req: 0, 
[2025-11-05 10:04:29 TP0] Prefill batch, #new-seq: 79, #new-token: 79, #cached-token: 57515, token usage: 0.05, #running-req: 707, #queue-req: 0, 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP7] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP5] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP6] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP2] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP1] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP4] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP3] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] Prefill batch, #new-seq: 80, #new-token: 80, #cached-token: 58362, token usage: 0.05, #running-req: 786, #queue-req: 0, 
[2025-11-05 10:04:29 TP0] Prefill batch, #new-seq: 84, #new-token: 84, #cached-token: 61157, token usage: 0.06, #running-req: 866, #queue-req: 0, 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP2] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP1] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP7] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP5] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP6] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP4] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP3] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] Prefill batch, #new-seq: 34, #new-token: 34, #cached-token: 24881, token usage: 0.06, #running-req: 950, #queue-req: 0, 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP6] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP5] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP4] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP7] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP2] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP1] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP3] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP7] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP5] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP4] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP6] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP1] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP2] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP3] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3706, token usage: 0.06, #running-req: 984, #queue-req: 0, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] Prefill batch, #new-seq: 35, #new-token: 35, #cached-token: 25782, token usage: 0.06, #running-req: 989, #queue-req: 15, 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP0] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP4] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP6] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP5] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP7] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP2] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP1] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:29 TP3] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:30 TP0] Decode batch, #running-req: 1024, #token: 66853, token usage: 0.07, cuda graph: False, gen throughput (token/s): 57.06, #queue-req: 221, 
[2025-11-05 10:04:32] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-05 10:04:33] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-05 10:04:34] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:49360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2201, token usage: 0.11, #running-req: 1021, #queue-req: 290, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8173, token usage: 0.11, #running-req: 1013, #queue-req: 279, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:34] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2196, token usage: 0.11, #running-req: 1021, #queue-req: 276, 
[2025-11-05 10:04:34] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:50490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:53586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:53758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5744, token usage: 0.11, #running-req: 1016, #queue-req: 268, 
[2025-11-05 10:04:34 TP0] Decode batch, #running-req: 1016, #token: 105717, token usage: 0.11, cuda graph: False, gen throughput (token/s): 9150.33, #queue-req: 268, 
[2025-11-05 10:04:34] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:34 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3654, token usage: 0.11, #running-req: 1019, #queue-req: 263, 
[2025-11-05 10:04:35] INFO:     127.0.0.1:49204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3571, token usage: 0.11, #running-req: 1019, #queue-req: 258, 
[2025-11-05 10:04:35] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.11, #running-req: 1023, #queue-req: 257, 
[2025-11-05 10:04:35] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7296, token usage: 0.11, #running-req: 1014, #queue-req: 247, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:35 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:35 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:35 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:35 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:35 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:35 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:35 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:35 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:35] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5934, token usage: 0.11, #running-req: 1016, #queue-req: 239, 
[2025-11-05 10:04:35] INFO:     127.0.0.1:49270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35] INFO:     127.0.0.1:57966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:35 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5062, token usage: 0.11, #running-req: 1017, #queue-req: 232, 
[2025-11-05 10:04:36] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:54436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5893, token usage: 0.12, #running-req: 1016, #queue-req: 224, 
[2025-11-05 10:04:36] INFO:     127.0.0.1:51348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:54316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:56808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5826, token usage: 0.12, #running-req: 1016, #queue-req: 216, 
[2025-11-05 10:04:36] INFO:     127.0.0.1:49546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3565, token usage: 0.12, #running-req: 1019, #queue-req: 211, 
[2025-11-05 10:04:36] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:49854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:54042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:54598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7157, token usage: 0.12, #running-req: 1014, #queue-req: 201, 
[2025-11-05 10:04:36] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:36 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3715, token usage: 0.12, #running-req: 1019, #queue-req: 196, 
[2025-11-05 10:04:37] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:51256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:52150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:55306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7309, token usage: 0.12, #running-req: 1014, #queue-req: 186, 
[2025-11-05 10:04:37] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:50660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:50834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7233, token usage: 0.12, #running-req: 1014, #queue-req: 176, 
[2025-11-05 10:04:37] INFO:     127.0.0.1:49338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5755, token usage: 0.12, #running-req: 1016, #queue-req: 168, 
[2025-11-05 10:04:37] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:56116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6500, token usage: 0.12, #running-req: 1015, #queue-req: 159, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:37 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:37 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:37 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:37 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:37 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:37 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:37 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:37 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:37] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:37 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5804, token usage: 0.12, #running-req: 1016, #queue-req: 151, 
[2025-11-05 10:04:38] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:54942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5878, token usage: 0.12, #running-req: 1016, #queue-req: 143, 
[2025-11-05 10:04:38] INFO:     127.0.0.1:49040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8719, token usage: 0.12, #running-req: 1012, #queue-req: 131, 
[2025-11-05 10:04:38] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8098, token usage: 0.12, #running-req: 1013, #queue-req: 120, 
[2025-11-05 10:04:38] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5903, token usage: 0.12, #running-req: 1016, #queue-req: 112, 
[2025-11-05 10:04:38] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:38 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8931, token usage: 0.12, #running-req: 1012, #queue-req: 100, 
[2025-11-05 10:04:39] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12282, token usage: 0.12, #running-req: 1007, #queue-req: 83, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8717, token usage: 0.12, #running-req: 1012, #queue-req: 71, 
[2025-11-05 10:04:39] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9600, token usage: 0.12, #running-req: 1011, #queue-req: 58, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:39] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:49672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12488, token usage: 0.12, #running-req: 1007, #queue-req: 41, 
[2025-11-05 10:04:39] INFO:     127.0.0.1:50988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6551, token usage: 0.12, #running-req: 1015, #queue-req: 32, 
[2025-11-05 10:04:39] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:39] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11673, token usage: 0.12, #running-req: 1008, #queue-req: 16, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:04:40] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:51866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5849, token usage: 0.13, #running-req: 1016, #queue-req: 8, 
[2025-11-05 10:04:40] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:51900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:52760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5777, token usage: 0.13, #running-req: 1015, #queue-req: 0, 
[2025-11-05 10:04:40] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:49702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:50964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:52450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:53330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:40] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP2] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP6] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP5] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP4] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP0] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP1] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP3] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:40 TP7] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56258 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57924 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:49920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (907, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] Decode batch, #running-req: 918, #token: 116634, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5740.85, #queue-req: 0, 
[2025-11-05 10:04:41] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:54406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:56790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:41] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP4] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP2] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP6] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP5] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP0] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP1] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP7] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:41 TP3] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP2] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP4] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP6] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP5] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP0] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP1] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP7] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP3] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP2] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP6] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP4] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP0] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP5] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP1] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP3] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP7] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:55352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:48974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP4] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP2] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP6] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP5] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP0] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP1] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP7] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP3] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP2] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP4] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP6] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP5] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP0] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP1] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP7] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP3] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP2] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP4] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP6] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP0] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP5] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP1] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP3] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP7] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:54166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:54708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP4] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP2] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP6] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP5] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP0] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP1] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP7] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP3] [fused_moe] using default for (816, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42] INFO:     127.0.0.1:51734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:55750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP2] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP4] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP6] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP0] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP5] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP1] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP7] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP3] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:55788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP4] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP2] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP6] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP0] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP5] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP1] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP7] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP3] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:52782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:42] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP4] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP6] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP2] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP0] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP5] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP1] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP7] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:42 TP3] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:56358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP4] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP2] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP6] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP5] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP0] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP1] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP7] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP3] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43] INFO:     127.0.0.1:49076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:54506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP4] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP6] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP2] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP5] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP0] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP1] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP7] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP3] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP2] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP6] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP4] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP5] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP0] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP1] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP7] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP3] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:49486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:58246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP2] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP6] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP4] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP5] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP0] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP1] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP7] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP3] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:51196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP2] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP6] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP4] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP5] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP0] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP1] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP7] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP3] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP6] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP4] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP2] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP0] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP5] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP1] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP7] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP3] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP4] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP2] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP6] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP5] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP0] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP1] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP7] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP3] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:43] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP6] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP2] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:43 TP4] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP0] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP5] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP1] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP7] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP3] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP4] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP6] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP2] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP5] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP0] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP1] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP7] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP3] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44] INFO:     127.0.0.1:49064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:52350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP2] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP4] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP6] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP0] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP5] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP1] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP3] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP7] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44] INFO:     127.0.0.1:49136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:52334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP4] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP6] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP2] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP5] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP0] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP1] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP7] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP3] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP4] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP2] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP6] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP5] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP0] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP7] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP1] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP3] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:51510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:52982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP4] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP2] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP6] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP0] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP5] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP1] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP7] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP3] [fused_moe] using default for (609, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP6] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP2] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP4] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP5] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP0] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP1] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP7] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP3] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44] INFO:     127.0.0.1:49458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:59310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP2] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP4] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP6] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP0] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP5] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP1] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP7] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP3] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:44] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP4] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP6] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP2] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP0] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP5] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP1] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP7] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:44 TP3] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45] INFO:     127.0.0.1:49514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP2] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP4] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP6] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP0] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP5] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP1] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP7] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP3] [fused_moe] using default for (577, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP6] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP2] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP4] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP0] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP5] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP1] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP7] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP3] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP2] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP4] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP6] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP0] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP5] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP1] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP7] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP3] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP2] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP4] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP6] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP0] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP5] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP1] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP7] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP3] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45] INFO:     127.0.0.1:49182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP2] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP6] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP4] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP0] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP5] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP1] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP7] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP3] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59296 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP2] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP6] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP4] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP0] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP5] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP1] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP7] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45 TP3] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:04:45] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:52430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:48958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45 TP0] Decode batch, #running-req: 500, #token: 80745, token usage: 0.08, cuda graph: True, gen throughput (token/s): 7175.48, #queue-req: 0, 
[2025-11-05 10:04:45] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:51610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:45] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:51496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:49324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:49932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:46] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:50596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:54078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:49058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:48930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:50408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:50950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:49820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:47] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:49282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48 TP0] Decode batch, #running-req: 232, #token: 46739, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5653.60, #queue-req: 0, 
[2025-11-05 10:04:48] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:50804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:48] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:49894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:50154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:56842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:51424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:49008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:49] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50 TP0] Decode batch, #running-req: 82, #token: 20884, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3253.82, #queue-req: 0, 
[2025-11-05 10:04:50] INFO:     127.0.0.1:53144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:52816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:50310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:52606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:59326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:59094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:51976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:57082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:50] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51 TP0] Decode batch, #running-req: 25, #token: 7810, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1429.61, #queue-req: 0, 
[2025-11-05 10:04:51] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:58936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:51] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:52] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:52] INFO:     127.0.0.1:59606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:52] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:52] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:52 TP0] Decode batch, #running-req: 5, #token: 2368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 594.34, #queue-req: 0, 
[2025-11-05 10:04:52] INFO:     127.0.0.1:59336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:52] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:52] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:52] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:04:52] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:05] INFO:     127.0.0.1:57910 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 10:05:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 10:05:05] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 10:05:05 TP0] Prefill batch, #new-seq: 33, #new-token: 33, #cached-token: 24063, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:05 TP0] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:05 TP2] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:05 TP4] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:05 TP1] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:05 TP6] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:05 TP7] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:05 TP5] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:05 TP3] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:05 TP0] Prefill batch, #new-seq: 46, #new-token: 46, #cached-token: 33400, token usage: 0.01, #running-req: 34, #queue-req: 0, 
[2025-11-05 10:05:05 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 37679, token usage: 0.01, #running-req: 80, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39703, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-11-05 10:05:06 TP0] Prefill batch, #new-seq: 58, #new-token: 58, #cached-token: 42277, token usage: 0.02, #running-req: 186, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44158, token usage: 0.02, #running-req: 244, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP0] Prefill batch, #new-seq: 64, #new-token: 64, #cached-token: 46653, token usage: 0.02, #running-req: 305, #queue-req: 0, 
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP7] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP5] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP6] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP4] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP2] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP3] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP0] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP1] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:05:06 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:05:06 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:05:06 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 48223, token usage: 0.03, #running-req: 369, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:06 TP0] Prefill batch, #new-seq: 76, #new-token: 76, #cached-token: 54985, token usage: 0.03, #running-req: 435, #queue-req: 0, 
[2025-11-05 10:05:06 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48729, token usage: 0.04, #running-req: 511, #queue-req: 0, 
[2025-11-05 10:05:07 TP0] Prefill batch, #new-seq: 81, #new-token: 81, #cached-token: 58944, token usage: 0.04, #running-req: 578, #queue-req: 0, 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP7] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP2] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP4] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP3] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP1] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP5] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP6] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP0] [fused_moe] using default for (81, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52425, token usage: 0.05, #running-req: 659, #queue-req: 0, 
[2025-11-05 10:05:07 TP0] Prefill batch, #new-seq: 87, #new-token: 87, #cached-token: 63514, token usage: 0.05, #running-req: 731, #queue-req: 0, 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP7] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP2] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP4] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP5] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP3] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP6] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP1] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP0] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP0] Prefill batch, #new-seq: 45, #new-token: 45, #cached-token: 32745, token usage: 0.05, #running-req: 818, #queue-req: 0, 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP6] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP7] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP4] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP5] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP2] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP1] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP3] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP0] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP0] Prefill batch, #new-seq: 45, #new-token: 45, #cached-token: 32569, token usage: 0.06, #running-req: 863, #queue-req: 0, 
[2025-11-05 10:05:07 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38858, token usage: 0.06, #running-req: 908, #queue-req: 0, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:07 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:08 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38931, token usage: 0.06, #running-req: 961, #queue-req: 0, 
[2025-11-05 10:05:08 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7340, token usage: 0.07, #running-req: 1014, #queue-req: 52, 
[2025-11-05 10:05:08 TP0] Decode batch, #running-req: 1024, #token: 66571, token usage: 0.07, cuda graph: False, gen throughput (token/s): 235.83, #queue-req: 223, 
[2025-11-05 10:05:10] INFO:     127.0.0.1:60804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-05 10:05:11] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-05 10:05:11] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-05 10:05:12] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:34188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 3033, token usage: 0.11, #running-req: 1020, #queue-req: 288, 
[2025-11-05 10:05:12] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5876, token usage: 0.11, #running-req: 1016, #queue-req: 280, 
[2025-11-05 10:05:12] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2178, token usage: 0.11, #running-req: 1021, #queue-req: 277, 
[2025-11-05 10:05:12] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:34550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:12] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5778, token usage: 0.11, #running-req: 1016, #queue-req: 269, 
[2025-11-05 10:05:13 TP0] Decode batch, #running-req: 1016, #token: 105717, token usage: 0.11, cuda graph: False, gen throughput (token/s): 8973.14, #queue-req: 269, 
[2025-11-05 10:05:13] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:38076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2914, token usage: 0.11, #running-req: 1020, #queue-req: 265, 
[2025-11-05 10:05:13] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:33516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3593, token usage: 0.11, #running-req: 1019, #queue-req: 260, 
[2025-11-05 10:05:13] INFO:     127.0.0.1:34492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:34500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:35660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:38190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:38328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3594, token usage: 0.11, #running-req: 1019, #queue-req: 255, 
[2025-11-05 10:05:13] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:59918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:37140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:37510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:38220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4377, token usage: 0.11, #running-req: 1018, #queue-req: 249, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:13 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:13 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:13 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:13 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:13 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:13 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:13 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:13 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:13] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:36936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:37210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:37752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:13] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8128, token usage: 0.11, #running-req: 1013, #queue-req: 238, 
[2025-11-05 10:05:14] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:36724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:37660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:38274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7317, token usage: 0.11, #running-req: 1014, #queue-req: 228, 
[2025-11-05 10:05:14] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:34116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:34472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:35302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:38718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5131, token usage: 0.12, #running-req: 1017, #queue-req: 221, 
[2025-11-05 10:05:14] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:60646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:33830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:33928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:34842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:35174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5794, token usage: 0.12, #running-req: 1016, #queue-req: 213, 
[2025-11-05 10:05:14] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:58738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:34916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5001, token usage: 0.12, #running-req: 1017, #queue-req: 206, 
[2025-11-05 10:05:14] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:35566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:14] INFO:     127.0.0.1:37554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6510, token usage: 0.12, #running-req: 1015, #queue-req: 197, 
[2025-11-05 10:05:15] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:34178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:36390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:36870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:37478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6582, token usage: 0.12, #running-req: 1015, #queue-req: 188, 
[2025-11-05 10:05:15] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:35570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:36520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:37992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5094, token usage: 0.12, #running-req: 1017, #queue-req: 181, 
[2025-11-05 10:05:15] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:34738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:35552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:36096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5051, token usage: 0.12, #running-req: 1017, #queue-req: 174, 
[2025-11-05 10:05:15] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:35608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5749, token usage: 0.12, #running-req: 1016, #queue-req: 166, 
[2025-11-05 10:05:15] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:34686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:15] INFO:     127.0.0.1:37334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5084, token usage: 0.12, #running-req: 1017, #queue-req: 159, 
[2025-11-05 10:05:16] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:36888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4327, token usage: 0.12, #running-req: 1018, #queue-req: 153, 
[2025-11-05 10:05:16] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:34176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:35406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5138, token usage: 0.12, #running-req: 1017, #queue-req: 146, 
[2025-11-05 10:05:16] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:59326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:36958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:37678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:38546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8096, token usage: 0.12, #running-req: 1013, #queue-req: 135, 
[2025-11-05 10:05:16] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:60584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:36560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:37294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:37792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9469, token usage: 0.12, #running-req: 1011, #queue-req: 122, 
[2025-11-05 10:05:16] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:35104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:35682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:36584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:37062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:16] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7379, token usage: 0.12, #running-req: 1014, #queue-req: 112, 
[2025-11-05 10:05:17] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:37574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6652, token usage: 0.12, #running-req: 1015, #queue-req: 103, 
[2025-11-05 10:05:17] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:60794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:37604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6579, token usage: 0.12, #running-req: 1015, #queue-req: 94, 
[2025-11-05 10:05:17] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:36970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:37444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:37606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7211, token usage: 0.12, #running-req: 1014, #queue-req: 84, 
[2025-11-05 10:05:17] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:60952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:33246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:35480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:36512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:36614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:37888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11754, token usage: 0.12, #running-req: 1008, #queue-req: 68, 
[2025-11-05 10:05:17] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:34252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:35246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:17] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8072, token usage: 0.13, #running-req: 1013, #queue-req: 57, 
[2025-11-05 10:05:18] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:34222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:37348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:37596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8128, token usage: 0.13, #running-req: 1013, #queue-req: 46, 
[2025-11-05 10:05:18] INFO:     127.0.0.1:59734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:60750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:32844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:33466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:35032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:35992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:37390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:37418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:37738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:37904 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP0] Prefill batch, #new-seq: 18, #new-token: 18, #cached-token: 13077, token usage: 0.13, #running-req: 1006, #queue-req: 28, 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP7] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP3] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP0] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP2] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP6] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP1] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP5] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18 TP4] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:18] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:33730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8763, token usage: 0.13, #running-req: 1012, #queue-req: 16, 
[2025-11-05 10:05:18] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:34056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:37494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:38422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7305, token usage: 0.13, #running-req: 1014, #queue-req: 6, 
[2025-11-05 10:05:18] INFO:     127.0.0.1:59076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:33200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:35738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:18] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4320, token usage: 0.13, #running-req: 1017, #queue-req: 0, 
[2025-11-05 10:05:19] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:32888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:37288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:37454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38712 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP4] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP2] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP6] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP0] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP5] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP3] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP1] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP7] [fused_moe] using default for (988, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38536 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP2] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP4] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP3] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP6] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP0] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP1] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP7] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP5] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:36280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38496 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP2] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP4] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP6] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP1] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP0] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP5] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP7] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP3] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:35284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:37182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:37292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:37762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP6] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP2] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP4] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP0] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP3] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP1] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP5] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP7] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:58936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:37754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:19] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP2] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP4] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP6] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP7] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP0] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP3] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP1] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:19 TP5] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP4] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP2] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP6] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP3] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP0] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP5] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP1] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP7] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20] INFO:     127.0.0.1:57972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37100 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP4] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP2] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP6] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP3] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP0] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP1] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP5] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP7] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP0] Decode batch, #running-req: 921, #token: 117135, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5649.98, #queue-req: 0, 
[2025-11-05 10:05:20] INFO:     127.0.0.1:58344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:35274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:34384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:35558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:38812 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP2] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP4] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP6] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP7] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP0] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP3] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP5] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP1] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:34994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:34768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP4] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP7] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP3] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP0] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP1] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP5] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP2] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP6] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP4] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP5] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP7] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP0] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP3] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP1] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP2] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP6] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:35766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP4] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP2] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP0] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP7] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP6] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP3] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP1] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20 TP5] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:20] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:20] INFO:     127.0.0.1:38468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38782 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:39222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21] INFO:     127.0.0.1:34750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:39442 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:37570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:37616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:37874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38864 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:37030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:39910 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36400 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:60930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:39352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:37304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:34068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:21] INFO:     127.0.0.1:38264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP2] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP4] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP6] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP7] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP3] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP0] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP5] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:21 TP1] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:36212 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP2] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP4] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP6] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP7] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP0] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP3] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP1] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP5] [fused_moe] using default for (712, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:59234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:37224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:37508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:39456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:39622 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP4] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP2] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP6] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP3] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP7] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP0] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP5] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP1] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:36248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:39838 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP4] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP2] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP6] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP0] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP7] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP1] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP3] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP5] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:36296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:37718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:37222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:37238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:39574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:40072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:41104 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP2] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP4] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP6] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP7] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP3] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP5] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP0] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP1] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:39290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:40220 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP2] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP4] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP6] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP0] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP3] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP7] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP5] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP1] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:37838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:37964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38854 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP2] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP4] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP6] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP7] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP0] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP3] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP5] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP1] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22] INFO:     127.0.0.1:59522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:34946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:22] INFO:     127.0.0.1:38702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP4] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP2] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP6] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP3] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP0] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP7] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP5] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:22 TP1] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:38448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:41222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP2] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP4] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP6] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP0] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP3] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP7] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP5] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP1] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23] INFO:     127.0.0.1:59004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:38650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP4] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP2] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP6] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP3] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP7] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP0] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP1] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP5] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:32772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP4] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP2] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP6] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP0] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP7] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP3] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP1] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP5] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP4] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP2] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP6] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP0] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP7] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP1] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP3] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP5] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:40006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:59878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP4] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP2] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP6] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP0] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP7] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP3] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP1] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP5] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:32780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:32952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:40572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:40668 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP4] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP2] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP6] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP7] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP0] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP3] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP5] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP1] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:59948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:40586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:41378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP4] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP2] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP6] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP7] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP0] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP5] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP3] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP1] [fused_moe] using default for (529, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:60966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39832 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP2] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP4] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP6] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP0] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP7] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP3] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP1] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23 TP5] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:23] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:60776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:32808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:37750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:23] INFO:     127.0.0.1:39900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:38204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24 TP0] Decode batch, #running-req: 500, #token: 79829, token usage: 0.08, cuda graph: True, gen throughput (token/s): 7366.34, #queue-req: 0, 
[2025-11-05 10:05:24] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:37286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:41100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:60890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:36952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:37060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:37172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:37894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:38236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:36146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:58666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:38296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:41174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:34450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:38804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:59380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:32838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:37406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:37636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:38484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:41352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:24] INFO:     127.0.0.1:40280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:34960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:37694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:37824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:36780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:37360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:59816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:37038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:59702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:35110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:37002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:36174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:36752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:37374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:34622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:35786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:37926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:33024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:33846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:36534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:59316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:35910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:39254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:25] INFO:     127.0.0.1:41188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:38946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:60210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:37150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:36842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:38344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:60352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26 TP0] Decode batch, #running-req: 230, #token: 46648, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5677.62, #queue-req: 0, 
[2025-11-05 10:05:26] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:33186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:33650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:60604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:35700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:38752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:38956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:26] INFO:     127.0.0.1:39166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:36800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:32824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:35196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:37044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:41322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:35604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:41172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:37582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:38974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:35834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:35292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:41390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:36378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:37460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:34512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:37856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:38824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:27] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:33776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:41334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:38700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:41278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:37906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:38688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:58052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:38876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:36680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:39256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28 TP0] Decode batch, #running-req: 79, #token: 20262, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3280.86, #queue-req: 0, 
[2025-11-05 10:05:28] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:39010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:36554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:39330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:59464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:60774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:33558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:39720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:39506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:34944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:37648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:36616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:60916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:28] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:36028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:38674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:41208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:37878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29 TP0] Decode batch, #running-req: 26, #token: 7782, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1408.05, #queue-req: 0, 
[2025-11-05 10:05:29] INFO:     127.0.0.1:33724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:41346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:39656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:29] INFO:     127.0.0.1:40086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:39312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:39472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:41042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:33376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:36912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:39396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:36582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30 TP0] Decode batch, #running-req: 6, #token: 2649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 607.53, #queue-req: 0, 
[2025-11-05 10:05:30] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:30] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:31] INFO:     127.0.0.1:39704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:31] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:31] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:43] INFO:     127.0.0.1:53350 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 10:05:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 10:05:43] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 10:05:44 TP0] Prefill batch, #new-seq: 42, #new-token: 42, #cached-token: 30405, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP0] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP4] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP2] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP6] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP5] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP1] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP7] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP3] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 35003, token usage: 0.01, #running-req: 43, #queue-req: 0, 
[2025-11-05 10:05:44 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 37998, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-11-05 10:05:44 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40160, token usage: 0.01, #running-req: 143, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP5] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP4] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP3] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP6] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP2] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP0] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP7] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP1] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:44 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44322, token usage: 0.02, #running-req: 198, #queue-req: 0, 
[2025-11-05 10:05:44 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 45004, token usage: 0.02, #running-req: 259, #queue-req: 0, 
[2025-11-05 10:05:44 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48797, token usage: 0.02, #running-req: 321, #queue-req: 0, 
[2025-11-05 10:05:44 TP0] Prefill batch, #new-seq: 69, #new-token: 69, #cached-token: 50450, token usage: 0.03, #running-req: 388, #queue-req: 0, 
[2025-11-05 10:05:45 TP0] Prefill batch, #new-seq: 73, #new-token: 73, #cached-token: 52906, token usage: 0.03, #running-req: 457, #queue-req: 0, 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP2] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP7] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP1] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP3] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP4] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP5] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP0] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP6] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52361, token usage: 0.04, #running-req: 530, #queue-req: 0, 
[2025-11-05 10:05:45 TP0] Prefill batch, #new-seq: 78, #new-token: 78, #cached-token: 56656, token usage: 0.04, #running-req: 602, #queue-req: 0, 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP2] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP4] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP7] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP1] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP6] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP5] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP3] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP0] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53902, token usage: 0.05, #running-req: 680, #queue-req: 0, 
[2025-11-05 10:05:45 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10838, token usage: 0.05, #running-req: 754, #queue-req: 0, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:45 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40359, token usage: 0.05, #running-req: 769, #queue-req: 0, 
[2025-11-05 10:05:46 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36277, token usage: 0.06, #running-req: 824, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51747, token usage: 0.06, #running-req: 874, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 43341, token usage: 0.06, #running-req: 945, #queue-req: 0, 
[2025-11-05 10:05:46 TP0] Prefill batch, #new-seq: 20, #new-token: 20, #cached-token: 14658, token usage: 0.07, #running-req: 1004, #queue-req: 58, 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP2] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP1] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP7] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP4] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP6] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP3] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP5] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP0] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:46 TP0] Decode batch, #running-req: 1024, #token: 66341, token usage: 0.07, cuda graph: False, gen throughput (token/s): 225.38, #queue-req: 227, 
[2025-11-05 10:05:49] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 723, token usage: 0.10, #running-req: 1023, #queue-req: 294, 
[2025-11-05 10:05:49] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-05 10:05:50] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:53380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2991, token usage: 0.11, #running-req: 1020, #queue-req: 289, 
[2025-11-05 10:05:50] INFO:     127.0.0.1:54836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:55112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:59092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5906, token usage: 0.11, #running-req: 1016, #queue-req: 281, 
[2025-11-05 10:05:50] INFO:     127.0.0.1:54766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:50] INFO:     127.0.0.1:59134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1477, token usage: 0.11, #running-req: 1022, #queue-req: 279, 
[2025-11-05 10:05:51] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:58214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7231, token usage: 0.11, #running-req: 1014, #queue-req: 269, 
[2025-11-05 10:05:51 TP0] Decode batch, #running-req: 1014, #token: 105380, token usage: 0.11, cuda graph: False, gen throughput (token/s): 9149.00, #queue-req: 269, 
[2025-11-05 10:05:51] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3632, token usage: 0.11, #running-req: 1019, #queue-req: 264, 
[2025-11-05 10:05:51] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:58554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3576, token usage: 0.11, #running-req: 1019, #queue-req: 259, 
[2025-11-05 10:05:51] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2175, token usage: 0.11, #running-req: 1021, #queue-req: 256, 
[2025-11-05 10:05:51] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:51] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5092, token usage: 0.11, #running-req: 1017, #queue-req: 249, 
[2025-11-05 10:05:52] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:59686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4364, token usage: 0.11, #running-req: 1018, #queue-req: 243, 
[2025-11-05 10:05:52] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5210, token usage: 0.11, #running-req: 1017, #queue-req: 236, 
[2025-11-05 10:05:52] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:58972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:33158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8055, token usage: 0.11, #running-req: 1013, #queue-req: 225, 
[2025-11-05 10:05:52] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:58466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6578, token usage: 0.12, #running-req: 1015, #queue-req: 216, 
[2025-11-05 10:05:52] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:56750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:52] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6457, token usage: 0.12, #running-req: 1015, #queue-req: 207, 
[2025-11-05 10:05:53] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5017, token usage: 0.12, #running-req: 1017, #queue-req: 200, 
[2025-11-05 10:05:53] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:33170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5856, token usage: 0.12, #running-req: 1016, #queue-req: 192, 
[2025-11-05 10:05:53] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3660, token usage: 0.12, #running-req: 1019, #queue-req: 187, 
[2025-11-05 10:05:53] INFO:     127.0.0.1:54218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:55352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7253, token usage: 0.12, #running-req: 1014, #queue-req: 177, 
[2025-11-05 10:05:53] INFO:     127.0.0.1:54204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:53] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7933, token usage: 0.12, #running-req: 1013, #queue-req: 166, 
[2025-11-05 10:05:54] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:59496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4325, token usage: 0.12, #running-req: 1018, #queue-req: 160, 
[2025-11-05 10:05:54] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2884, token usage: 0.12, #running-req: 1020, #queue-req: 156, 
[2025-11-05 10:05:54] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:60534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6563, token usage: 0.12, #running-req: 1015, #queue-req: 147, 
[2025-11-05 10:05:54] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:60424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8847, token usage: 0.12, #running-req: 1012, #queue-req: 135, 
[2025-11-05 10:05:54] INFO:     127.0.0.1:54664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:32980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:54] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6516, token usage: 0.12, #running-req: 1015, #queue-req: 126, 
[2025-11-05 10:05:55] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:55700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7392, token usage: 0.12, #running-req: 1014, #queue-req: 116, 
[2025-11-05 10:05:55] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:58474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:60374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8103, token usage: 0.12, #running-req: 1013, #queue-req: 105, 
[2025-11-05 10:05:55] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:60806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:59978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10950, token usage: 0.12, #running-req: 1009, #queue-req: 90, 
[2025-11-05 10:05:55] INFO:     127.0.0.1:54462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:60210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7964, token usage: 0.12, #running-req: 1013, #queue-req: 79, 
[2025-11-05 10:05:55] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:55168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:55624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:57870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:55] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 11048, token usage: 0.12, #running-req: 1009, #queue-req: 64, 
[2025-11-05 10:05:56] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7453, token usage: 0.12, #running-req: 1014, #queue-req: 54, 
[2025-11-05 10:05:56] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:55038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8002, token usage: 0.13, #running-req: 1013, #queue-req: 43, 
[2025-11-05 10:05:56] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:60272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10893, token usage: 0.13, #running-req: 1009, #queue-req: 28, 
[2025-11-05 10:05:56] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:60188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11746, token usage: 0.13, #running-req: 1008, #queue-req: 12, 
[2025-11-05 10:05:56] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:56] INFO:     127.0.0.1:33188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5752, token usage: 0.13, #running-req: 1016, #queue-req: 4, 
[2025-11-05 10:05:57] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2890, token usage: 0.13, #running-req: 1018, #queue-req: 0, 
[2025-11-05 10:05:57] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:58494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP4] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP5] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP0] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP6] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP2] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP1] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP7] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP3] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:53614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:54790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:58936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:57] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP2] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP0] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP4] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP6] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP5] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP1] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP3] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:57 TP7] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP4] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP6] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP5] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP2] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP3] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP7] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP1] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:32952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:53870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP4] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP5] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP2] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP6] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP3] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP7] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP1] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP4] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP6] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP5] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP2] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP7] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP1] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP3] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] Decode batch, #running-req: 926, #token: 117103, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5654.73, #queue-req: 0, 
[2025-11-05 10:05:58] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:32894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP2] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP4] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP6] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP5] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP1] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP7] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP3] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:57194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP4] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP6] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP2] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP5] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP1] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP7] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP3] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP4] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP6] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP2] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP5] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP1] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP7] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP3] [fused_moe] using default for (879, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:53784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:32854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:33136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP6] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP2] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP4] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP5] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP1] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP7] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP3] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP4] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP6] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP2] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP5] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP1] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP7] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP3] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:53860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:58] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP4] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP2] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP6] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP5] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP0] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP1] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP7] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:58 TP3] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:55750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:54714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:56258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP4] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP6] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP2] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP5] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP0] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP1] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP7] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP3] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:32822 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP4] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP2] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP6] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP0] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP5] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP1] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP7] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP3] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60366 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP4] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP6] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP2] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP5] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP0] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP1] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP7] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP3] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP4] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP6] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP2] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP5] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP0] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP1] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP3] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP7] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59] INFO:     127.0.0.1:54466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:55294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP2] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP4] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP6] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP0] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP5] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP1] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP3] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP7] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:05:59] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP6] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP4] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP2] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP5] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP0] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP1] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP7] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP3] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP2] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP6] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP0] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP4] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP3] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP7] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP1] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:05:59 TP5] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00] INFO:     127.0.0.1:54316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP4] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP2] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP6] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP0] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP5] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP1] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP3] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP7] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP5] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP4] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP6] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP2] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP0] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP1] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP3] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP7] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00] INFO:     127.0.0.1:60966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP6] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP2] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP4] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP5] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP0] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP1] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP7] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP3] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP2] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP4] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP6] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP0] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP5] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP1] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP3] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP7] [fused_moe] using default for (709, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP6] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP2] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP4] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP5] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP0] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP1] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP7] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP3] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP6] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP4] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP2] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP5] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP0] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP1] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP7] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00 TP3] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:00] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:55242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:59446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:00] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP4] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP6] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP2] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP5] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP0] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP1] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP7] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP3] [fused_moe] using default for (637, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP2] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP4] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP6] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP5] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP0] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP1] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP3] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP7] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01] INFO:     127.0.0.1:54048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:54200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:54406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:53992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33972 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP4] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP6] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP2] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP0] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP5] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP1] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP7] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP3] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP6] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP4] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP2] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP5] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP0] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP1] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP7] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP3] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:34000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP4] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP6] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP2] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP5] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP0] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP1] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP7] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP3] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60892 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP4] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP2] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP6] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP5] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP0] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP1] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP7] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP3] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:58888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:34594 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP6] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP2] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP4] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP5] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP0] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP1] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP7] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP3] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:34226 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP4] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP2] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP6] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP5] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP0] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP1] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP7] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP3] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01] INFO:     127.0.0.1:55016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:56480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:34162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:01] INFO:     127.0.0.1:34844 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP4] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP6] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP2] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP5] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP0] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP1] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP7] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:01 TP3] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:02] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:55602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02 TP0] Decode batch, #running-req: 491, #token: 78932, token usage: 0.08, cuda graph: True, gen throughput (token/s): 7300.36, #queue-req: 0, 
[2025-11-05 10:06:02] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:32806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:53880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:32800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:32888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:33874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:02] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:32910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:60794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:33954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:03] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:32808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:32866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:59992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:32786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:32838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04 TP0] Decode batch, #running-req: 227, #token: 45682, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5632.51, #queue-req: 0, 
[2025-11-05 10:06:04] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:59004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:04] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:55984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:32828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:54084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:34388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:05] INFO:     127.0.0.1:60820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:59154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:32932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06 TP0] Decode batch, #running-req: 86, #token: 21301, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3351.06, #queue-req: 0, 
[2025-11-05 10:06:06] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:33632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:06] INFO:     127.0.0.1:34804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:33818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:59878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:34784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:57082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:07 TP0] Decode batch, #running-req: 28, #token: 8324, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1463.59, #queue-req: 0, 
[2025-11-05 10:06:07] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:56204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:33574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:33216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:33038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:32918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:32850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:60394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:08 TP0] Decode batch, #running-req: 8, #token: 2987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 717.27, #queue-req: 0, 
[2025-11-05 10:06:08] INFO:     127.0.0.1:33892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:09] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:09] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:09] INFO:     127.0.0.1:33706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:09] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:09 TP0] Decode batch, #running-req: 2, #token: 1404, token usage: 0.00, cuda graph: True, gen throughput (token/s): 203.61, #queue-req: 0, 
[2025-11-05 10:06:10 TP0] Decode batch, #running-req: 2, #token: 1484, token usage: 0.00, cuda graph: True, gen throughput (token/s): 109.84, #queue-req: 0, 
[2025-11-05 10:06:10] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:10] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:23] INFO:     127.0.0.1:53202 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-05 10:06:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 10:06:23] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-05 10:06:23 TP0] Prefill batch, #new-seq: 40, #new-token: 40, #cached-token: 28994, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-05 10:06:23 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34984, token usage: 0.01, #running-req: 41, #queue-req: 0, 
[2025-11-05 10:06:24 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36484, token usage: 0.01, #running-req: 89, #queue-req: 0, 
[2025-11-05 10:06:24 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40934, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-11-05 10:06:24 TP0] Prefill batch, #new-seq: 57, #new-token: 57, #cached-token: 41389, token usage: 0.02, #running-req: 195, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:24 TP6] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:24 TP2] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:24 TP3] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:24 TP4] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:24 TP5] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:24 TP1] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:24 TP7] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:24 TP0] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:24 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 45048, token usage: 0.02, #running-req: 252, #queue-req: 0, 
[2025-11-05 10:06:24 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 45201, token usage: 0.02, #running-req: 314, #queue-req: 0, 
[2025-11-05 10:06:24 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48960, token usage: 0.03, #running-req: 376, #queue-req: 0, 
[2025-11-05 10:06:24 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49261, token usage: 0.03, #running-req: 443, #queue-req: 0, 
[2025-11-05 10:06:24 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53673, token usage: 0.04, #running-req: 511, #queue-req: 0, 
[2025-11-05 10:06:25 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47307, token usage: 0.04, #running-req: 585, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP7] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP6] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP4] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP1] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP5] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP2] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP3] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP0] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8019, token usage: 0.04, #running-req: 650, #queue-req: 0, 
[2025-11-05 10:06:25 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34971, token usage: 0.05, #running-req: 661, #queue-req: 0, 
[2025-11-05 10:06:25 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 37836, token usage: 0.05, #running-req: 709, #queue-req: 0, 
[2025-11-05 10:06:25 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43883, token usage: 0.05, #running-req: 761, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP2] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP7] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP1] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP3] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP4] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP6] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP0] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP5] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:25 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 42851, token usage: 0.06, #running-req: 821, #queue-req: 0, 
[2025-11-05 10:06:26 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 48103, token usage: 0.06, #running-req: 880, #queue-req: 0, 
[2025-11-05 10:06:26 TP0] Prefill batch, #new-seq: 64, #new-token: 64, #cached-token: 47006, token usage: 0.06, #running-req: 946, #queue-req: 0, 
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP7] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP2] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP1] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP3] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP6] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP4] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP5] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP0] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:06:26 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:06:26 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:06:26 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:06:26 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:06:26 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:06:26 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:06:26 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-05 10:06:26 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-05 10:06:26 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-05 10:06:26 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10292, token usage: 0.06, #running-req: 1010, #queue-req: 59, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:26 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:26 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:26 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:26 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:26 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:26 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:26 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:26 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:27 TP0] Decode batch, #running-req: 1024, #token: 70241, token usage: 0.07, cuda graph: False, gen throughput (token/s): 449.31, #queue-req: 295, 
[2025-11-05 10:06:28] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-05 10:06:29] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:29] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-05 10:06:29] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:29 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1445, token usage: 0.10, #running-req: 1022, #queue-req: 291, 
[2025-11-05 10:06:30] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 3028, token usage: 0.11, #running-req: 1020, #queue-req: 287, 
[2025-11-05 10:06:30] INFO:     127.0.0.1:54306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6598, token usage: 0.11, #running-req: 1015, #queue-req: 278, 
[2025-11-05 10:06:30] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:30 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2190, token usage: 0.11, #running-req: 1021, #queue-req: 275, 
[2025-11-05 10:06:31] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5763, token usage: 0.11, #running-req: 1016, #queue-req: 267, 
[2025-11-05 10:06:31] INFO:     127.0.0.1:54880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2896, token usage: 0.11, #running-req: 1020, #queue-req: 263, 
[2025-11-05 10:06:31] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3592, token usage: 0.11, #running-req: 1019, #queue-req: 258, 
[2025-11-05 10:06:31] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:47760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2897, token usage: 0.11, #running-req: 1020, #queue-req: 254, 
[2025-11-05 10:06:31] INFO:     127.0.0.1:53574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:60510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:31 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4388, token usage: 0.11, #running-req: 1018, #queue-req: 248, 
[2025-11-05 10:06:32 TP0] Decode batch, #running-req: 1018, #token: 108322, token usage: 0.11, cuda graph: False, gen throughput (token/s): 8234.55, #queue-req: 248, 
[2025-11-05 10:06:32] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:47284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5235, token usage: 0.11, #running-req: 1017, #queue-req: 241, 
[2025-11-05 10:06:32] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:32874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6509, token usage: 0.11, #running-req: 1015, #queue-req: 232, 
[2025-11-05 10:06:32] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:60552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:48280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8802, token usage: 0.11, #running-req: 1012, #queue-req: 220, 
[2025-11-05 10:06:32] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5080, token usage: 0.12, #running-req: 1017, #queue-req: 213, 
[2025-11-05 10:06:32] INFO:     127.0.0.1:54048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:56238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:57114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:32 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5003, token usage: 0.12, #running-req: 1017, #queue-req: 206, 
[2025-11-05 10:06:33] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:54078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:47952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5086, token usage: 0.12, #running-req: 1017, #queue-req: 199, 
[2025-11-05 10:06:33] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5831, token usage: 0.12, #running-req: 1016, #queue-req: 191, 
[2025-11-05 10:06:33] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4372, token usage: 0.12, #running-req: 1018, #queue-req: 185, 
[2025-11-05 10:06:33] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:47356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7225, token usage: 0.12, #running-req: 1014, #queue-req: 175, 
[2025-11-05 10:06:33] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:33 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6470, token usage: 0.12, #running-req: 1015, #queue-req: 166, 
[2025-11-05 10:06:34] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:59694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:32888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5084, token usage: 0.12, #running-req: 1017, #queue-req: 159, 
[2025-11-05 10:06:34] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3595, token usage: 0.12, #running-req: 1019, #queue-req: 154, 
[2025-11-05 10:06:34] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4388, token usage: 0.12, #running-req: 1018, #queue-req: 148, 
[2025-11-05 10:06:34] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:53524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9578, token usage: 0.12, #running-req: 1011, #queue-req: 135, 
[2025-11-05 10:06:34] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34] INFO:     127.0.0.1:47314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:34 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3559, token usage: 0.12, #running-req: 1019, #queue-req: 130, 
[2025-11-05 10:06:35] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:60568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5910, token usage: 0.12, #running-req: 1016, #queue-req: 122, 
[2025-11-05 10:06:35] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:53678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:54506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:60786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:32958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 11074, token usage: 0.12, #running-req: 1009, #queue-req: 107, 
[2025-11-05 10:06:35] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:59524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:60084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35 TP0] Prefill batch, #new-seq: 18, #new-token: 18, #cached-token: 13131, token usage: 0.12, #running-req: 1006, #queue-req: 89, 
[2025-11-05 10:06:35] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:32824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7998, token usage: 0.12, #running-req: 1013, #queue-req: 78, 
[2025-11-05 10:06:35] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:35 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9541, token usage: 0.12, #running-req: 1011, #queue-req: 65, 
[2025-11-05 10:06:36] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:59004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9705, token usage: 0.12, #running-req: 1011, #queue-req: 52, 
[2025-11-05 10:06:36] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:58430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:58894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8663, token usage: 0.12, #running-req: 1012, #queue-req: 40, 
[2025-11-05 10:06:36] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:57602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:48128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11631, token usage: 0.12, #running-req: 1008, #queue-req: 24, 
[2025-11-05 10:06:36] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7334, token usage: 0.13, #running-req: 1014, #queue-req: 14, 
[2025-11-05 10:06:36] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:56134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:36 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9447, token usage: 0.13, #running-req: 1011, #queue-req: 1, 
[2025-11-05 10:06:37] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:47574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 716, token usage: 0.13, #running-req: 1013, #queue-req: 0, 
[2025-11-05 10:06:37] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:54866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:55624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP3] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP0] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP2] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP4] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP6] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP7] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP1] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP5] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP2] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP6] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP3] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP4] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP0] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP7] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP1] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP5] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:55708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:37] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP3] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP7] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP0] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP2] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP4] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP6] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP1] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:37 TP5] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP2] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP6] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP4] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP7] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP3] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP0] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP1] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP5] [fused_moe] using default for (934, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP2] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP6] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP4] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP3] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP0] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP7] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP5] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP1] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:53992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP3] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP7] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP0] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP2] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP6] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP4] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP5] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP1] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:48380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP2] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP6] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP4] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP3] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP7] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP0] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP1] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP5] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:32836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP2] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP6] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP3] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP4] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP7] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP0] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP1] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP5] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP0] Decode batch, #running-req: 873, #token: 113487, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5907.05, #queue-req: 0, 
[2025-11-05 10:06:38] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP6] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP2] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP4] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP3] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP0] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP7] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP1] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP5] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:59848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP2] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP6] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP4] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP3] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP0] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP7] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP1] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP5] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:33010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:38] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP2] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP3] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP6] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP4] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP7] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP0] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP1] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:38 TP5] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:33050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:47994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP3] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP2] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP6] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP0] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP4] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP7] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP1] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP5] [fused_moe] using default for (822, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP2] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP6] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP4] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP7] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP5] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP0] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP3] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP1] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:48454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP3] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP6] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP4] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP2] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP7] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP0] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP1] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP5] [fused_moe] using default for (792, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:32952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP3] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP2] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP6] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP7] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP0] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP4] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP1] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP5] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP2] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP1] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP3] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP0] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP5] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP6] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP4] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP7] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:47680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP2] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP6] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP3] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP7] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP0] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP4] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP1] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP5] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP2] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP3] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP6] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP7] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP0] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP4] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP1] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39 TP5] [fused_moe] using default for (742, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:39] INFO:     127.0.0.1:54256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:54400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:60724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:39] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:32822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP2] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP6] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP4] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP3] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP7] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP0] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP1] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP5] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP2] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP4] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP6] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP3] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP7] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP0] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP1] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP5] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:53788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:55750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:47472 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP2] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP6] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP4] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP3] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP7] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP0] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP1] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP5] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:32894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:49058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP6] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP2] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP4] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP3] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP7] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP0] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP1] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP5] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40] INFO:     127.0.0.1:55260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP6] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP4] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP2] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP3] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP0] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP7] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP1] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP5] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:49190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:60530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:40] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP2] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP4] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP6] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP3] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP0] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:40 TP7] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP1] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP5] [fused_moe] using default for (625, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:47598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:47658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:50660 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP2] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP4] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP6] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP3] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP0] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP7] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP1] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP5] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:59248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP6] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP2] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP4] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP3] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP7] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP0] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP1] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP5] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:59296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:57862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP6] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP2] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP4] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP3] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP7] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP0] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP1] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP5] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:47288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP3] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP6] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP2] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP0] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP4] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP7] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP1] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP5] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:47696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:41] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP2] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP6] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP4] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP3] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP7] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP0] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP1] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:41 TP5] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-05 10:06:42] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:47822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:53318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:47794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42 TP0] Decode batch, #running-req: 471, #token: 77334, token usage: 0.08, cuda graph: True, gen throughput (token/s): 7080.53, #queue-req: 0, 
[2025-11-05 10:06:42] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:49272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:48622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:42] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:58502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:60680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:60248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:60442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:58548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:43] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:47648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:60790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:32840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:47900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:47732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44 TP0] Decode batch, #running-req: 213, #token: 43534, token usage: 0.04, cuda graph: True, gen throughput (token/s): 5409.56, #queue-req: 0, 
[2025-11-05 10:06:44] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:49684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:50434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:44] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:47614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:58554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:47788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:54008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:32806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:60694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:45] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:48436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:48414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:54630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:54020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:58028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46 TP0] Decode batch, #running-req: 74, #token: 18577, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3036.85, #queue-req: 0, 
[2025-11-05 10:06:46] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:46] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:59630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:49676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:48984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:47 TP0] Decode batch, #running-req: 23, #token: 7286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1317.65, #queue-req: 0, 
[2025-11-05 10:06:47] INFO:     127.0.0.1:48124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:56842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:60518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:49870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48 TP0] Decode batch, #running-req: 7, #token: 2663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 615.79, #queue-req: 0, 
[2025-11-05 10:06:48] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:48] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:49] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:49] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:49] INFO:     127.0.0.1:49294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:49] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-05 10:06:54] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-11-05 10:06:58] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
